= Curso de Rancher: Gestión Avanzada de Kubernetes
:doctype: book
:toc: left
:icons: font
:source-highlighter: highlight.js
:toclevels: 3
:diagram-plantuml-format: png


== Módulo 1: Introducción a Rancher

=== 1.1 ¿Qué es Rancher?

Rancher es una plataforma completa de gestión de contenedores y Kubernetes que simplifica la implementación, gestión y operación de clústeres Kubernetes en cualquier infraestructura. Desarrollada por Rancher Labs (adquirida por SUSE en 2020), Rancher se ha convertido en una de las soluciones más populares para organizaciones que buscan adoptar y escalar Kubernetes de manera efectiva.

==== Historia y evolución de Rancher

La evolución de Rancher ha marcado hitos importantes en la gestión de contenedores:

.Cronología de Rancher:
[cols="1,3"]
|===
|Año |Evento

|2014
|Fundación de Rancher Labs por Sheng Liang, Shannon Williams y Darren Shepherd

|2015
|Lanzamiento de Rancher 1.0, enfocado en orquestación multi-plataforma (Swarm, Mesos, Kubernetes)

|2016
|Introducción de Rancher Compose y soporte mejorado para múltiples orquestadores

|2018
|Lanzamiento de Rancher 2.0, rediseñado completamente para centrarse 100% en Kubernetes

|2019
|Introducción de RKE (Rancher Kubernetes Engine) y K3s (Kubernetes ligero)

|2020
|Adquisición de Rancher Labs por SUSE por $600-700 millones

|2021
|Lanzamiento de Rancher Desktop y mejoras en multi-clúster

|2022
|Introducción de Rancher Prime (versión empresarial con soporte extendido)

|2023-2024
|Integración profunda con el ecosistema SUSE, mejoras en seguridad y compliance

|2025
|Rancher Manager 2.9 con características avanzadas de IA/ML y edge computing
|===

.Evolución arquitectónica:
* **Rancher 1.x**: Soportaba múltiples orquestadores (Cattle, Kubernetes, Swarm, Mesos)
* **Rancher 2.x**: Rediseño completo centrado exclusivamente en Kubernetes
* **Rancher Prime**: Versión empresarial con soporte comercial y características adicionales

==== Arquitectura y componentes principales

Rancher está diseñado con una arquitectura modular que permite gestionar múltiples clústeres Kubernetes desde una única interfaz.

.Diagrama de arquitectura de Rancher
[plantuml, rancher-architecture, png]
----
@startuml
!define RECTANGLE class

package "Rancher Management Server" {
  [Rancher API Server] as API
  [Authentication Provider] as Auth
  [Cluster Controller] as CC
  [User Controller] as UC
  database "etcd" as etcd
}

package "Downstream Cluster 1" {
  [Cluster Agent] as CA1
  [Node Agent] as NA1
  [Kubernetes API] as K8S1
}

package "Downstream Cluster 2" {
  [Cluster Agent] as CA2
  [Node Agent] as NA2
  [Kubernetes API] as K8S2
}

package "Cloud Providers" {
  [AWS EKS] as EKS
  [Azure AKS] as AKS
  [Google GKE] as GKE
}

actor Usuario as User

User --> API : "Gestiona clústeres"
API --> etcd : "Estado persistente"
API --> Auth : "Autenticación"
API --> CC : "Control de clústeres"
API --> UC : "Gestión usuarios"

CC --> CA1 : "Comunicación"
CC --> CA2 : "Comunicación"

CA1 --> K8S1
CA2 --> K8S2

API ..> EKS : "Provisioning"
API ..> AKS : "Provisioning"
API ..> GKE : "Provisioning"

@enduml
----

.Componentes principales del servidor Rancher:

**1. Rancher API Server**
* Punto de entrada para todas las operaciones de gestión
* Interfaz web y API RESTful
* Gestión de autenticación y autorización
* Proxy para acceso a clústeres downstream

**2. Authentication Provider**
* Integración con proveedores de identidad externos
* Soporte para LDAP, Active Directory, SAML, OAuth
* Gestión de tokens y sesiones
* RBAC (Role-Based Access Control)

**3. Cluster Controllers**
* Gestión del ciclo de vida de clústeres
* Sincronización de estado
* Health checking
* Comunicación con cluster agents

**4. etcd**
* Almacenamiento de estado del servidor Rancher
* Configuración de clústeres
* Datos de usuarios y permisos
* Metadatos de aplicaciones

.Componentes en clústeres downstream:

**1. Cluster Agent**
* Comunicación bidireccional con el servidor Rancher
* Sincronización de recursos
* Ejecución de comandos
* Reporte de métricas

**2. Node Agent**
* Desplegado en cada nodo del clúster
* Gestión de recursos del nodo
* Monitoreo de estado
* Ejecución de tareas de mantenimiento

**3. Fleet Agent (opcional)**
* Gestión de despliegues GitOps
* Sincronización de repositorios Git
* Multi-cluster deployments

==== Comparación con otras plataformas de gestión Kubernetes

.Comparativa de plataformas de gestión Kubernetes
[cols="2,2,2,2,2"]
|===
|Característica |Rancher |OpenShift |Tanzu |Lens

|Tipo
|Plataforma completa
|PaaS completa
|Suite empresarial
|Desktop IDE

|Código abierto
|Sí (Community)
|Sí (OKD)
|No
|Sí (Parcial)

|Multi-clúster
|✓ Excelente
|✓ Bueno
|✓ Bueno
|✓ Básico

|Multi-cloud
|✓ Nativo
|✓ Bueno
|✓ Limitado
|✓ Básico

|Curva de aprendizaje
|Media
|Alta
|Alta
|Baja

|Cost
|Gratuito/Suscripción
|Suscripción
|Suscripción
|Gratuito/Pro

|Integración CI/CD
|Fleet, Pipelines
|Jenkins, Tekton
|Concourse
|Limitado

|Marketplace
|Helm Charts
|Operators
|Tanzu Solutions
|Helm Charts

|Edge Computing
|K3s integrado
|Microshift
|Limitado
|No

|Soporte comercial
|SUSE Rancher Prime
|Red Hat
|VMware
|Mirantis
|===

**Ventajas de Rancher:**
* Interfaz intuitiva y fácil de usar
* Soporte para cualquier distribución de Kubernetes (EKS, AKS, GKE, RKE, K3s)
* Gestión multi-clúster desde una única consola
* Amplio ecosistema y comunidad activa
* Flexibilidad para entornos híbridos y multi-cloud
* K3s para edge computing y IoT

**Desventajas comparativas:**
* Menos características enterprise que OpenShift
* Requiere gestión del servidor Rancher adicional
* Menor integración con CI/CD nativo comparado con OpenShift

==== Casos de uso y beneficios empresariales

.Casos de uso principales:

**1. Gestión Multi-Clúster**
[source,text]
----
Escenario: Empresa con clústeres en AWS, Azure y on-premises
Beneficio: Gestión unificada desde una única interfaz
Resultado: Reducción del 60% en tiempo de operaciones
----

**2. Migración a Kubernetes**
* Importación gradual de clústeres existentes
* Estandarización de operaciones
* Formación de equipos con interfaz unificada
* Reducción de curva de aprendizaje

**3. Edge Computing e IoT**
[source,yaml]
----
Arquitectura típica:
- Datacenter central: Rancher Management Server
- Edge locations: K3s clusters (50-100 sites)
- Gestión centralizada con Fleet
- Despliegues automatizados vía GitOps
----

**4. Entornos de desarrollo y testing**
* Aprovisionamiento rápido de clústeres
* Catálogo de aplicaciones pre-configuradas
* Ambientes efímeros y reproducibles
* Integración con pipelines CI/CD

**5. Cumplimiento normativo y seguridad**
* Políticas de seguridad centralizadas
* Auditoría y logging centralizado
* Escaneo de vulnerabilidades
* Compliance automatizado (CIS benchmarks)

.Beneficios empresariales cuantificables:

[cols="2,3,1"]
|===
|Beneficio |Descripción |Impacto

|Reducción de costos operativos
|Gestión centralizada reduce personal necesario
|30-50%

|Time to market
|Despliegue más rápido de aplicaciones
|40-60%

|Reducción de errores
|Automatización y estandarización
|70%

|Utilización de recursos
|Mejor scheduling y optimización
|25-35%

|Seguridad mejorada
|Políticas centralizadas y auditoría
|N/A
|===

.Ejemplo real de implementación:
[source,text]
----
Empresa: Retailer global con 500 tiendas
Desafío: Gestionar aplicaciones en cada ubicación
Solución: Rancher + K3s + Fleet
Resultados:
- 500 clústeres K3s gestionados desde un Rancher central
- Despliegues automatizados en < 5 minutos
- Reducción de 80% en incidentes por configuración
- ROI positivo en 8 meses
----

**Industrias que se benefician especialmente:**
* Retail y comercio electrónico
* Manufactura e industria 4.0
* Telecomunicaciones
* Servicios financieros
* Gobierno y sector público
* Educación e investigación

=== 1.2 Arquitectura de Rancher

La arquitectura de Rancher está diseñada para proporcionar una gestión centralizada y escalable de múltiples clústeres Kubernetes. Comprender esta arquitectura es fundamental para implementar y operar Rancher de manera efectiva.

==== Componentes del plano de control

El plano de control de Rancher consiste en varios componentes que trabajan juntos para gestionar la infraestructura de Kubernetes.

.Arquitectura detallada del plano de control
[plantuml, rancher-control-plane, png]
----
@startuml
skinparam componentStyle rectangle

package "Rancher Server (Control Plane)" {
  component "Rancher API Server" as API {
    [REST API]
    [WebSocket API]
    [gRPC API]
  }
  
  component "Management Controllers" as MC {
    [Cluster Controller]
    [User Controller]
    [RBAC Controller]
    [Project Controller]
  }
  
  component "Authentication" as Auth {
    [Local Auth]
    [LDAP/AD]
    [SAML/OAuth]
    [GitHub/GitLab]
  }
  
  database "etcd Cluster" as etcd {
    [Cluster Config]
    [User Data]
    [RBAC Policies]
    [Settings]
  }
  
  component "Web UI" as UI
}

cloud "Load Balancer" as LB

LB --> API
UI --> API
API --> MC
API --> Auth
MC --> etcd
Auth --> etcd

@enduml
----

**1. Rancher API Server**

El API Server es el núcleo del plano de control y maneja todas las interacciones con Rancher.

.Funciones principales:
* **REST API**: Interfaz principal para operaciones CRUD
* **WebSocket**: Comunicación en tiempo real con la UI y cluster agents
* **Proxy**: Redirige peticiones a clústeres downstream
* **Validación**: Valida todas las peticiones entrantes
* **Autenticación**: Integra con proveedores de identidad

.Endpoints principales:
[source,text]
----
/v3/clusters              - Gestión de clústeres
/v3/projects              - Gestión de proyectos
/v3/users                 - Gestión de usuarios
/v3/catalogs              - Catálogos de aplicaciones
/k8s/clusters/<id>        - Proxy a Kubernetes API
----

**2. Management Controllers**

Los controladores gestionan el estado deseado vs el estado actual de los recursos.

.Cluster Controller:
[source,yaml]
----
Responsabilidades:
  - Provisioning de nuevos clústeres
  - Sincronización de estado
  - Health checking
  - Actualización de versiones
  - Gestión de nodos
----

.User & RBAC Controller:
* Sincroniza usuarios y permisos
* Gestiona roles globales y de clúster
* Propaga cambios de RBAC a clústeres downstream
* Mantiene consistencia de permisos

.Project Controller:
* Gestiona el concepto de "Project" (agrupación de namespaces)
* Aplica políticas a nivel de proyecto
* Gestiona cuotas de recursos por proyecto
* Controla el acceso multiusuario

**3. Authentication & Authorization**

Sistema de autenticación modular que soporta múltiples proveedores.

.Flujo de autenticación:
[plantuml, rancher-auth-flow, png]
----
@startuml
participant "Usuario" as U
participant "Rancher UI" as UI
participant "API Server" as API
participant "Auth Provider" as Auth
participant "etcd" as DB

U -> UI: Login
UI -> API: POST /v3/login
API -> Auth: Validate credentials
Auth -> Auth: Check LDAP/SAML/etc
Auth -> API: User info + groups
API -> DB: Store session token
API -> UI: Return token
UI -> U: Redirect to dashboard

@enduml
----

.Proveedores soportados:
[cols="2,3,2"]
|===
|Provider |Tipo |Uso común

|Local
|Username/Password
|Dev/Testing

|Active Directory
|LDAP
|Enterprise Windows

|OpenLDAP
|LDAP
|Enterprise Linux

|Azure AD
|SAML 2.0 / OAuth
|Microsoft cloud

|Okta
|SAML 2.0
|Identity management

|Keycloak
|OIDC
|Open source SSO

|GitHub
|OAuth 2.0
|Developer teams

|Google
|OAuth 2.0
|G Suite orgs
|===

**4. etcd Cluster**

Base de datos distribuida que almacena todo el estado de Rancher.

.Datos almacenados:
[source,text]
----
Clusters:
  - Configuración de clústeres downstream
  - Estado de conexión
  - Credenciales de acceso

Users & Auth:
  - Usuarios y tokens
  - Configuración de auth providers
  - Políticas RBAC

Catalogs:
  - Repositorios Helm
  - Apps instaladas
  - Versiones disponibles

Settings:
  - Configuración global
  - Feature flags
  - Telemetría
----

.Requisitos de etcd:
* Mínimo 3 nodos para HA
* SSD storage recomendado
* Backup automático cada 12 horas
* Snapshots antes de upgrades

==== Arquitectura multi-clúster

Rancher gestiona múltiples clústeres Kubernetes mediante una arquitectura hub-and-spoke.

.Modelo hub-and-spoke
[plantuml, rancher-hub-spoke, png]
----
@startuml
!define RECTANGLE class

cloud "Rancher Server\n(Hub)" as Hub {
  [Management Server]
}

package "Production Cluster" as Prod {
  [Cluster Agent] as CA1
  [Node Agent] as NA1
  [Fleet Agent] as FA1
}

package "Staging Cluster" as Stg {
  [Cluster Agent] as CA2
  [Node Agent] as NA2
  [Fleet Agent] as FA2
}

package "Development Cluster" as Dev {
  [Cluster Agent] as CA3
  [Node Agent] as NA3
  [Fleet Agent] as FA3
}

package "Edge Cluster (K3s)" as Edge {
  [Cluster Agent] as CA4
  [Fleet Agent] as FA4
}

Hub <--> CA1 : "WebSocket/HTTPS"
Hub <--> CA2 : "WebSocket/HTTPS"
Hub <--> CA3 : "WebSocket/HTTPS"
Hub <--> CA4 : "WebSocket/HTTPS"

CA1 --> NA1
CA2 --> NA2
CA3 --> NA3

Hub --> FA1 : "GitOps"
Hub --> FA2 : "GitOps"
Hub --> FA3 : "GitOps"
Hub --> FA4 : "GitOps"

@enduml
----

**Tipos de clústeres en Rancher:**

**1. Local Cluster (Management Cluster)**
* Clúster donde corre Rancher Server
* No debería usarse para workloads de aplicaciones
* Solo para componentes de gestión de Rancher
* Alta disponibilidad crítica

**2. Downstream Clusters (Managed Clusters)**
* Clústeres gestionados por Rancher
* Pueden ser importados o creados por Rancher
* Ejecutan workloads de aplicaciones
* Tipos:
  ** **RKE Clusters**: Creados por Rancher Kubernetes Engine
  ** **Imported Clusters**: EKS, AKS, GKE, etc.
  ** **Hosted Clusters**: Creados en cloud providers
  ** **Custom Clusters**: On-premises con nodos propios

**3. Arquitectura de agentes en clústeres downstream**

.Cluster Agent:
[source,yaml]
----
Deployment: cattle-cluster-agent
Namespace: cattle-system
Responsabilidades:
  - Comunicación bidireccional con Rancher Server
  - Sincronización de recursos (Projects, Users, RBAC)
  - Ejecución de comandos remotos
  - Reporte de métricas y eventos
  - Túnel para kubectl proxy

Configuración:
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
----

.Node Agent (rancher-agent):
[source,yaml]
----
DaemonSet: cattle-node-agent
Namespace: cattle-system
Responsabilidades:
  - Health check de nodos
  - Recolección de métricas del nodo
  - Ejecución de comandos en el nodo
  - Gestión de logs

Configuración:
  hostNetwork: true
  hostPID: true
  privileged: true
----

==== Comunicación entre componentes

La comunicación segura entre componentes es esencial para la arquitectura de Rancher.

.Flujo de comunicación completo
[plantuml, rancher-communication, png]
----
@startuml
participant "Admin/User" as User
participant "Rancher Server" as RS
participant "Cluster Agent" as CA
participant "Kubernetes API" as K8S
participant "Node Agent" as NA

User -> RS: kubectl command
RS -> RS: Authenticate user
RS -> RS: Check RBAC
RS -> CA: Forward via WebSocket tunnel
CA -> K8S: Execute kubectl
K8S -> CA: Return result
CA -> RS: Send response
RS -> User: Display result

User -> RS: View node metrics
RS -> CA: Request node info
CA -> NA: Get node status
NA -> CA: Return metrics
CA -> RS: Send metrics
RS -> User: Display dashboard

@enduml
----

**1. Protocolos de comunicación**

.User → Rancher Server:
* **HTTPS**: REST API calls (puerto 443)
* **WSS**: WebSocket para UI en tiempo real
* **Autenticación**: Token bearer o session cookie

.Rancher Server → Cluster Agent:
* **WebSocket**: Conexión bidireccional persistente
* **Iniciado por**: Cluster Agent (outbound desde el clúster)
* **Puerto**: 443 (HTTPS)
* **Ventaja**: No requiere inbound al clúster

.Cluster Agent → Kubernetes API:
* **HTTPS**: REST API
* **Puerto**: 6443 (típicamente)
* **Auth**: ServiceAccount token

**2. Modelo de seguridad**

.Certificados y TLS:
[source,yaml]
----
Rancher Server:
  - Certificado TLS para acceso HTTPS
  - Puede usar Let's Encrypt automático
  - O certificados personalizados/privados

Cluster Agents:
  - Validan certificado del Rancher Server
  - Usan tokens de registro para autenticación inicial
  - Posteriormente usan tokens rotativos

Comunicación interna:
  - Todo el tráfico encriptado con TLS 1.2+
  - Mutual TLS entre agentes y server
  - Tokens JWT para autenticación
----

**3. Modelo de túnel inverso**

Una de las características clave de Rancher es el túnel inverso que elimina la necesidad de exponer clústeres downstream.

.Ventajas del túnel inverso:
* No requiere abrir puertos de entrada en clústeres
* Cluster Agents inician conexiones outbound (puerto 443)
* Firewall-friendly para entornos corporativos
* Funciona en entornos con NAT y proxies

.Configuración de proxy para cluster agent:
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cattle-proxy
  namespace: cattle-system
data:
  HTTP_PROXY: "http://proxy.example.com:8080"
  HTTPS_PROXY: "http://proxy.example.com:8080"
  NO_PROXY: "localhost,127.0.0.1,10.0.0.0/8"
----

==== Alta disponibilidad y escalabilidad

Para entornos de producción, Rancher debe desplegarse en configuración de alta disponibilidad.

.Arquitectura de alta disponibilidad
[plantuml, rancher-ha, png]
----
@startuml
skinparam componentStyle rectangle

cloud "Load Balancer\n(Layer 4/7)" as LB

package "Rancher HA Cluster" {
  node "Control Node 1" {
    [Rancher Pod 1]
    [etcd 1]
  }
  
  node "Control Node 2" {
    [Rancher Pod 2]
    [etcd 2]
  }
  
  node "Control Node 3" {
    [Rancher Pod 3]
    [etcd 3]
  }
}

database "Shared Storage\n(Optional)" as Storage

LB --> [Rancher Pod 1]
LB --> [Rancher Pod 2]
LB --> [Rancher Pod 3]

[Rancher Pod 1] --> [etcd 1]
[Rancher Pod 2] --> [etcd 2]
[Rancher Pod 3] --> [etcd 3]

[etcd 1] <--> [etcd 2]
[etcd 2] <--> [etcd 3]
[etcd 3] <--> [etcd 1]

[Rancher Pod 1] ..> Storage
[Rancher Pod 2] ..> Storage
[Rancher Pod 3] ..> Storage

@enduml
----

**1. Requisitos de alta disponibilidad**

.Configuración mínima de HA:
[cols="2,3,2"]
|===
|Componente |Requisito |Recomendación

|Nodos
|Mínimo 3
|5 para mejor tolerancia

|CPU por nodo
|2 cores
|4 cores

|RAM por nodo
|8 GB
|16 GB

|Disco (etcd)
|50 GB SSD
|100 GB NVMe

|Load Balancer
|Layer 4 o Layer 7
|Layer 7 con health checks

|Red
|1 Gbps
|10 Gbps
|===

**2. Configuración de HA con RKE**

.cluster.yml para despliegue HA:
[source,yaml]
----
nodes:
  - address: 10.0.0.10
    hostname_override: rancher-1
    user: ubuntu
    role:
      - controlplane
      - etcd
      - worker
  
  - address: 10.0.0.11
    hostname_override: rancher-2
    user: ubuntu
    role:
      - controlplane
      - etcd
      - worker
  
  - address: 10.0.0.12
    hostname_override: rancher-3
    user: ubuntu
    role:
      - controlplane
      - etcd
      - worker

services:
  etcd:
    backup_config:
      enabled: true
      interval_hours: 12
      retention: 6
      s3_backup_config:
        access_key: "your-access-key"
        bucket_name: "rancher-backups"
        endpoint: "s3.amazonaws.com"
        region: "us-west-2"
    
    snapshot: true
    creation: 12h
    retention: 72h

  kube-api:
    service_cluster_ip_range: 10.43.0.0/16
    service_node_port_range: 30000-32767
    pod_security_policy: false

  kube-controller:
    cluster_cidr: 10.42.0.0/16
    service_cluster_ip_range: 10.43.0.0/16

  kubelet:
    cluster_domain: cluster.local
    cluster_dns_server: 10.43.0.10
    fail_swap_on: false

network:
  plugin: canal
  options:
    flannel_backend_type: vxlan

ingress:
  provider: nginx
  options:
    use-forwarded-headers: "true"

# Certificados
certificates:
  - CN: rancher.example.com
    hosts:
      - rancher.example.com
      - 10.0.0.10
      - 10.0.0.11
      - 10.0.0.12
----

**3. Configuración del Load Balancer**

.NGINX como Load Balancer (Layer 4):
[source,nginx]
----
stream {
    upstream rancher_servers {
        least_conn;
        server 10.0.0.10:443 max_fails=3 fail_timeout=5s;
        server 10.0.0.11:443 max_fails=3 fail_timeout=5s;
        server 10.0.0.12:443 max_fails=3 fail_timeout=5s;
    }
    
    server {
        listen 443;
        proxy_pass rancher_servers;
        proxy_timeout 900s;
        proxy_connect_timeout 15s;
    }
    
    upstream rancher_servers_http {
        least_conn;
        server 10.0.0.10:80 max_fails=3 fail_timeout=5s;
        server 10.0.0.11:80 max_fails=3 fail_timeout=5s;
        server 10.0.0.12:80 max_fails=3 fail_timeout=5s;
    }
    
    server {
        listen 80;
        proxy_pass rancher_servers_http;
    }
}
----

.HAProxy como Load Balancer:
[source,haproxy]
----
global
    log /dev/log local0
    log /dev/log local1 notice
    maxconn 4096

defaults
    log global
    mode tcp
    option tcplog
    option dontlognull
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend rancher_https
    bind *:443
    mode tcp
    default_backend rancher_backend_https

backend rancher_backend_https
    mode tcp
    balance roundrobin
    option httpchk GET /healthz
    http-check expect status 200
    server rancher1 10.0.0.10:443 check
    server rancher2 10.0.0.11:443 check
    server rancher3 10.0.0.12:443 check

frontend rancher_http
    bind *:80
    mode tcp
    default_backend rancher_backend_http

backend rancher_backend_http
    mode tcp
    balance roundrobin
    server rancher1 10.0.0.10:80 check
    server rancher2 10.0.0.11:80 check
    server rancher3 10.0.0.12:80 check
----

**4. Estrategias de escalabilidad**

.Escalabilidad vertical (Scale Up):
* Aumentar recursos de nodos existentes
* Útil hasta cierto límite
* Requiere downtime para cada nodo

.Escalabilidad horizontal (Scale Out):
[source,bash]
----
# Añadir nodos al clúster RKE
# 1. Añadir nodo al cluster.yml
# 2. Ejecutar RKE
rke up --config cluster.yml

# Para Rancher en Kubernetes
kubectl scale deployment rancher -n cattle-system --replicas=5
----

.Límites de escalabilidad:
[cols="2,2,3"]
|===
|Recurso |Límite |Notas

|Clústeres downstream
|2000+
|Depende de recursos

|Nodos totales gestionados
|100,000+
|Distribuidos en clústeres

|Usuarios concurrentes
|10,000+
|Con HA adecuado

|Projects
|Ilimitados
|Por lógica de namespace

|Workloads por clúster
|15,000+
|Límite de Kubernetes
|===

**5. Monitoreo de HA**

.Métricas clave a monitorear:
[source,yaml]
----
Rancher Server:
  - CPU/Memory utilization
  - Request latency
  - WebSocket connections
  - API error rates

etcd:
  - Latency de operaciones
  - Tamaño del database
  - Éxito de snapshots
  - Sync duration

Load Balancer:
  - Health check status
  - Connection count
  - Request distribution
  - Response times
----

.Health checks recomendados:
[source,bash]
----
# Check Rancher API
curl -k https://rancher.example.com/healthz

# Check etcd cluster
kubectl -n kube-system exec -it etcd-rancher-1 -- \
  etcdctl --endpoints=https://127.0.0.1:2379 \
  --cert=/etc/kubernetes/ssl/kube-etcd-*.pem \
  --key=/etc/kubernetes/ssl/kube-etcd-*-key.pem \
  --cacert=/etc/kubernetes/ssl/kube-ca.pem \
  endpoint health

# Check cluster connectivity
kubectl get nodes
kubectl get pods -n cattle-system
----

=== 1.3 Versiones y ediciones de Rancher

Rancher ofrece diferentes ediciones y modelos de soporte para adaptarse a las necesidades de distintas organizaciones, desde proyectos de código abierto hasta implementaciones empresariales críticas.

==== Rancher Prime vs Community Edition

SUSE ofrece dos ediciones principales de Rancher, cada una diseñada para diferentes escenarios de uso y niveles de soporte.

.Comparativa entre ediciones
[cols="2,3,3"]
|===
|Característica |Rancher Prime |Community Edition

|**Licencia**
|Suscripción comercial SUSE
|Apache 2.0 (Open Source)

|**Código fuente**
|Mismo código base
|Mismo código base

|**Soporte técnico**
|24x7x365 con SLA
|Comunidad (foros, Slack)

|**Actualizaciones**
|Releases estables y LTS
|Releases upstream

|**Parches de seguridad**
|Backports garantizados
|Best effort

|**Ciclo de vida**
|18 meses soporte estándar
|6 meses típico

|**Soporte LTS**
|Hasta 24 meses
|No disponible

|**Certificación**
|Certificado para producción
|Sin certificación formal

|**Documentación**
|Guías empresariales + KB
|Documentación pública

|**Training**
|Cursos oficiales incluidos
|Recursos comunitarios

|**Compliance**
|CVE tracking y reportes
|Comunitario

|**Precio**
|Por nodo/clúster gestionado
|Gratuito
|===

**Rancher Prime (Suscripción Empresarial)**

Rancher Prime es la versión empresarial con soporte comercial completo de SUSE.

.Beneficios clave:
* **Soporte técnico premium**: 24x7x365 con tiempos de respuesta garantizados por SLA
* **Lifecycle extendido**: Soporte hasta 24 meses por versión LTS
* **Parches de seguridad**: CVE patches backported a versiones soportadas
* **Certificación y compliance**: Validado para entornos regulados
* **Roadmap prioritario**: Influencia en el desarrollo futuro
* **Professional services**: Acceso a servicios de consultoría

.Niveles de soporte Prime:
[cols="2,2,2,2"]
|===
|Nivel |Respuesta P1 |Respuesta P2 |Disponibilidad

|Standard
|4 horas
|8 horas
|24x7

|Premium
|1 hora
|4 horas
|24x7

|Enterprise
|30 minutos
|2 horas
|24x7 + TAM

|Mission Critical
|15 minutos
|1 hora
|24x7 + TAM dedicado
|===

.Modelo de precios (estimado):
[source,text]
----
Opción 1: Por nodo gestionado
  - $100-300 USD/nodo/año
  - Incluye todos los nodos en clústeres downstream
  
Opción 2: Por clúster
  - $5,000-15,000 USD/clúster/año
  - Sin límite de nodos por clúster
  
Opción 3: Unlimited
  - Precio negociado según volumen
  - Para > 1000 nodos o > 50 clústeres
----

**Rancher Community Edition**

Versión gratuita y de código abierto ideal para desarrollo, testing y organizaciones con capacidad interna de soporte.

.Características:
* Todas las funcionalidades técnicas de Rancher
* Actualizaciones frecuentes (mensual típicamente)
* Soporte comunitario vía foros y Slack
* Sin compromisos de SLA
* Ideal para: Dev/Test, POCs, startups, homelab

.Limitaciones comparativas:
* Sin garantía de parches de seguridad backported
* Ciclo de vida de soporte más corto
* Sin acceso a training empresarial
* Sin servicios profesionales incluidos

**¿Cuándo elegir cada edición?**

.Rancher Prime es recomendado para:
* Entornos de producción críticos
* Organizaciones reguladas (finanzas, salud, gobierno)
* Equipos pequeños sin expertise profundo en Kubernetes
* Necesidad de compliance y auditoría
* SLAs contractuales requeridos
* Más de 3 clústeres en producción

.Community Edition es adecuado para:
* Entornos de desarrollo y testing
* Equipos con fuerte expertise en Kubernetes
* POCs y evaluaciones
* Proyectos open source
* Presupuesto limitado con recursos internos

==== Matriz de soporte y versiones compatibles

Rancher soporta múltiples versiones de Kubernetes y tiene un ciclo de vida de versiones bien definido.

.Matriz de compatibilidad Rancher 2.9.x (Oct 2025)
[cols="2,3,2,2"]
|===
|Rancher Version |Kubernetes Versions |Release Date |EOL Date

|2.9.3 (Latest)
|1.29.x, 1.28.x, 1.27.x
|Oct 2025
|Apr 2027 (Prime)

|2.9.2
|1.29.x, 1.28.x, 1.27.x
|Aug 2025
|Feb 2027 (Prime)

|2.9.1
|1.28.x, 1.27.x, 1.26.x
|Jun 2025
|Dec 2026 (Prime)

|2.9.0
|1.28.x, 1.27.x, 1.26.x
|Apr 2025
|Oct 2026 (Prime)

|2.8.7 (LTS)
|1.28.x, 1.27.x, 1.26.x
|Mar 2025
|Sep 2026 (Prime)

|2.8.6
|1.27.x, 1.26.x, 1.25.x
|Jan 2025
|Jul 2026 (Prime)

|2.7.x (Legacy)
|1.26.x, 1.25.x, 1.24.x
|2024
|EOL Mar 2025
|===

**Soporte de Distribuciones Kubernetes**

Rancher es compatible con múltiples distribuciones y proveedores de Kubernetes.

.Distribuciones soportadas:
[cols="2,2,3"]
|===
|Distribución |Tipo |Notas

|RKE (Rancher Kubernetes Engine)
|Nativo
|Creado y gestionado por Rancher

|RKE2
|Nativo
|Certificado FIPS, CIS hardened

|K3s
|Ligero
|Para edge, IoT, ARM

|Amazon EKS
|Cloud
|Importación y provisioning

|Azure AKS
|Cloud
|Importación y provisioning

|Google GKE
|Cloud
|Importación y provisioning

|Red Hat OpenShift
|Enterprise
|Solo importación (limitado)

|VMware Tanzu
|Enterprise
|Importación

|Vanilla Kubernetes
|Custom
|Cualquier instalación estándar

|Minikube/Kind
|Dev
|Para desarrollo local
|===

**Compatibilidad de componentes**

.Versiones mínimas requeridas (Rancher 2.9.x):
[cols="2,2,3"]
|===
|Componente |Versión mínima |Recomendado

|Docker
|20.10.x
|24.x

|containerd
|1.6.x
|1.7.x

|Kubernetes
|1.26.x
|1.28.x o 1.29.x

|Helm
|3.10
|3.14+

|kubectl
|1.26
|Matching K8s version

|etcd
|3.5.x
|3.5.10+

|cert-manager
|1.11.x
|1.14.x

|Ingress NGINX
|1.8.x
|1.10.x
|===

**Requisitos de navegadores web**

.Soporte de navegadores para Rancher UI:
[cols="2,2,2"]
|===
|Navegador |Versión mínima |Notas

|Chrome
|90+
|Recomendado

|Firefox
|88+
|Totalmente soportado

|Safari
|14+
|macOS/iOS

|Edge (Chromium)
|90+
|Totalmente soportado

|Edge (Legacy)
|N/A
|No soportado
|===

==== Políticas de actualización y soporte

SUSE mantiene políticas claras sobre el ciclo de vida y soporte de las versiones de Rancher.

**Ciclo de vida de versiones**

.Fases del ciclo de vida
[plantuml, rancher-lifecycle, png]
----
@startuml
!define RECTANGLE rectangle

RECTANGLE "Development" as Dev #LightBlue
RECTANGLE "Beta Testing" as Beta #LightYellow
RECTANGLE "General Availability\n(GA)" as GA #LightGreen
RECTANGLE "Maintenance\n(Security patches)" as Maint #Orange
RECTANGLE "Extended Support\n(Prime only)" as Extended #Gold
RECTANGLE "End of Life\n(EOL)" as EOL #Red

Dev --> Beta : "Feature complete"
Beta --> GA : "Stable release"
GA --> Maint : "6 months"
Maint --> Extended : "Prime customers"
Maint --> EOL : "Community"
Extended --> EOL : "12-18 months"

note right of GA
  Full support
  New features
  Bug fixes
  Security patches
end note

note right of Maint
  Security patches only
  Critical bug fixes
  No new features
end note

note right of Extended
  CVE patches
  Consulting available
  Premium support
end note

@enduml
----

**Política de versiones semánticas**

Rancher sigue versionado semántico (SemVer): MAJOR.MINOR.PATCH

.Significado de cada nivel:
[source,text]
----
MAJOR (2.x.x):
  - Cambios incompatibles en la API
  - Cambios arquitectónicos significativos
  - Requiere planificación de migración

MINOR (2.9.x):
  - Nuevas características compatibles
  - Mejoras de funcionalidad
  - Deprecations anunciadas

PATCH (2.9.3):
  - Bug fixes
  - Security patches
  - Sin cambios de funcionalidad
----

**Cadencia de releases**

.Calendario típico de lanzamientos:
[cols="2,2,3"]
|===
|Tipo de release |Frecuencia |Ejemplo

|Major release
|18-24 meses
|2.0 → 2.x

|Minor release
|3-4 meses
|2.8 → 2.9

|Patch release
|2-4 semanas
|2.9.2 → 2.9.3

|Security patch
|As needed
|CVE response

|LTS release
|12 meses
|2.8 LTS
|===

**Estrategia de actualización**

.Path de actualización recomendado:
[source,text]
----
Regla general:
- Actualizar de PATCH a PATCH: Directo (ej: 2.9.1 → 2.9.3)
- Actualizar de MINOR a MINOR: Una versión (ej: 2.8.x → 2.9.x)
- Actualizar de MAJOR a MAJOR: Requiere planificación

Ejemplo de path correcto:
  2.7.9 → 2.8.5 → 2.9.3 ✓

Ejemplo de path incorrecto:
  2.7.9 → 2.9.3 ✗ (skip minor version)
----

.Proceso de actualización recomendado:
[source,bash]
----
# 1. Backup del clúster Rancher
./rke-backup.sh

# 2. Verificar versión actual
kubectl -n cattle-system get deploy rancher -o jsonpath='{.spec.template.spec.containers[0].image}'

# 3. Revisar release notes
# https://github.com/rancher/rancher/releases

# 4. Actualizar Rancher via Helm
helm repo update
helm upgrade rancher rancher-stable/rancher \
  --namespace cattle-system \
  --version=2.9.3 \
  --reuse-values

# 5. Verificar actualización
kubectl -n cattle-system rollout status deploy/rancher
kubectl -n cattle-system get pods

# 6. Verificar funcionalidad
curl -k https://rancher.example.com/healthz
----

**Deprecations y removals**

SUSE anuncia deprecations con anticipación siguiendo esta política:

.Política de deprecation:
* **Anuncio**: Mínimo 2 minor releases antes de removal
* **Warning**: Logs y UI muestran advertencias
* **Grace period**: Mínimo 12 meses para Prime customers
* **Migration guide**: Documentación de migración provista

.Ejemplo reciente de deprecations:
[cols="2,2,2,2"]
|===
|Feature |Deprecated |Removed |Alternativa

|Legacy Monitoring
|2.7.0
|2.9.0
|Rancher Monitoring v2

|Rancher CLI v1
|2.8.0
|TBD
|Rancher CLI v2

|Projects (partial)
|TBD
|TBD
|Namespaces + RBAC

|Cluster Templates
|2.6.0
|2.8.0
|Fleet + GitOps
|===

**Soporte de Kubernetes upstream**

Rancher soporta versiones de Kubernetes siguiendo la política oficial de Kubernetes.

.Política de soporte K8s:
[source,text]
----
Kubernetes mantiene:
  - 3 minor versions activas
  - ~14 meses de soporte por version

Rancher mantiene:
  - Mínimo 3 minor versions de K8s
  - Añade soporte de nuevas versiones en 30-60 días
  - Extiende soporte para Prime customers

Ejemplo (Oct 2025):
  K8s upstream soporta: 1.29, 1.28, 1.27
  Rancher 2.9 soporta: 1.29, 1.28, 1.27, 1.26 (extended)
----

**Testing y certificación**

.Proceso de certificación Prime:
* Testing en múltiples cloud providers
* Validación de upgrade paths
* Security scanning y penetration testing
* Performance benchmarking
* Compliance validation (CIS, FIPS, etc.)
* Documentación de known issues

**Recursos de actualización**

.Herramientas y documentación:
* **Release Notes**: https://github.com/rancher/rancher/releases
* **Upgrade Guide**: Documentación oficial de SUSE
* **Known Issues**: KB articles por versión
* **Compatibility Matrix**: Tabla de compatibilidad actualizada
* **Migration Tools**: Scripts de migración si aplican

.Checklist pre-actualización:
[source,checklist]
----
☐ Revisar release notes y breaking changes
☐ Verificar compatibilidad de Kubernetes versions
☐ Backup completo de Rancher server
☐ Backup de clústeres downstream críticos
☐ Verificar espacio en disco suficiente
☐ Programar ventana de mantenimiento
☐ Notificar a usuarios sobre downtime potencial
☐ Preparar rollback plan
☐ Verificar health de etcd cluster
☐ Documentar versiones actuales de todos los componentes
☐ Revisar custom configurations que puedan afectarse
☐ Testing en ambiente no-productivo primero
----

**Opciones de soporte técnico**

.Canales de soporte Prime:
* **Portal web**: Casos 24x7 con tracking
* **Teléfono**: Línea directa para incidentes críticos
* **Chat**: Support chat en horario laboral
* **Email**: Respuesta garantizada por SLA
* **TAM**: Technical Account Manager (tiers superiores)

.Canales de soporte Community:
* **GitHub Issues**: https://github.com/rancher/rancher/issues
* **Forums**: https://forums.rancher.com/
* **Slack**: https://slack.rancher.io/
* **Stack Overflow**: Tag `rancher`
* **Documentation**: https://ranchermanager.docs.rancher.com/

== Módulo 2: Instalación y Configuración

=== 2.1 Requisitos del sistema

Antes de instalar Rancher, es fundamental comprender y preparar los requisitos de hardware, software, red y compatibilidad necesarios para garantizar un despliegue exitoso y estable.

==== Requisitos de hardware y software

Los requisitos varían significativamente según el tipo de instalación (desarrollo vs producción) y el número de clústeres a gestionar.

**Requisitos para Rancher Server**

.Configuración de desarrollo (Docker single-node):
[cols="2,3"]
|===
|Recurso |Especificación

|CPU
|2 cores

|RAM
|4 GB mínimo, 8 GB recomendado

|Disco
|50 GB

|OS
|Ubuntu 20.04+, RHEL/CentOS 8+, SLES 15+

|Docker
|20.10.x o superior

|Arquitectura
|x86_64 / AMD64
|===

.Configuración de producción (HA con 3 nodos):
[cols="2,2,2"]
|===
|Recurso |Mínimo (por nodo) |Recomendado (por nodo)

|CPU
|2 cores
|4-8 cores

|RAM
|8 GB
|16-32 GB

|Disco (OS)
|50 GB SSD
|100 GB SSD

|Disco (etcd)
|50 GB SSD
|100-200 GB NVMe

|IOPS (etcd)
|3000 IOPS
|5000+ IOPS

|Network
|1 Gbps
|10 Gbps

|Latencia entre nodos
|< 10ms
|< 5ms
|===

.Escalabilidad según número de clústeres gestionados:
[cols="2,2,2,2"]
|===
|Clústeres downstream |CPU (total) |RAM (total) |Nodos recomendados

|1-5
|8 cores
|32 GB
|3

|5-15
|16 cores
|64 GB
|3

|15-50
|32 cores
|128 GB
|3-5

|50-100
|64 cores
|256 GB
|5-7

|100+
|Escalamiento horizontal
|Según carga
|5+
|===

**Sistemas operativos soportados**

.Rancher Server:
[cols="2,2,3"]
|===
|OS |Versiones |Notas

|Ubuntu
|20.04 LTS, 22.04 LTS, 24.04 LTS
|Recomendado

|RHEL/CentOS
|8.x, 9.x
|Totalmente soportado

|SLES
|15 SP3+
|Con Rancher Prime

|Rocky Linux
|8.x, 9.x
|Compatible

|Oracle Linux
|8.x, 9.x
|Compatible

|Debian
|11, 12
|Comunidad

|Amazon Linux 2
|2023
|Para AWS
|===

**Container Runtime soportados**

.Para clústeres RKE2/K3s:
[cols="2,2,3"]
|===
|Runtime |Versión |Notas

|containerd
|1.6.x, 1.7.x
|Recomendado

|Docker
|20.10.x, 23.x, 24.x
|Para RKE1

|CRI-O
|1.26+
|Soporte limitado
|===

**Requisitos de software adicional**

.Herramientas necesarias:
[source,bash]
----
# Rancher Server (HA installation)
- Kubernetes cluster: v1.26+
- Helm: v3.10+
- kubectl: v1.26+
- cert-manager: v1.11+

# Rancher Server (Docker installation)
- Docker: 20.10+
- docker-compose (opcional): v2.x

# Clústeres downstream (RKE)
- Docker: 20.10+
- SSH access a todos los nodos

# Clústeres downstream (RKE2/K3s)
- containerd (incluido)
- Ningún prerequisito adicional
----

==== Configuraciones de red necesarias

La red es uno de los aspectos más críticos para el correcto funcionamiento de Rancher.

**Puertos requeridos para Rancher Server**

.Puertos de acceso externo:
[cols="1,1,3"]
|===
|Puerto |Protocolo |Descripción

|80
|TCP
|HTTP (redirige a 443)

|443
|TCP
|HTTPS - Acceso principal a Rancher UI y API

|6443
|TCP
|Kubernetes API (si Rancher en K8s)
|===

.Puertos entre nodos Rancher (HA):
[cols="1,1,3"]
|===
|Puerto |Protocolo |Descripción

|2379-2380
|TCP
|etcd client y peer communication

|6443
|TCP
|Kubernetes API server

|8472
|UDP
|Canal/Flannel VXLAN overlay

|9099
|TCP
|Canal/Flannel livenessProbe/readinessProbe

|10250
|TCP
|kubelet API

|10254
|TCP
|Ingress controller livenessProbe/readinessProbe

|30000-32767
|TCP/UDP
|NodePort range (si se usa)
|===

**Puertos para clústeres downstream**

.Comunicación Rancher → Clústeres downstream:
[cols="1,1,1,3"]
|===
|Puerto |Protocolo |Dirección |Descripción

|443
|TCP
|Outbound
|Cluster agents → Rancher Server

|6443
|TCP
|Bidireccional
|kubectl proxy para K8s API

|22
|TCP
|Outbound (opcional)
|SSH para node provisioning
|===

IMPORTANT: Los cluster agents inician conexiones OUTBOUND hacia Rancher Server en puerto 443. No se requieren conexiones inbound a los clústeres downstream.

**Requisitos de DNS**

.DNS necesario:
[source,text]
----
Rancher Server:
  - FQDN público/privado: rancher.example.com
  - Resuelve a IP del Load Balancer o nodos Rancher
  - Certificado SSL debe coincidir con FQDN

Downstream clusters:
  - No requieren DNS público
  - DNS interno para resolución entre nodos
  - CoreDNS/kube-dns en el clúster
----

.Ejemplo de configuración DNS:
[source,dns]
----
# Zona DNS pública
rancher.example.com.      IN  A     203.0.113.10
rancher.example.com.      IN  A     203.0.113.11
rancher.example.com.      IN  A     203.0.113.12

# O con Load Balancer
rancher.example.com.      IN  CNAME lb.example.com.
lb.example.com.           IN  A     203.0.113.100
----

**Configuración de Firewall**

.Reglas de firewall típicas (iptables):
[source,bash]
----
# Permitir acceso HTTPS a Rancher Server
iptables -A INPUT -p tcp --dport 443 -j ACCEPT
iptables -A INPUT -p tcp --dport 80 -j ACCEPT

# Permitir comunicación entre nodos Rancher (ejemplo para etcd)
iptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 2379:2380 -j ACCEPT
iptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 6443 -j ACCEPT

# Permitir tráfico Kubernetes interno
iptables -A INPUT -s 10.42.0.0/16 -j ACCEPT
iptables -A INPUT -s 10.43.0.0/16 -j ACCEPT

# Permitir outbound a internet (para cluster agents)
iptables -A OUTPUT -p tcp --dport 443 -j ACCEPT
----

**Proxy y entornos corporativos**

Para entornos con proxy HTTP(S):

.Configuración de proxy para Rancher:
[source,bash]
----
# Variables de entorno para Rancher Server
HTTP_PROXY=http://proxy.corp.com:8080
HTTPS_PROXY=http://proxy.corp.com:8080
NO_PROXY=localhost,127.0.0.1,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local

# Para Docker
cat >> /etc/systemd/system/docker.service.d/http-proxy.conf <<EOF
[Service]
Environment="HTTP_PROXY=http://proxy.corp.com:8080"
Environment="HTTPS_PROXY=http://proxy.corp.com:8080"
Environment="NO_PROXY=localhost,127.0.0.1,10.0.0.0/8"
EOF

systemctl daemon-reload
systemctl restart docker
----

**Rangos de red recomendados**

.Rangos CIDR estándar:
[cols="2,2,3"]
|===
|Red |CIDR |Uso

|Pod network
|10.42.0.0/16
|IPs para pods (Canal/Flannel)

|Service network
|10.43.0.0/16
|ClusterIP services

|Node network
|Depende de infra
|IPs de nodos físicos/VMs

|Ingress
|Variable
|LoadBalancer o NodePort
|===

CAUTION: Asegúrate de que los rangos de red no se solapen con la red corporativa existente.

==== Compatibilidad con proveedores cloud

Rancher es compatible con los principales proveedores de cloud y ofrece integración nativa para provisioning de clústeres.

**Amazon Web Services (AWS)**

.Requisitos para AWS:
[cols="2,3"]
|===
|Requisito |Detalles

|Credenciales
|Access Key ID + Secret Access Key con permisos EC2, IAM

|VPC
|VPC existente o crear nueva

|Subnets
|Mínimo 1 subnet, recomendado 3 (multi-AZ)

|Security Groups
|Permitir puertos requeridos

|IAM Roles
|Roles para nodos con políticas adecuadas

|EBS volumes
|Para storage persistente
|===

.Servicios AWS compatibles:
* **EC2**: Para nodos de clústeres RKE
* **EKS**: Importación y gestión de clústeres EKS existentes
* **EBS**: Storage backend para PersistentVolumes
* **ELB/ALB**: Load balancers para ingress
* **Route53**: DNS management
* **IAM**: IRSA (IAM Roles for Service Accounts)

.Ejemplo de política IAM mínima:
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:DescribeInstances",
        "ec2:DescribeRegions",
        "ec2:DescribeSecurityGroups",
        "ec2:DescribeSubnets",
        "ec2:DescribeVolumes",
        "ec2:CreateSecurityGroup",
        "ec2:CreateTags",
        "ec2:CreateVolume",
        "ec2:RunInstances",
        "ec2:TerminateInstances",
        "iam:PassRole",
        "iam:CreateServiceLinkedRole"
      ],
      "Resource": "*"
    }
  ]
}
----

**Microsoft Azure**

.Requisitos para Azure:
[cols="2,3"]
|===
|Requisito |Detalles

|Credenciales
|Service Principal (Client ID + Secret) o Managed Identity

|Resource Group
|Grupo de recursos para el clúster

|Virtual Network
|VNet con subnets adecuadas

|Network Security Groups
|NSG con reglas requeridas

|Azure Disk
|Para storage persistente
|===

.Servicios Azure compatibles:
* **Azure VM**: Para nodos de clústeres RKE
* **AKS**: Importación y gestión de clústeres AKS
* **Azure Disk**: Storage backend
* **Azure Load Balancer**: Para ingress
* **Azure DNS**: DNS management
* **Azure AD**: Autenticación

.Crear Service Principal para Rancher:
[source,bash]
----
# Crear service principal
az ad sp create-for-rbac --name RancherSP --role Contributor

# Output:
{
  "appId": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "displayName": "RancherSP",
  "password": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "tenant": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
}

# Asignar rol adicional para AKS
az role assignment create \
  --assignee <appId> \
  --role "Azure Kubernetes Service Cluster Admin Role"
----

**Google Cloud Platform (GCP)**

.Requisitos para GCP:
[cols="2,3"]
|===
|Requisito |Detalles

|Credenciales
|Service Account JSON key

|Project
|GCP Project ID

|VPC Network
|Red VPC con subnets

|Firewall Rules
|Reglas para puertos requeridos

|Persistent Disk
|Para storage
|===

.Servicios GCP compatibles:
* **Compute Engine**: Para nodos de clústeres RKE
* **GKE**: Importación y gestión de clústeres GKE
* **Persistent Disk**: Storage backend
* **Cloud Load Balancing**: Para ingress
* **Cloud DNS**: DNS management
* **Cloud IAM**: Autenticación

.Crear Service Account para Rancher:
[source,bash]
----
# Crear service account
gcloud iam service-accounts create rancher-sa \
  --display-name "Rancher Service Account"

# Asignar roles necesarios
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:rancher-sa@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/compute.admin"

gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:rancher-sa@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/container.admin"

# Generar key
gcloud iam service-accounts keys create rancher-key.json \
  --iam-account=rancher-sa@PROJECT_ID.iam.gserviceaccount.com
----

**Otros proveedores cloud**

.Compatibilidad adicional:
[cols="2,2,2"]
|===
|Provider |Soporte |Notas

|DigitalOcean
|Completo
|DOKS + Droplets

|Linode
|Completo
|LKE + VMs

|Vultr
|Vía custom
|Custom node driver

|OVH Cloud
|Vía custom
|Custom node driver

|OpenStack
|Completo
|Node driver nativo

|VMware vSphere
|Completo
|Node driver nativo

|Nutanix
|Completo
|Node driver nativo
|===

==== Requisitos para entornos air-gapped

Los entornos air-gapped (sin acceso a internet) requieren preparación adicional para descargar y alojar todos los artefactos necesarios.

**Concepto de air-gapped**

Un entorno air-gapped es aquel que no tiene conectividad directa a internet por razones de seguridad, compliance o restricciones de red.

.Arquitectura air-gapped
[plantuml, rancher-airgap, png]
----
@startuml
package "Internet Zone" {
  cloud "Docker Hub" as DH
  cloud "GitHub Releases" as GH
  cloud "Helm Charts" as HC
}

package "DMZ / Bastion" {
  [Workstation\ncon acceso] as WS
}

package "Air-Gapped Network" {
  [Private Registry] as PR
  [Rancher Server] as RS
  [Downstream Clusters] as DC
}

DH --> WS : "Pull images"
GH --> WS : "Download binaries"
HC --> WS : "Download charts"

WS --> PR : "Push images\n(offline transfer)"
PR --> RS : "Pull images"
RS --> DC : "Deploy"

@enduml
----

**Componentes necesarios en air-gapped**

.Lista de artefactos requeridos:
[source,text]
----
1. Imágenes Docker/Container:
   - Rancher server images
   - Rancher agent images
   - System images (nginx, coredns, etc.)
   - Monitoring stack images (Prometheus, Grafana)
   - Logging stack images (Fluentd, Elasticsearch)

2. Helm charts:
   - Rancher chart
   - cert-manager chart
   - Monitoring charts
   - Logging charts

3. Binaries:
   - Rancher CLI
   - kubectl
   - helm
   - RKE/RKE2 binaries

4. Certificados SSL:
   - Certificados para Rancher
   - CA certificates si son custom
----

**Private Docker Registry**

Es obligatorio tener un registry privado accesible desde el entorno air-gapped.

.Opciones de registry:
* **Harbor**: Open source, features completas, recomendado
* **Docker Registry**: Básico, open source
* **Nexus Repository**: Multi-format repository manager
* **Artifactory**: Enterprise registry
* **Cloud provider registries**: ACR, ECR, GCR (si accesibles)

.Deploy Harbor como private registry:
[source,bash]
----
# Instalar Harbor con docker-compose
wget https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-offline-installer-v2.10.0.tgz
tar xvf harbor-offline-installer-v2.10.0.tgz
cd harbor

# Configurar harbor.yml
cp harbor.yml.tmpl harbor.yml
vim harbor.yml
# Modificar:
#   hostname: registry.airgap.local
#   https.certificate: /path/to/cert.crt
#   https.private_key: /path/to/cert.key
#   harbor_admin_password: YourSecurePassword

# Instalar
./install.sh --with-chartmuseum

# Verificar
docker-compose ps
----

**Proceso de preparación air-gapped**

.Paso 1: Descargar todas las imágenes (desde máquina con internet):
[source,bash]
----
# Script para descargar imágenes de Rancher 2.9.3
RANCHER_VERSION=v2.9.3

# Descargar la lista de imágenes requeridas
curl -L https://github.com/rancher/rancher/releases/download/${RANCHER_VERSION}/rancher-images.txt -o rancher-images.txt

# Descargar script de save
curl -L https://github.com/rancher/rancher/releases/download/${RANCHER_VERSION}/rancher-save-images.sh -o rancher-save-images.sh

# Dar permisos y ejecutar
chmod +x rancher-save-images.sh
./rancher-save-images.sh --image-list rancher-images.txt

# Se genera: rancher-images.tar.gz (varios GB)
----

.Paso 2: Transferir artefactos al entorno air-gapped:
[source,bash]
----
# Método 1: USB/Disco duro externo
cp rancher-images.tar.gz /media/usb/
cp cert-manager-v1.14.0.tar.gz /media/usb/
cp helm-charts/ /media/usb/ -r

# Método 2: Transfer a bastion host
scp rancher-images.tar.gz user@bastion:/tmp/
scp cert-manager-v1.14.0.tar.gz user@bastion:/tmp/
----

.Paso 3: Cargar imágenes al registry privado:
[source,bash]
----
# Desde el entorno air-gapped
REGISTRY=registry.airgap.local

# Load images
docker load -i rancher-images.tar.gz

# Tag y push a registry privado
curl -L https://github.com/rancher/rancher/releases/download/${RANCHER_VERSION}/rancher-load-images.sh -o rancher-load-images.sh
chmod +x rancher-load-images.sh

./rancher-load-images.sh \
  --image-list rancher-images.txt \
  --registry ${REGISTRY}

# Verificar imágenes en registry
curl -u admin:password https://${REGISTRY}/v2/_catalog
----

**Configuración de CA certificates custom**

Si el registry privado usa certificados autofirmados o CA privada:

.Añadir CA cert a los nodos:
[source,bash]
----
# Ubuntu/Debian
cp ca.crt /usr/local/share/ca-certificates/
update-ca-certificates

# RHEL/CentOS
cp ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust

# Reiniciar Docker/containerd
systemctl restart docker
systemctl restart containerd
----

**Instalación de Rancher en air-gapped**

.Pasos específicos para air-gapped:
[source,bash]
----
# 1. Instalar cert-manager desde registry privado
helm install cert-manager cert-manager/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --set image.repository=${REGISTRY}/jetstack/cert-manager-controller \
  --set webhook.image.repository=${REGISTRY}/jetstack/cert-manager-webhook \
  --set cainjector.image.repository=${REGISTRY}/jetstack/cert-manager-cainjector \
  --set installCRDs=true

# 2. Instalar Rancher desde registry privado
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --create-namespace \
  --set hostname=rancher.airgap.local \
  --set rancherImage=${REGISTRY}/rancher/rancher \
  --set systemDefaultRegistry=${REGISTRY} \
  --set useBundledSystemChart=true

# 3. Verificar instalación
kubectl -n cattle-system get pods
kubectl -n cattle-system get svc
----

**Checklist de requisitos air-gapped**

.Antes de instalar Rancher en air-gapped:
[source,checklist]
----
☐ Private registry instalado y accesible
☐ Todas las imágenes de Rancher cargadas en registry
☐ Imágenes de cert-manager en registry
☐ Imágenes de system charts en registry
☐ Helm charts descargados localmente
☐ Certificados CA configurados en todos los nodos
☐ DNS interno configurado para resolver registry
☐ Ancho de banda interno suficiente para pull de imágenes
☐ Storage adecuado para registry (100+ GB)
☐ Backup strategy para registry
☐ Documentación de proceso de actualización
☐ Testing en ambiente de prueba completado
----

**Actualizaciones en entornos air-gapped**

Las actualizaciones requieren repetir el proceso de descarga y carga de nuevas imágenes:

[source,bash]
----
# 1. Descargar nueva versión (desde máquina con internet)
RANCHER_VERSION=v2.9.4
curl -L https://github.com/rancher/rancher/releases/download/${RANCHER_VERSION}/rancher-images.txt -o rancher-images-${RANCHER_VERSION}.txt
./rancher-save-images.sh --image-list rancher-images-${RANCHER_VERSION}.txt

# 2. Transferir al entorno air-gapped
# 3. Cargar en registry privado
./rancher-load-images.sh \
  --image-list rancher-images-${RANCHER_VERSION}.txt \
  --registry ${REGISTRY}

# 4. Actualizar Rancher
helm upgrade rancher rancher-stable/rancher \
  --namespace cattle-system \
  --version ${RANCHER_VERSION} \
  --set rancherImage=${REGISTRY}/rancher/rancher \
  --set systemDefaultRegistry=${REGISTRY} \
  --reuse-values
----

=== 2.2 Instalación de Rancher

Rancher puede instalarse de diferentes formas según las necesidades del entorno. Las dos opciones principales son instalación con Helm (producción) e instalación con Docker (desarrollo/testing).

==== Instalación con Helm

La instalación con Helm es el método recomendado para entornos de producción, proporcionando alta disponibilidad y escalabilidad.

**Prerequisitos para instalación con Helm**

.Requisitos previos:
* Clúster Kubernetes funcional (v1.26+)
* Helm 3.10+ instalado
* kubectl configurado para acceder al clúster
* Acceso a internet o registry privado con las imágenes necesarias
* Certificados SSL (Let's Encrypt, custom, o autofirmados)

**Paso 1: Preparar el clúster Kubernetes**

Si no tienes un clúster Kubernetes, puedes crear uno con RKE, RKE2, o usar un clúster gestionado (EKS, AKS, GKE).

.Crear clúster con RKE2 (ejemplo):
[source,bash]
----
# Instalar RKE2 en el primer nodo (controlplane + etcd + worker)
curl -sfL https://get.rke2.io | sh -
systemctl enable rke2-server.service
systemctl start rke2-server.service

# Esperar a que el clúster esté listo
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
/var/lib/rancher/rke2/bin/kubectl get nodes

# Para nodos adicionales, obtener el token
cat /var/lib/rancher/rke2/server/node-token

# En nodos adicionales
curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE="server" sh -
mkdir -p /etc/rancher/rke2/
cat > /etc/rancher/rke2/config.yaml <<EOF
server: https://<primer-nodo-ip>:9345
token: <token-del-primer-nodo>
tls-san:
  - <load-balancer-ip>
EOF

systemctl enable rke2-server.service
systemctl start rke2-server.service
----

**Paso 2: Instalar cert-manager**

cert-manager es necesario para gestionar certificados TLS en Rancher.

.Instalar cert-manager:
[source,bash]
----
# Añadir el repositorio de Helm de cert-manager
helm repo add jetstack https://charts.jetstack.io
helm repo update

# Crear namespace
kubectl create namespace cert-manager

# Instalar cert-manager
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version v1.14.0 \
  --set installCRDs=true

# Verificar instalación
kubectl get pods --namespace cert-manager

# Esperar a que todos los pods estén Running
kubectl wait --for=condition=Available --timeout=300s \
  -n cert-manager deployment/cert-manager \
  deployment/cert-manager-webhook \
  deployment/cert-manager-cainjector
----

**Paso 3: Añadir el repositorio Helm de Rancher**

.Añadir repositorio:
[source,bash]
----
# Repositorio stable (releases oficiales)
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

# O repositorio latest (más reciente, menos estable)
# helm repo add rancher-latest https://releases.rancher.com/server-charts/latest

# Actualizar índice de repositorios
helm repo update
----

**Paso 4: Crear namespace para Rancher**

[source,bash]
----
kubectl create namespace cattle-system
----

**Paso 5: Instalar Rancher**

Existen tres opciones principales para certificados SSL:

**Opción A: Certificados de Rancher (autofirmados)**

La opción más simple para pruebas y desarrollo.

[source,bash]
----
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=rancher.example.com \
  --set replicas=3 \
  --set bootstrapPassword=admin

# Verificar instalación
kubectl -n cattle-system rollout status deploy/rancher
kubectl -n cattle-system get deploy rancher
----

**Opción B: Let's Encrypt (certificados gratuitos)**

Recomendado para entornos públicos con DNS válido.

[source,bash]
----
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=rancher.example.com \
  --set replicas=3 \
  --set bootstrapPassword=admin \
  --set ingress.tls.source=letsEncrypt \
  --set letsEncrypt.email=admin@example.com \
  --set letsEncrypt.environment=production
  
# Para staging (testing):
# --set letsEncrypt.environment=staging
----

**Opción C: Certificados propios (custom certificates)**

Para entornos empresariales con PKI corporativa.

.Crear secret con certificados custom:
[source,bash]
----
# Crear secret TLS con tu certificado y key
kubectl -n cattle-system create secret tls tls-rancher-ingress \
  --cert=/path/to/tls.crt \
  --key=/path/to/tls.key

# Si tienes una CA intermedia, crear secret adicional
kubectl -n cattle-system create secret generic tls-ca \
  --from-file=cacerts.pem=/path/to/ca.crt

# Instalar Rancher con certificados custom
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=rancher.example.com \
  --set replicas=3 \
  --set bootstrapPassword=admin \
  --set ingress.tls.source=secret \
  --set privateCA=true
----

**Paso 6: Verificar la instalación**

[source,bash]
----
# Ver el estado del deployment
kubectl -n cattle-system rollout status deploy/rancher

# Ver todos los recursos
kubectl -n cattle-system get all

# Ver logs si hay problemas
kubectl -n cattle-system logs -l app=rancher

# Obtener la URL de acceso
echo https://rancher.example.com/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}')
----

**Configuración avanzada con Helm values**

Para configuraciones más complejas, crear un archivo `values.yaml`:

.Ejemplo de values.yaml completo:
[source,yaml]
----
# Hostname de Rancher
hostname: rancher.example.com

# Número de réplicas (mínimo 3 para HA)
replicas: 3

# Contraseña inicial del admin (cambiar en producción)
bootstrapPassword: "SecurePassword123!"

# Configuración de ingress
ingress:
  tls:
    source: secret  # o letsEncrypt, o rancher
  extraAnnotations:
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "30"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "1800"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "1800"

# Para certificados custom
privateCA: true

# Recursos para los pods
resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 4Gi

# Anti-affinity para distribuir pods en diferentes nodos
antiAffinity: required

# Configuración de proxy (si aplica)
proxy: http://proxy.example.com:8080
noProxy: 127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local

# Audit logging
auditLog:
  level: 2
  maxAge: 10
  maxBackup: 10
  maxSize: 100

# Sistema de autenticación por defecto
# (se puede cambiar después en la UI)
# authenticationMode: restricted

# Registry privado (para air-gapped)
# systemDefaultRegistry: registry.example.com
# useBundledSystemChart: true
----

.Instalar con values.yaml:
[source,bash]
----
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --values values.yaml
----

**Actualización de Rancher con Helm**

[source,bash]
----
# Actualizar repositorio
helm repo update

# Ver versión actual
helm list -n cattle-system

# Backup antes de actualizar (crítico!)
kubectl -n cattle-system get secret bootstrap-secret -o yaml > bootstrap-secret.yaml
helm get values rancher -n cattle-system -o yaml > rancher-values.yaml

# Actualizar a nueva versión
helm upgrade rancher rancher-stable/rancher \
  --namespace cattle-system \
  --version=2.9.3 \
  --reuse-values

# O con nuevos valores
helm upgrade rancher rancher-stable/rancher \
  --namespace cattle-system \
  --version=2.9.3 \
  --values values.yaml

# Verificar actualización
kubectl -n cattle-system rollout status deploy/rancher
----

**Troubleshooting instalación Helm**

.Problemas comunes y soluciones:
[source,bash]
----
# Problema: Pods en CrashLoopBackOff
kubectl -n cattle-system describe pod -l app=rancher
kubectl -n cattle-system logs -l app=rancher --tail=100

# Problema: Certificados inválidos
kubectl -n cattle-system get secret tls-rancher-ingress -o yaml
kubectl -n cattle-system describe certificate

# Problema: cert-manager no funciona
kubectl -n cert-manager get pods
kubectl -n cert-manager logs -l app=cert-manager

# Problema: Ingress no responde
kubectl -n cattle-system get ingress
kubectl -n cattle-system describe ingress rancher

# Problema: DNS no resuelve
nslookup rancher.example.com
curl -k https://rancher.example.com/ping

# Reinstalar desde cero
helm uninstall rancher -n cattle-system
kubectl delete namespace cattle-system
# Esperar a que todo se elimine completamente
kubectl create namespace cattle-system
# Reinstalar
----

==== Instalación con Docker (desarrollo)

La instalación con Docker es ideal para desarrollo, testing y demos rápidos. **No usar en producción.**

**Método 1: Instalación básica con Docker**

.Instalación simple:
[source,bash]
----
# Instalar Docker si no está instalado
curl -fsSL https://get.docker.com | sh
systemctl start docker
systemctl enable docker

# Ejecutar Rancher en un contenedor
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  rancher/rancher:v2.9.3

# Ver logs
docker logs -f rancher

# Esperar a que Rancher esté listo (buscar "Bootstrap Password")
docker logs rancher 2>&1 | grep "Bootstrap Password:"

# O generar nueva contraseña
docker exec -it rancher reset-password
----

**Método 2: Con volumen persistente**

Para mantener datos entre reinicios:

[source,bash]
----
# Crear volumen para persistencia
docker volume create rancher-data

# Ejecutar Rancher con volumen
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  -v rancher-data:/var/lib/rancher \
  rancher/rancher:v2.9.3

# Acceder a Rancher
echo "Accede a https://$(hostname -I | awk '{print $1}')"
----

**Método 3: Con hostname personalizado**

[source,bash]
----
# Con hostname específico (requiere DNS o /etc/hosts configurado)
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  -v rancher-data:/var/lib/rancher \
  -e CATTLE_BOOTSTRAP_PASSWORD=mypassword123 \
  rancher/rancher:v2.9.3 \
  --no-cacerts

# Configurar /etc/hosts si no tienes DNS
echo "192.168.1.100 rancher.local" | sudo tee -a /etc/hosts
----

**Método 4: Con certificados SSL custom (Docker)**

[source,bash]
----
# Preparar certificados
mkdir -p /opt/rancher/certs
cp tls.crt /opt/rancher/certs/
cp tls.key /opt/rancher/certs/
cp ca.crt /opt/rancher/certs/

# Ejecutar Rancher con certificados
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  -v rancher-data:/var/lib/rancher \
  -v /opt/rancher/certs/tls.crt:/etc/rancher/ssl/cert.pem:ro \
  -v /opt/rancher/certs/tls.key:/etc/rancher/ssl/key.pem:ro \
  -v /opt/rancher/certs/ca.crt:/etc/rancher/ssl/cacerts.pem:ro \
  rancher/rancher:v2.9.3
----

**Método 5: Con docker-compose**

Más fácil de gestionar y versionar.

.docker-compose.yml:
[source,yaml]
----
version: '3.8'

services:
  rancher:
    image: rancher/rancher:v2.9.3
    container_name: rancher
    restart: unless-stopped
    privileged: true
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - rancher-data:/var/lib/rancher
      # Opcional: certificados custom
      # - ./certs/tls.crt:/etc/rancher/ssl/cert.pem:ro
      # - ./certs/tls.key:/etc/rancher/ssl/key.pem:ro
      # - ./certs/ca.crt:/etc/rancher/ssl/cacerts.pem:ro
    environment:
      - CATTLE_BOOTSTRAP_PASSWORD=SecurePassword123!
      # Opcional: para debugging
      # - CATTLE_DEBUG=true
      # Opcional: proxy
      # - HTTP_PROXY=http://proxy.example.com:8080
      # - HTTPS_PROXY=http://proxy.example.com:8080
      # - NO_PROXY=localhost,127.0.0.1,rancher

volumes:
  rancher-data:
    driver: local
----

.Comandos docker-compose:
[source,bash]
----
# Iniciar Rancher
docker-compose up -d

# Ver logs
docker-compose logs -f

# Detener Rancher
docker-compose down

# Detener y eliminar volúmenes (cuidado!)
docker-compose down -v

# Actualizar a nueva versión
docker-compose pull
docker-compose up -d
----

**Backup y restauración (Docker)**

[source,bash]
----
# Backup del volumen de datos
docker run --rm \
  -v rancher-data:/source \
  -v $(pwd):/backup \
  alpine tar czf /backup/rancher-backup-$(date +%Y%m%d).tar.gz -C /source .

# Restauración
docker run --rm \
  -v rancher-data:/target \
  -v $(pwd):/backup \
  alpine sh -c "cd /target && tar xzf /backup/rancher-backup-20251030.tar.gz"

# Reiniciar Rancher
docker restart rancher
----

==== Configuración de certificados SSL

La configuración correcta de certificados SSL es crítica para la seguridad y funcionamiento de Rancher.

**Opción 1: Let's Encrypt (automático)**

Ya configurado durante la instalación con Helm. Renovación automática cada 60-90 días.

.Verificar certificados Let's Encrypt:
[source,bash]
----
# Ver el certificado
kubectl -n cattle-system get certificate

# Ver detalles
kubectl -n cattle-system describe certificate rancher

# Ver el secret del certificado
kubectl -n cattle-system get secret tls-rancher-ingress -o yaml

# Forzar renovación (si es necesario)
kubectl -n cattle-system delete secret tls-rancher-ingress
# cert-manager creará uno nuevo automáticamente
----

**Opción 2: Certificados autofirmados de Rancher**

Generados automáticamente por Rancher. Solo para testing.

.Generar nuevos certificados autofirmados:
[source,bash]
----
# Rancher regenera automáticamente al iniciar
# Para forzar regeneración:
kubectl -n cattle-system delete secret tls-rancher-ingress
kubectl -n cattle-system rollout restart deploy/rancher
----

**Opción 3: Certificados corporativos/custom**

**Generar certificados custom con OpenSSL**

[source,bash]
----
# 1. Generar CA privada (si no tienes una)
openssl genrsa -out ca.key 4096
openssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 \
  -out ca.crt \
  -subj "/C=US/ST=State/L=City/O=Organization/CN=MyCA"

# 2. Generar key privada para Rancher
openssl genrsa -out tls.key 4096

# 3. Crear CSR (Certificate Signing Request)
openssl req -new -key tls.key -out tls.csr \
  -subj "/C=US/ST=State/L=City/O=Organization/CN=rancher.example.com"

# 4. Crear archivo de extensiones para SAN
cat > rancher-ext.cnf <<EOF
subjectAltName = DNS:rancher.example.com,DNS:rancher,IP:192.168.1.100
EOF

# 5. Firmar el certificado con la CA
openssl x509 -req -in tls.csr -CA ca.crt -CAkey ca.key \
  -CAcreateserial -out tls.crt -days 365 -sha256 \
  -extfile rancher-ext.cnf

# 6. Verificar el certificado
openssl x509 -in tls.crt -text -noout | grep -A1 "Subject Alternative Name"
----

**Instalar certificados custom en Rancher (Helm)**

[source,bash]
----
# Crear secrets en Kubernetes
kubectl -n cattle-system create secret tls tls-rancher-ingress \
  --cert=tls.crt \
  --key=tls.key

kubectl -n cattle-system create secret generic tls-ca \
  --from-file=cacerts.pem=ca.crt

# Si Rancher ya está instalado, actualizar
helm upgrade rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set ingress.tls.source=secret \
  --set privateCA=true \
  --reuse-values

# Reiniciar pods
kubectl -n cattle-system rollout restart deploy/rancher
----

**Instalar certificados custom en Rancher (Docker)**

Ya mostrado en la sección de instalación Docker, pero para referencia:

[source,bash]
----
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  -v rancher-data:/var/lib/rancher \
  -v /path/to/tls.crt:/etc/rancher/ssl/cert.pem:ro \
  -v /path/to/tls.key:/etc/rancher/ssl/key.pem:ro \
  -v /path/to/ca.crt:/etc/rancher/ssl/cacerts.pem:ro \
  rancher/rancher:v2.9.3
----

**Renovación de certificados**

.Proceso de renovación de certificados custom:
[source,bash]
----
# 1. Generar nuevos certificados (como arriba)

# 2. Actualizar secret en Kubernetes
kubectl -n cattle-system create secret tls tls-rancher-ingress \
  --cert=tls-new.crt \
  --key=tls-new.key \
  --dry-run=client -o yaml | kubectl apply -f -

# 3. Reiniciar Rancher
kubectl -n cattle-system rollout restart deploy/rancher

# 4. Verificar
kubectl -n cattle-system get pods
curl -vk https://rancher.example.com 2>&1 | grep "SSL certificate"
----

**Troubleshooting certificados**

[source,bash]
----
# Ver detalles del certificado actual
echo | openssl s_client -connect rancher.example.com:443 2>/dev/null | \
  openssl x509 -noout -text

# Ver fecha de expiración
echo | openssl s_client -connect rancher.example.com:443 2>/dev/null | \
  openssl x509 -noout -dates

# Verificar chain completo
openssl s_client -connect rancher.example.com:443 -showcerts

# Verificar desde el navegador
# Chrome: Developer Tools → Security → View Certificate
# Firefox: Lock icon → Connection Secure → More Information
----

==== Configuración inicial del servidor

Después de instalar Rancher, es necesario completar la configuración inicial.

**Acceso inicial a Rancher**

.Obtener URL y contraseña inicial:
[source,bash]
----
# Para instalación con Helm
echo https://rancher.example.com/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}')

# Para instalación con Docker
docker logs rancher 2>&1 | grep "Bootstrap Password:"

# O resetear contraseña
docker exec -it rancher reset-password
kubectl -n cattle-system exec -it $(kubectl -n cattle-system get pods -l app=rancher -o name | head -1) -- reset-password
----

**Wizard de configuración inicial**

Al acceder por primera vez, Rancher presenta un wizard:

.Pasos del wizard:
1. **Aceptar términos y condiciones**
2. **Establecer contraseña del admin** (cambiar la bootstrap password)
3. **Configurar Server URL** (debe coincidir con el hostname)
4. **Configurar telemetría** (opcional, para estadísticas de uso)

.Ejemplo de Server URL:
[source,text]
----
https://rancher.example.com

# Debe ser accesible desde:
- Navegador del administrador
- Cluster agents en clústeres downstream
- API clients

# NO usar:
- localhost
- 127.0.0.1
- IPs privadas si los clústeres están en diferentes redes
----

**Configuración vía CLI (automatización)**

Para automatizar la configuración inicial:

[source,bash]
----
# Esperar a que Rancher esté listo
kubectl -n cattle-system wait --for=condition=Available --timeout=600s deployment/rancher

# Obtener token de bootstrap
BOOTSTRAP_PASSWORD=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}')

# Configurar contraseña del admin vía API
RANCHER_URL="https://rancher.example.com"
NEW_PASSWORD="MySecurePassword123!"

# Login con bootstrap password
TOKEN=$(curl -sk -X POST "${RANCHER_URL}/v3-public/localProviders/local?action=login" \
  -H 'content-type: application/json' \
  -d "{\"username\":\"admin\",\"password\":\"${BOOTSTRAP_PASSWORD}\"}" | jq -r .token)

# Cambiar contraseña
curl -sk -X POST "${RANCHER_URL}/v3/users?action=changepassword" \
  -H "Authorization: Bearer ${TOKEN}" \
  -H 'content-type: application/json' \
  -d "{\"currentPassword\":\"${BOOTSTRAP_PASSWORD}\",\"newPassword\":\"${NEW_PASSWORD}\"}"

# Configurar Server URL
curl -sk -X PUT "${RANCHER_URL}/v3/settings/server-url" \
  -H "Authorization: Bearer ${TOKEN}" \
  -H 'content-type: application/json' \
  -d "{\"name\":\"server-url\",\"value\":\"${RANCHER_URL}\"}"

# Crear API token para automatización futura
API_TOKEN=$(curl -sk -X POST "${RANCHER_URL}/v3/token" \
  -H "Authorization: Bearer ${TOKEN}" \
  -H 'content-type: application/json' \
  -d '{"type":"token","description":"Automation Token","ttl":0}' | jq -r .token)

echo "API Token: ${API_TOKEN}"
----

**Configuraciones post-instalación recomendadas**

.Configuraciones críticas:
[source,text]
----
1. Backup automático:
   - Configurar snapshots de etcd
   - Backup del namespace cattle-system
   - Documentar proceso de restauración

2. Monitoreo:
   - Habilitar Rancher Monitoring
   - Configurar alertas críticas
   - Dashboard de health

3. Seguridad:
   - Cambiar contraseña por defecto
   - Configurar autenticación externa (LDAP/SAML)
   - Habilitar audit logging
   - Revisar RBAC policies

4. Red:
   - Verificar firewall rules
   - Configurar proxy si aplica
   - Validar DNS resolution

5. Alta disponibilidad:
   - Verificar que hay 3+ réplicas
   - Comprobar anti-affinity rules
   - Testear failover
----

**Verificación de instalación completa**

.Checklist de verificación:
[source,bash]
----
#!/bin/bash
echo "=== Verificación de instalación de Rancher ==="

# 1. Pods corriendo
echo "1. Verificando pods..."
kubectl -n cattle-system get pods -l app=rancher
kubectl -n cattle-system get pods -l app=rancher | grep -q "Running" && echo "✓ Pods OK" || echo "✗ Pods con problemas"

# 2. Servicio accesible
echo "2. Verificando servicio..."
kubectl -n cattle-system get svc rancher
echo "✓ Servicio OK"

# 3. Ingress configurado
echo "3. Verificando ingress..."
kubectl -n cattle-system get ingress rancher
echo "✓ Ingress OK"

# 4. Certificados válidos
echo "4. Verificando certificados..."
kubectl -n cattle-system get secret tls-rancher-ingress
echo "✓ Certificados OK"

# 5. API responde
echo "5. Verificando API..."
curl -k https://rancher.example.com/ping && echo "✓ API responde" || echo "✗ API no responde"

# 6. UI accesible
echo "6. Verificando UI..."
curl -k https://rancher.example.com/ | grep -q "Rancher" && echo "✓ UI OK" || echo "✗ UI con problemas"

# 7. etcd saludable (si es clúster local)
echo "7. Verificando etcd..."
kubectl -n cattle-system exec -it $(kubectl -n cattle-system get pods -l app=rancher -o name | head -1) -- \
  kubectl -n kube-system exec -it etcd-rancher-0 -- etcdctl endpoint health 2>/dev/null && \
  echo "✓ etcd OK" || echo "⚠ etcd check skipped"

echo ""
echo "=== Fin de verificación ==="
----

**Próximos pasos**

Después de la instalación exitosa:

1. Configurar autenticación (ver sección 2.3)
2. Importar o crear el primer clúster (ver módulo 3)
3. Configurar backup y disaster recovery (ver módulo 9)
4. Instalar aplicaciones desde el marketplace (ver módulo 5)

=== 2.3 Configuración del entorno

Una vez instalado Rancher, es fundamental configurar el entorno de forma segura y robusta antes de comenzar a gestionar clústeres.

==== Configuración de autenticación

Rancher soporta múltiples métodos de autenticación. Por defecto usa autenticación local, pero en producción se recomienda integrar con sistemas externos.

**Tipos de autenticación disponibles**

.Métodos de autenticación soportados:
[cols="1,2,1", options="header"]
|===
|Método |Descripción |Uso recomendado

|Local
|Usuarios y contraseñas almacenados en Rancher
|Solo para desarrollo y testing

|Active Directory (LDAP)
|Integración con Microsoft AD
|Entornos corporativos Windows

|FreeIPA
|Integración con FreeIPA/Red Hat Identity Management
|Entornos corporativos Linux

|Azure AD
|Integración con Microsoft Azure AD
|Entornos cloud Microsoft

|GitHub
|Autenticación mediante GitHub
|Equipos de desarrollo

|Google OAuth
|Autenticación con cuentas Google
|Equipos pequeños, G Suite

|Keycloak (OIDC/SAML)
|Integración con Keycloak
|Entornos complejos, SSO

|Okta
|Integración con Okta
|Empresas con Okta

|Shibboleth (SAML)
|Federación académica/corporativa
|Universidades, grandes empresas

|Ping Identity
|Integración con Ping
|Empresas enterprise

|OpenLDAP
|LDAP genérico
|Servidores LDAP custom
|===

**Autenticación local (por defecto)**

Ya configurada al instalar Rancher. Gestión básica de usuarios:

.Gestión de usuarios locales:
[source,bash]
----
# Via UI: Global → Security → Authentication → Users
# Para crear usuario vía API:

RANCHER_URL="https://rancher.example.com"
API_TOKEN="token-xxxxx:xxxxxxxxxxxxxx"

# Crear nuevo usuario local
curl -k -X POST "${RANCHER_URL}/v3/users" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "type": "user",
    "username": "johndoe",
    "password": "SecurePassword123!",
    "name": "John Doe",
    "mustChangePassword": true,
    "enabled": true
  }'

# Listar usuarios
curl -k "${RANCHER_URL}/v3/users" \
  -H "Authorization: Bearer ${API_TOKEN}"

# Deshabilitar usuario
curl -k -X PUT "${RANCHER_URL}/v3/users/u-xxxxx" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{"enabled": false}'
----

**Configurar política de contraseñas**

.Política de contraseñas (via UI):
[source,text]
----
Global → Security → Authentication → Local

Opciones:
- Minimum Password Length: 12 caracteres
- Require at least one uppercase letter
- Require at least one lowercase letter
- Require at least one number
- Require at least one special character
- Password expiration: 90 días
- Password history: últimas 5 contraseñas
----

==== Integración con proveedores de identidad (LDAP, SAML, OIDC)

**Integración con Active Directory (LDAP)**

La integración más común en entornos corporativos.

.Configuración de Active Directory:
[source,text]
----
1. Ir a: Global → Security → Authentication → Active Directory

2. Configurar servidor:
   - Hostname: ad.example.com
   - Port: 636 (LDAPS) o 389 (LDAP)
   - TLS: Enabled (recomendado)
   - CA Certificate: (si usas certificado custom)

3. Service Account:
   - Distinguished Name: CN=rancher-svc,OU=Service Accounts,DC=example,DC=com
   - Password: <password del service account>

4. User Search Base:
   - DN: OU=Users,DC=example,DC=com
   - Search Filter: (&(objectClass=user)(sAMAccountName={username}))
   - Username Attribute: sAMAccountName
   - Login Attribute: sAMAccountName
   - User Object Class: person

5. Group Search Base:
   - DN: OU=Groups,DC=example,DC=com
   - Search Filter: (objectClass=group)
   - Group Object Class: group
   - Group Name Attribute: cn
   - Group Member Mapping Attribute: member

6. Test connection:
   - Username: testuser
   - Password: <contraseña>
   - Verify: Should show user info and groups

7. Enable y Save
----

.Configuración de AD via API:
[source,json]
----
{
  "type": "activeDirectoryConfig",
  "enabled": true,
  "servers": ["ad.example.com"],
  "port": 636,
  "tls": true,
  "startTls": false,
  "serviceAccountDistinguishedName": "CN=rancher-svc,OU=Service Accounts,DC=example,DC=com",
  "serviceAccountPassword": "SecurePassword123!",
  "userSearchBase": "OU=Users,DC=example,DC=com",
  "userSearchFilter": "(&(objectClass=user)(sAMAccountName={username}))",
  "userLoginAttribute": "sAMAccountName",
  "userNameAttribute": "cn",
  "userObjectClass": "person",
  "groupSearchBase": "OU=Groups,DC=example,DC=com",
  "groupSearchFilter": "(objectClass=group)",
  "groupObjectClass": "group",
  "groupNameAttribute": "cn",
  "groupMemberMappingAttribute": "member",
  "connectionTimeout": 5000,
  "nestedGroupMembershipEnabled": true
}
----

**Integración con Keycloak (OIDC)**

OpenID Connect es el estándar moderno para autenticación.

.Configuración de Keycloak:

**Paso 1: Configurar cliente en Keycloak**

[source,text]
----
1. Login en Keycloak admin console
2. Ir a: Clients → Create Client
3. Configurar:
   - Client ID: rancher
   - Client Protocol: openid-connect
   - Access Type: confidential
   - Valid Redirect URIs: https://rancher.example.com/*
   - Web Origins: https://rancher.example.com
4. En Credentials tab:
   - Copiar Client Secret
----

**Paso 2: Configurar Rancher**

[source,text]
----
1. Ir a: Global → Security → Authentication → Keycloak (OIDC)

2. Configurar endpoints:
   - Auth Endpoint: https://keycloak.example.com/auth/realms/master/protocol/openid-connect/auth
   - Token Endpoint: https://keycloak.example.com/auth/realms/master/protocol/openid-connect/token
   - User Info Endpoint: https://keycloak.example.com/auth/realms/master/protocol/openid-connect/userinfo
   - JWKS Endpoint: https://keycloak.example.com/auth/realms/master/protocol/openid-connect/certs

3. Cliente OAuth:
   - Client ID: rancher
   - Client Secret: <client secret from Keycloak>
   - Scope: openid profile email groups

4. Test y Enable
----

.Configuración automatizada con Terraform:
[source,hcl]
----
resource "rancher2_auth_config_keycloak_oidc" "keycloak" {
  display_name_field = "name"
  groups_field       = "groups"
  uid_field          = "sub"
  user_name_field    = "preferred_username"
  
  auth_endpoint       = "https://keycloak.example.com/auth/realms/master/protocol/openid-connect/auth"
  token_endpoint      = "https://keycloak.example.com/auth/realms/master/protocol/openid-connect/token"
  user_info_endpoint  = "https://keycloak.example.com/auth/realms/master/protocol/openid-connect/userinfo"
  
  client_id     = "rancher"
  client_secret = var.keycloak_client_secret
  
  enabled = true
}
----

**Integración con Azure AD (SAML)**

.Configuración de Azure AD:

**Paso 1: Registrar aplicación en Azure AD**

[source,bash]
----
# Via Azure Portal:
1. Azure Active Directory → Enterprise Applications → New Application
2. Create your own application
3. Nombre: Rancher
4. Integrar cualquier otra aplicación que no encuentre en la galería (Non-gallery)

# Configurar SSO
1. Single sign-on → SAML
2. Basic SAML Configuration:
   - Identifier (Entity ID): https://rancher.example.com
   - Reply URL (ACS): https://rancher.example.com/v1-saml/acs/azure
3. User Attributes & Claims:
   - Unique User Identifier: user.userprincipalname
   - Additional claims:
     - email: user.mail
     - givenname: user.givenname
     - surname: user.surname
     - groups: user.groups
4. SAML Certificates:
   - Descargar Federation Metadata XML
   - O copiar App Federation Metadata URL
----

**Paso 2: Configurar Rancher**

[source,text]
----
1. Ir a: Global → Security → Authentication → Azure AD

2. Configurar:
   - Display Name Field: givenname
   - User Name Field: email
   - UID Field: email
   - Groups Field: groups
   
   - Entity ID: https://rancher.example.com
   - Rancher API Host: https://rancher.example.com
   - IDP Metadata: <pegar contenido del XML de Azure>

3. Test y Enable
----

**Integración con GitHub**

Más simple, ideal para equipos de desarrollo.

.Configuración de GitHub OAuth:

**Paso 1: Crear OAuth App en GitHub**

[source,text]
----
1. GitHub → Settings → Developer settings → OAuth Apps → New OAuth App
2. Configurar:
   - Application name: Rancher
   - Homepage URL: https://rancher.example.com
   - Authorization callback URL: https://rancher.example.com/verify-auth
3. Register application
4. Copiar Client ID y generar Client Secret
----

**Paso 2: Configurar Rancher**

[source,text]
----
1. Ir a: Global → Security → Authentication → GitHub

2. Configurar:
   - Client ID: <from GitHub>
   - Client Secret: <from GitHub>
   - GitHub Endpoint: https://github.com (para GitHub Enterprise usar URL custom)

3. Authenticate with GitHub
4. Autorizar en GitHub
5. Configure Authorization:
   - Allow any valid users
   - Allow members of Organizations: <org1>, <org2>
   - Allow members of Teams: <org>/<team>

6. Save
----

**Sincronización de grupos**

Para mapear grupos externos a roles en Rancher:

.Ejemplo de asignación de grupos:
[source,bash]
----
# Crear Global Role Binding para grupo AD
curl -k -X POST "${RANCHER_URL}/v3/globalrolebindings" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "type": "globalRoleBinding",
    "globalRoleId": "admin",
    "groupPrincipalId": "activedirectory_group://CN=RancherAdmins,OU=Groups,DC=example,DC=com"
  }'

# Crear Cluster Role Binding para grupo
curl -k -X POST "${RANCHER_URL}/v3/clusterroletemplatebindings" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "type": "clusterRoleTemplateBinding",
    "clusterId": "c-xxxxx",
    "roleTemplateId": "cluster-owner",
    "groupPrincipalId": "activedirectory_group://CN=DevOps,OU=Groups,DC=example,DC=com"
  }'
----

==== Configuración de notificaciones

Rancher puede enviar notificaciones sobre eventos importantes del sistema.

**Tipos de notificadores disponibles**

.Notificadores soportados:
[cols="1,2,1", options="header"]
|===
|Tipo |Descripción |Casos de uso

|Slack
|Webhooks a canales Slack
|Equipos que usan Slack

|Email (SMTP)
|Correos electrónicos
|Alertas críticas, reportes

|PagerDuty
|Integración con PagerDuty
|Equipos de operaciones 24/7

|Webhook
|HTTP POST a endpoint custom
|Integraciones personalizadas

|Microsoft Teams
|Webhooks a Teams
|Equipos que usan Microsoft Teams

|DingTalk
|Integración con DingTalk
|Equipos en Asia-Pacífico
|===

**Configurar notificaciones Slack**

.Configuración de Slack:

**Paso 1: Crear Incoming Webhook en Slack**

[source,text]
----
1. Ir a: https://api.slack.com/apps
2. Create New App → From scratch
3. Nombre: Rancher Notifications
4. Seleccionar workspace
5. Incoming Webhooks → Activate
6. Add New Webhook to Workspace
7. Seleccionar canal destino (#rancher-alerts)
8. Copiar Webhook URL: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXX
----

**Paso 2: Configurar en Rancher**

[source,text]
----
# Via UI:
1. Cluster → Tools → Notifiers → Add Notifier
2. Tipo: Slack
3. Configurar:
   - Name: slack-alerts
   - Webhook URL: <URL de Slack>
   - Default Channel: #rancher-alerts
   - Proxy URL: (si aplica)

# Via API:
curl -k -X POST "${RANCHER_URL}/v3/notifiers" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "type": "notifier",
    "clusterId": "c-xxxxx",
    "name": "slack-alerts",
    "slackConfig": {
      "defaultRecipient": "#rancher-alerts",
      "url": "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXX"
    }
  }'
----

**Configurar notificaciones Email (SMTP)**

.Configuración de SMTP:
[source,text]
----
1. Cluster → Tools → Notifiers → Add Notifier
2. Tipo: Email
3. Configurar SMTP:
   - Default Recipient: alerts@example.com
   - SMTP Server: smtp.gmail.com
   - Port: 587
   - Use TLS: Yes
   - Username: rancher@example.com
   - Password: <app password>
   - Sender: rancher@example.com
----

.Ejemplo con Gmail:
[source,json]
----
{
  "type": "notifier",
  "clusterId": "c-xxxxx",
  "name": "email-alerts",
  "smtpConfig": {
    "defaultRecipient": "alerts@example.com",
    "host": "smtp.gmail.com",
    "port": 587,
    "username": "rancher@example.com",
    "password": "app-specific-password",
    "sender": "rancher@example.com",
    "tls": true
  }
}
----

**Configurar notificaciones PagerDuty**

[source,text]
----
1. En PagerDuty:
   - Services → Rancher → Integrations → Add Integration
   - Integration Type: Events API V2
   - Copiar Integration Key

2. En Rancher:
   - Cluster → Tools → Notifiers → Add Notifier
   - Tipo: PagerDuty
   - Integration Key: <key de PagerDuty>
   - Service Key Type: Integration Key
----

**Configurar alertas**

Una vez configurados los notificadores, crear reglas de alerta:

.Ejemplo de alerta para uso alto de CPU:
[source,yaml]
----
# Via UI: Cluster → Tools → Alerts → Add Alert
# O via YAML:

apiVersion: management.cattle.io/v3
kind: ClusterAlertRule
metadata:
  name: high-cpu-alert
  namespace: c-xxxxx
spec:
  clusterName: c-xxxxx
  displayName: "High CPU Usage"
  groupName: cluster-alerts
  recipients:
    - notifier: slack-alerts
      recipient: "#critical-alerts"
  metricRule:
    comparison: greater-than
    duration: 5m
    expression: avg(cpu_usage)
    threshold: 80
    description: "CPU usage above 80% for 5 minutes"
  severity: critical
----

.Tipos de alertas disponibles:
[source,text]
----
1. System Events:
   - Node down
   - Pod crashlooping
   - Deployment failed

2. Resource Alerts:
   - High CPU usage
   - High memory usage
   - Disk space low

3. Workload Alerts:
   - Pod not running
   - Available replicas below threshold

4. Event-based:
   - Kubernetes events (warning/error)
----

**Testing de notificaciones**

[source,bash]
----
# Test notifier via API
curl -k -X POST "${RANCHER_URL}/v3/notifiers/c-xxxxx:n-xxxxx?action=send" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "message": "Test notification from Rancher",
    "recipient": "#test-channel"
  }'

# Generar alerta de prueba
kubectl run test-pod --image=nginx --limits=cpu=50m,memory=50Mi
kubectl delete pod test-pod
# Debería generar alerta de pod deleted
----

==== Backup y restauración del servidor Rancher

El backup regular es crítico para disaster recovery.

**Componentes a respaldar**

.Datos críticos en Rancher:
[source,text]
----
1. Local cluster etcd:
   - Contiene toda la configuración de Rancher
   - Clústeres registrados
   - Usuarios, roles, permisos
   - Apps instaladas
   - Configuración de cattle-system

2. Persistent volumes (si aplica):
   - Logs de auditoría
   - Monitoring data

3. Secrets de Kubernetes:
   - Certificados SSL
   - Tokens de API
   - Credenciales de clústeres downstream
----

**Método 1: Backup con rancher-backup operator (recomendado)**

El método oficial y más robusto.

.Instalar rancher-backup:
[source,bash]
----
# Via Helm
helm repo add rancher-charts https://charts.rancher.io
helm repo update

# Instalar el operador
helm install rancher-backup-crd rancher-charts/rancher-backup-crd \
  --namespace cattle-resources-system \
  --create-namespace

helm install rancher-backup rancher-charts/rancher-backup \
  --namespace cattle-resources-system

# Verificar instalación
kubectl -n cattle-resources-system get pods -l app.kubernetes.io/name=rancher-backup
----

.Configurar S3 storage para backups:
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: s3-creds
  namespace: cattle-resources-system
type: Opaque
stringData:
  accessKey: <AWS_ACCESS_KEY_ID>
  secretKey: <AWS_SECRET_ACCESS_KEY>
---
apiVersion: resources.cattle.io/v1
kind: Backup
metadata:
  name: rancher-backup-daily
  namespace: cattle-resources-system
spec:
  resourceSetName: rancher-resource-set
  schedule: "0 2 * * *"  # Daily at 2 AM
  retentionCount: 30
  storageLocation:
    s3:
      credentialSecretName: s3-creds
      credentialSecretNamespace: cattle-resources-system
      bucketName: rancher-backups
      folder: prod
      region: us-east-1
      endpoint: s3.amazonaws.com
----

.Backup manual (on-demand):
[source,yaml]
----
apiVersion: resources.cattle.io/v1
kind: Backup
metadata:
  name: rancher-backup-manual
  namespace: cattle-resources-system
spec:
  resourceSetName: rancher-resource-set
  storageLocation:
    s3:
      credentialSecretName: s3-creds
      credentialSecretNamespace: cattle-resources-system
      bucketName: rancher-backups
      folder: manual
      region: us-east-1
      endpoint: s3.amazonaws.com
----

.Aplicar backup:
[source,bash]
----
kubectl apply -f backup.yaml

# Verificar estado
kubectl -n cattle-resources-system get backups
kubectl -n cattle-resources-system describe backup rancher-backup-manual

# Ver logs
kubectl -n cattle-resources-system logs -l app.kubernetes.io/name=rancher-backup
----

**Método 2: Backup de etcd (clúster local)**

Para backups más bajos nivel del clúster Kubernetes de Rancher.

.Backup de etcd con RKE2:
[source,bash]
----
# RKE2 hace snapshots automáticos cada 12 horas
# Ubicación: /var/lib/rancher/rke2/server/db/snapshots/

# Backup manual
rke2 etcd-snapshot save --name manual-backup-$(date +%Y%m%d-%H%M%S)

# Listar snapshots
rke2 etcd-snapshot list

# Configurar snapshots automáticos
cat > /etc/rancher/rke2/config.yaml <<EOF
etcd-snapshot-schedule-cron: "0 */6 * * *"  # Cada 6 horas
etcd-snapshot-retention: 14  # Mantener 14 snapshots
etcd-snapshot-dir: /var/lib/rancher/rke2/server/db/snapshots
EOF

systemctl restart rke2-server
----

.Backup de etcd con RKE1:
[source,bash]
----
# Con RKE CLI
rke etcd snapshot-save \
  --config cluster.yml \
  --name manual-backup-$(date +%Y%m%d-%H%M%S)

# Backups automáticos configurados en cluster.yml
services:
  etcd:
    backup_config:
      enabled: true
      interval_hours: 6
      retention: 14
      s3_backup_config:
        access_key: <AWS_ACCESS_KEY>
        secret_key: <AWS_SECRET_KEY>
        bucket_name: rancher-etcd-backups
        region: us-east-1
        folder: prod
----

**Método 3: Backup con Docker (single-node)**

Para instalaciones de desarrollo con Docker.

[source,bash]
----
# Backup del contenedor completo
docker stop rancher
docker commit rancher rancher-backup-$(date +%Y%m%d)
docker start rancher

# Backup del volumen
docker run --rm \
  -v rancher-data:/source:ro \
  -v $(pwd):/backup \
  alpine tar czf /backup/rancher-backup-$(date +%Y%m%d).tar.gz -C /source .

# Listar backups
ls -lh rancher-backup-*.tar.gz
----

**Restauración del servidor Rancher**

.Restaurar con rancher-backup operator:
[source,yaml]
----
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-rancher
  namespace: cattle-resources-system
spec:
  backupFilename: rancher-backup-daily-20251030-020000.tar.gz
  storageLocation:
    s3:
      credentialSecretName: s3-creds
      credentialSecretNamespace: cattle-resources-system
      bucketName: rancher-backups
      folder: prod
      region: us-east-1
      endpoint: s3.amazonaws.com
----

[source,bash]
----
kubectl apply -f restore.yaml

# Monitorear restauración
kubectl -n cattle-resources-system get restores
kubectl -n cattle-resources-system describe restore restore-rancher
kubectl -n cattle-resources-system logs -l app.kubernetes.io/name=rancher-backup

# Reiniciar Rancher después de restauración
kubectl -n cattle-system rollout restart deploy/rancher
----

.Restaurar desde snapshot de etcd (RKE2):
[source,bash]
----
# Detener RKE2
systemctl stop rke2-server

# Restaurar snapshot
rke2 server \
  --cluster-reset \
  --cluster-reset-restore-path=/var/lib/rancher/rke2/server/db/snapshots/manual-backup-20251030-120000

# Iniciar RKE2
systemctl start rke2-server

# Verificar clúster
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
kubectl get nodes
kubectl -n cattle-system get pods
----

.Restaurar con Docker:
[source,bash]
----
# Desde imagen backup
docker stop rancher
docker rm rancher
docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  rancher-backup-20251030

# Desde volumen backup
docker volume create rancher-data-restored
docker run --rm \
  -v rancher-data-restored:/target \
  -v $(pwd):/backup \
  alpine sh -c "cd /target && tar xzf /backup/rancher-backup-20251030.tar.gz"

docker run -d --restart=unless-stopped \
  -p 80:80 -p 443:443 \
  --privileged \
  --name rancher \
  -v rancher-data-restored:/var/lib/rancher \
  rancher/rancher:v2.9.3
----

**Disaster Recovery completo**

.Procedimiento completo de DR:
[source,bash]
----
#!/bin/bash
# Script de disaster recovery de Rancher

echo "=== Rancher Disaster Recovery ==="

# 1. Preparar nuevo clúster Kubernetes
echo "1. Preparando clúster Kubernetes..."
# (Instalar RKE2/K3s según necesidad)

# 2. Instalar cert-manager
echo "2. Instalando cert-manager..."
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.14.0 \
  --set installCRDs=true
kubectl wait --for=condition=Available --timeout=300s -n cert-manager deployment --all

# 3. Instalar rancher-backup operator
echo "3. Instalando rancher-backup operator..."
helm install rancher-backup-crd rancher-charts/rancher-backup-crd \
  --namespace cattle-resources-system \
  --create-namespace
helm install rancher-backup rancher-charts/rancher-backup \
  --namespace cattle-resources-system

# 4. Instalar Rancher
echo "4. Instalando Rancher..."
kubectl create namespace cattle-system
helm install rancher rancher-stable/rancher \
  --namespace cattle-system \
  --set hostname=rancher.example.com \
  --set replicas=3 \
  --set bootstrapPassword=temp123
kubectl -n cattle-system rollout status deploy/rancher

# 5. Restaurar backup
echo "5. Restaurando backup..."
cat <<EOF | kubectl apply -f -
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-dr
  namespace: cattle-resources-system
spec:
  backupFilename: rancher-backup-latest.tar.gz
  storageLocation:
    s3:
      credentialSecretName: s3-creds
      credentialSecretNamespace: cattle-resources-system
      bucketName: rancher-backups
      folder: prod
      region: us-east-1
EOF

# 6. Esperar restauración
echo "6. Esperando restauración..."
kubectl -n cattle-resources-system wait --for=condition=Complete --timeout=600s restore/restore-dr

# 7. Reiniciar Rancher
echo "7. Reiniciando Rancher..."
kubectl -n cattle-system rollout restart deploy/rancher
kubectl -n cattle-system rollout status deploy/rancher

# 8. Verificar
echo "8. Verificando instalación..."
kubectl -n cattle-system get pods
kubectl get clusters

echo "=== DR completado ==="
echo "Accede a: https://rancher.example.com"
----

**Best practices para backup**

.Recomendaciones:
[source,text]
----
1. Frecuencia:
   - Backups automáticos cada 6-12 horas
   - Backup manual antes de cada cambio importante
   - Backup antes de actualizar Rancher

2. Retención:
   - Mantener últimos 7-14 backups diarios
   - Backups semanales por 3 meses
   - Backups mensuales por 1 año

3. Almacenamiento:
   - Usar S3 o storage externo (no local)
   - Cifrar backups en reposo
   - Replicar en múltiples regiones

4. Testing:
   - Probar restauración mensualmente
   - Documentar tiempo de recovery (RTO)
   - Documentar pérdida de datos aceptable (RPO)

5. Monitoreo:
   - Alertar si backup falla
   - Verificar integridad de backups
   - Documentar proceso de DR
----

**Verificación de backup**

[source,bash]
----
# Verificar último backup exitoso
kubectl -n cattle-resources-system get backups -o json | \
  jq '.items | sort_by(.status.completedAt) | last | {name: .metadata.name, completed: .status.completedAt, size: .status.storageSize}'

# Verificar backups en S3
aws s3 ls s3://rancher-backups/prod/ --recursive --human-readable

# Test de restauración en clúster de prueba
kubectl apply -f test-restore.yaml
kubectl -n cattle-resources-system logs -f -l app.kubernetes.io/name=rancher-backup

# Comparar configuración antes/después del backup
kubectl get all -A --export > pre-backup.yaml
# Hacer backup y restauración
kubectl get all -A --export > post-restore.yaml
diff pre-backup.yaml post-restore.yaml
----

== Módulo 3: Gestión de Clústeres Kubernetes

=== 3.1 Creación de nuevos clústeres

Rancher facilita la creación y gestión de clústeres Kubernetes mediante diferentes métodos. Puede crear clústeres desde cero, importar clústeres existentes, o usar clústeres gestionados de proveedores cloud.

==== Creación de clústeres RKE (Rancher Kubernetes Engine)

RKE (Rancher Kubernetes Engine) y RKE2 son las distribuciones de Kubernetes creadas y mantenidas por Rancher. RKE2 es la evolución de RKE con mayor seguridad y certificación del gobierno de EE.UU.

**Diferencias entre RKE1 y RKE2**

.Comparación RKE1 vs RKE2:
[cols="1,2,2", options="header"]
|===
|Característica |RKE1 |RKE2

|Arquitectura
|Basado en Docker
|Basado en containerd

|Certificación
|CNCF Certified
|CNCF Certified + FIPS 140-2

|Security Hardening
|Manual
|CIS Hardening by default

|Control Plane
|Como contenedores Docker
|Como procesos systemd

|Actualizaciones
|Más manual
|Rolling updates integradas

|Windows Support
|Limitado
|Completo

|Recomendación
|Legacy (mantenimiento)
|Nuevas instalaciones
|===

**Método 1: Crear clúster RKE2 con infraestructura existente**

Este método asume que ya tienes VMs o servidores bare metal preparados.

.Crear clúster RKE2 desde Rancher UI:

[source,text]
----
1. En Rancher UI:
   - Cluster Management → Create
   - Seleccionar "Custom" cluster
   - Nombre: production-cluster

2. Configuración básica:
   - Kubernetes Version: v1.28.15+rke2r1 (seleccionar última stable)
   - Network Provider: Cilium (o Calico/Canal)
   - Cloud Provider: None (o seleccionar si es AWS/Azure/GCP)

3. Configuración de nodos:
   Seleccionar roles para cada nodo:
   - etcd: Al menos 3 nodos (número impar)
   - Control Plane: Al menos 3 nodos (para HA)
   - Worker: Según capacidad necesaria
   
   Mejores prácticas:
   - Dedicar nodos separados para etcd + controlplane
   - Workers solo para workloads
   - O combinar etcd + controlplane + worker en clústeres pequeños

4. Member Roles:
   - Add Member: Asignar usuarios/grupos con roles

5. Labels & Annotations:
   - Añadir labels para organización:
     environment: production
     region: us-east-1
     owner: platform-team

6. Advanced Options:
   - Additional Controller Manager Args:
     --node-monitor-grace-period=40s
   - Additional Scheduler Args:
     --bind-address=0.0.0.0
   - Additional API Server Args:
     --audit-log-path=/var/log/kube-audit/audit.log

7. Registro de nodos:
   Al crear, Rancher genera comandos de registro
----

**Comandos de registro de nodos**

Rancher genera comandos únicos para cada clúster:

.Registrar nodo con todos los roles (small cluster):
[source,bash]
----
# En el nodo Linux (Ubuntu/RHEL/SLES)
sudo curl -fL https://rancher.example.com/system-agent-install.sh | \
  sudo sh -s - --server https://rancher.example.com \
  --label 'cattle.io/os=linux' \
  --token <registration-token> \
  --ca-checksum <ca-checksum> \
  --etcd --controlplane --worker
----

.Registrar nodos dedicados para HA:
[source,bash]
----
# Nodos 1-3: etcd + controlplane (sin workloads)
sudo curl -fL https://rancher.example.com/system-agent-install.sh | \
  sudo sh -s - --server https://rancher.example.com \
  --label 'cattle.io/os=linux' \
  --token <registration-token> \
  --ca-checksum <ca-checksum> \
  --etcd --controlplane

# Nodos 4+: workers (solo workloads)
sudo curl -fL https://rancher.example.com/system-agent-install.sh | \
  sudo sh -s - --server https://rancher.example.com \
  --label 'cattle.io/os=linux' \
  --token <registration-token> \
  --ca-checksum <ca-checksum> \
  --worker

# Para Windows workers
# Ejecutar PowerShell script proporcionado por Rancher UI
----

**Preparación de nodos antes de registro**

.Requisitos en cada nodo:
[source,bash]
----
#!/bin/bash
# Script de preparación de nodo para RKE2

set -e

echo "=== Preparando nodo para RKE2 ==="

# 1. Actualizar sistema
echo "1. Actualizando sistema..."
apt-get update && apt-get upgrade -y  # Ubuntu/Debian
# yum update -y  # RHEL/CentOS

# 2. Deshabilitar swap
echo "2. Deshabilitando swap..."
swapoff -a
sed -i '/swap/d' /etc/fstab

# 3. Configurar módulos del kernel
echo "3. Configurando módulos kernel..."
cat <<EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

# 4. Configurar sysctl
echo "4. Configurando sysctl..."
cat <<EOF | tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system

# 5. Instalar dependencias
echo "5. Instalando dependencias..."
apt-get install -y curl wget tar  # Ubuntu/Debian
# yum install -y curl wget tar  # RHEL/CentOS

# 6. Configurar firewall (ejemplo UFW en Ubuntu)
echo "6. Configurando firewall..."
ufw allow 22/tcp    # SSH
ufw allow 80/tcp    # HTTP
ufw allow 443/tcp   # HTTPS
ufw allow 6443/tcp  # Kubernetes API
ufw allow 2379:2380/tcp  # etcd
ufw allow 10250/tcp # kubelet
ufw allow 30000:32767/tcp  # NodePort Services

# 7. Configurar NTP
echo "7. Configurando NTP..."
apt-get install -y chrony
systemctl enable chrony
systemctl start chrony

# 8. Verificar conectividad a Rancher
echo "8. Verificando conectividad a Rancher..."
curl -k https://rancher.example.com/ping && echo "✓ Rancher accesible" || echo "✗ No se puede alcanzar Rancher"

# 9. Configurar hostname único
echo "9. Configurando hostname..."
hostnamectl set-hostname node-$(hostname -I | awk '{print $1}' | tr '.' '-')

echo "=== Nodo preparado ==="
echo "Ejecutar comando de registro de Rancher ahora"
----

**Método 2: Crear clúster RKE2 en proveedores cloud**

Rancher puede crear clústeres directamente en AWS, Azure, GCP, etc.

.Crear clúster en AWS (EC2):

[source,text]
----
1. Prerequisitos:
   - Cloud Credentials configuradas en Rancher
   - Ir a: Cluster Management → Cloud Credentials → Create
   - Tipo: AWS
   - Access Key: <AWS_ACCESS_KEY_ID>
   - Secret Key: <AWS_SECRET_ACCESS_KEY>

2. Crear clúster:
   - Cluster Management → Create
   - Seleccionar: Amazon EC2

3. Configuración:
   Cluster Name: aws-production
   
   Machine Pools:
   
   Pool 1 - Control Plane + etcd:
   - Count: 3
   - Instance Type: t3.medium (2 vCPU, 4 GB RAM)
   - Roles: etcd + controlplane
   - Root Disk Size: 40 GB
   - Region: us-east-1
   - Zone: us-east-1a, us-east-1b, us-east-1c
   
   Pool 2 - Workers:
   - Count: 5
   - Instance Type: t3.xlarge (4 vCPU, 16 GB RAM)
   - Roles: worker
   - Root Disk Size: 100 GB
   - Region: us-east-1
   - Zone: us-east-1a, us-east-1b, us-east-1c

4. Network Configuration:
   - VPC: Create new (o usar existente)
   - Subnet: Auto-create
   - Security Group: Auto-create
   - Public IP: Yes (para acceso inicial)

5. Labels & Taints:
   - Labels para workers:
     workload.type: general
     zone: us-east-1a
   - Taints (opcional):
     dedicated=gpu:NoSchedule (para nodos GPU)

6. Create
----

.Crear clúster en Azure (AKS):
[source,text]
----
1. Cloud Credentials (Azure):
   - Subscription ID
   - Client ID (App ID)
   - Client Secret
   - Tenant ID

2. Crear clúster:
   - Cluster Management → Create
   - Seleccionar: Azure AKS

3. Configuración:
   Cluster Name: azure-production
   Resource Group: rancher-clusters (nuevo o existente)
   Region: East US
   Kubernetes Version: 1.28.5

   Node Pools:
   
   System Pool:
   - Count: 3
   - VM Size: Standard_D4s_v3
   - OS Disk Size: 100 GB
   - Mode: System
   
   User Pool:
   - Count: 5
   - VM Size: Standard_D8s_v3
   - OS Disk Size: 200 GB
   - Mode: User

4. Networking:
   - Network Plugin: Azure CNI (o kubenet)
   - Service CIDR: 10.0.0.0/16
   - DNS Service IP: 10.0.0.10
   - Docker Bridge CIDR: 172.17.0.1/16
   - Load Balancer SKU: Standard

5. Create
----

**Método 3: Crear clúster RKE1 (legacy)**

Para compatibilidad con instalaciones antiguas.

.Crear clúster RKE1:
[source,text]
----
1. En Rancher UI:
   - Cluster Management → Create
   - Seleccionar "Custom" cluster
   - Toggle: Use existing nodes and create cluster using RKE

2. Configuración:
   - Nombre: legacy-cluster
   - Kubernetes Version: v1.27.10-rancher2-1
   - Network Provider: Canal
   - Enable Project Network Isolation: Yes

3. Registrar nodos con Docker:
   # Nodos deben tener Docker instalado
   sudo docker run -d --privileged --restart=unless-stopped \
     --net=host -v /etc/kubernetes:/etc/kubernetes \
     -v /var/run:/var/run \
     rancher/rancher-agent:v2.9.3 \
     --server https://rancher.example.com \
     --token <token> \
     --ca-checksum <checksum> \
     --etcd --controlplane --worker
----

**Método 4: Crear clúster con Terraform**

Automatización completa con Infrastructure as Code.

.Terraform para clúster RKE2 en AWS:
[source,hcl]
----
terraform {
  required_providers {
    rancher2 = {
      source  = "rancher/rancher2"
      version = "~> 4.0"
    }
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "rancher2" {
  api_url   = "https://rancher.example.com"
  token_key = var.rancher_token
  insecure  = false
}

provider "aws" {
  region = var.aws_region
}

# Cloud Credential
resource "rancher2_cloud_credential" "aws" {
  name = "aws-credentials"
  amazonec2_credential_config {
    access_key = var.aws_access_key
    secret_key = var.aws_secret_key
  }
}

# Machine Config Template para control plane
resource "rancher2_machine_config_v2" "controlplane" {
  generate_name = "controlplane"
  amazonec2_config {
    ami                 = "ami-0c55b159cbfafe1f0"  # Ubuntu 22.04
    region              = var.aws_region
    security_group      = [aws_security_group.rancher_cluster.name]
    subnet_id           = aws_subnet.rancher.id
    vpc_id              = aws_vpc.rancher.id
    zone                = "a"
    root_size           = "40"
    instance_type       = "t3.medium"
    ssh_user            = "ubuntu"
    iam_instance_profile = aws_iam_instance_profile.rancher_node.name
  }
}

# Machine Config Template para workers
resource "rancher2_machine_config_v2" "worker" {
  generate_name = "worker"
  amazonec2_config {
    ami                 = "ami-0c55b159cbfafe1f0"
    region              = var.aws_region
    security_group      = [aws_security_group.rancher_cluster.name]
    subnet_id           = aws_subnet.rancher.id
    vpc_id              = aws_vpc.rancher.id
    zone                = "a"
    root_size           = "100"
    instance_type       = "t3.xlarge"
    ssh_user            = "ubuntu"
    iam_instance_profile = aws_iam_instance_profile.rancher_node.name
  }
}

# Clúster RKE2
resource "rancher2_cluster_v2" "production" {
  name                   = "production-cluster"
  kubernetes_version     = "v1.28.15+rke2r1"
  enable_network_policy  = true
  default_pod_security_policy_template_name = "restricted"

  rke_config {
    machine_pools {
      name                         = "controlplane-pool"
      cloud_credential_secret_name = rancher2_cloud_credential.aws.id
      control_plane_role           = true
      etcd_role                    = true
      worker_role                  = false
      quantity                     = 3
      machine_config {
        kind = rancher2_machine_config_v2.controlplane.kind
        name = rancher2_machine_config_v2.controlplane.name
      }
    }

    machine_pools {
      name                         = "worker-pool"
      cloud_credential_secret_name = rancher2_cloud_credential.aws.id
      control_plane_role           = false
      etcd_role                    = false
      worker_role                  = true
      quantity                     = 5
      machine_config {
        kind = rancher2_machine_config_v2.worker.kind
        name = rancher2_machine_config_v2.worker.name
      }
    }

    machine_selector_config {
      config = {
        protect-kernel-defaults = true
      }
    }

    upgrade_strategy {
      control_plane_concurrency = "1"
      worker_concurrency        = "2"
      
      control_plane_drain_options {
        enabled = true
        delete_empty_dir_data = true
        timeout = 300
      }
      
      worker_drain_options {
        enabled = true
        delete_empty_dir_data = true
        timeout = 300
      }
    }

    etcd {
      snapshot_schedule_cron = "0 */6 * * *"
      snapshot_retention     = 14
      s3_config {
        bucket = "rancher-etcd-backups"
        region = var.aws_region
        folder = "production"
      }
    }

    chart_values = <<EOF
rke2-cilium:
  ipam:
    mode: kubernetes
  tunnel: vxlan
EOF
  }

  agent_env_vars {
    name  = "HTTP_PROXY"
    value = var.proxy_url
  }
}

# Outputs
output "cluster_id" {
  value = rancher2_cluster_v2.production.cluster_v1_id
}

output "kubeconfig" {
  value     = rancher2_cluster_v2.production.kube_config
  sensitive = true
}
----

==== Configuración de nodos y roles

Los nodos en Kubernetes pueden tener diferentes roles según su función en el clúster.

**Roles de nodos en Rancher**

.Roles disponibles:
[cols="1,2,2", options="header"]
|===
|Rol |Función |Requisitos mínimos

|etcd
|Base de datos del clúster (key-value store)
|2 CPU, 4 GB RAM, 50 GB disco

|Control Plane
|API server, scheduler, controller manager
|2 CPU, 4 GB RAM, 50 GB disco

|Worker
|Ejecuta workloads (pods)
|Según workloads (mín: 2 CPU, 4 GB RAM)
|===

**Arquitecturas recomendadas**

.Clúster pequeño (desarrollo/testing):
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent
skinparam rectangle {
  BackgroundColor<<etcd>> LightYellow
  BackgroundColor<<controlplane>> LightBlue
  BackgroundColor<<worker>> LightGreen
}

RECTANGLE "Node 1" <<etcd>> as n1 {
  [etcd]
  [Control Plane]
  [Worker]
}

RECTANGLE "Node 2" <<etcd>> as n2 {
  [etcd]
  [Control Plane]
  [Worker]
}

RECTANGLE "Node 3" <<etcd>> as n3 {
  [etcd]
  [Control Plane]
  [Worker]
}

note right of n1
  Todos los roles combinados
  3 nodos mínimo para HA
  Económico pero menos aislado
end note

@enduml
----

.Clúster mediano (staging):
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

RECTANGLE "Control Plane Pool" as cp {
  RECTANGLE "Node 1" {
    [etcd]
    [Control Plane]
  }
  RECTANGLE "Node 2" {
    [etcd]
    [Control Plane]
  }
  RECTANGLE "Node 3" {
    [etcd]
    [Control Plane]
  }
}

RECTANGLE "Worker Pool" as wp {
  RECTANGLE "Worker 1" {
    [Worker]
  }
  RECTANGLE "Worker 2" {
    [Worker]
  }
  RECTANGLE "Worker 3" {
    [Worker]
  }
  RECTANGLE "..." {
  }
}

cp --> wp : gestiona

note right of cp
  Control plane dedicado
  Workers separados
  Mejor aislamiento
  Escalado independiente
end note

@enduml
----

.Clúster grande (producción):
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

RECTANGLE "etcd Pool" as etcd {
  RECTANGLE "etcd 1"
  RECTANGLE "etcd 2"
  RECTANGLE "etcd 3"
}

RECTANGLE "Control Plane Pool" as cp {
  RECTANGLE "CP 1" {
    [API Server]
    [Scheduler]
    [Controller]
  }
  RECTANGLE "CP 2" {
    [API Server]
    [Scheduler]
    [Controller]
  }
  RECTANGLE "CP 3" {
    [API Server]
    [Scheduler]
    [Controller]
  }
}

RECTANGLE "Worker Pool General" as wg {
  RECTANGLE "Worker 1-N"
}

RECTANGLE "Worker Pool GPU" as gpu {
  RECTANGLE "GPU Worker 1-M"
}

etcd --> cp : datos
cp --> wg : gestiona
cp --> gpu : gestiona

note right of etcd
  Máxima separación
  etcd dedicado
  Control plane dedicado
  Workers especializados
  Mejor rendimiento
end note

@enduml
----

**Taints y tolerations para workers especializados**

.Aplicar taints a nodos:
[source,bash]
----
# Via kubectl (después de crear clúster)
kubectl taint nodes gpu-node-1 dedicated=gpu:NoSchedule

# Via labels en Rancher durante creación:
# En Node Template o Machine Pool:
# Taints: dedicated=gpu:NoSchedule

# Para remover taint
kubectl taint nodes gpu-node-1 dedicated=gpu:NoSchedule-
----

.Configurar tolerations en deployments:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-training
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ml-training
  template:
    metadata:
      labels:
        app: ml-training
    spec:
      tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"
      nodeSelector:
        accelerator: nvidia-tesla-v100
      containers:
      - name: training
        image: tensorflow/tensorflow:latest-gpu
        resources:
          limits:
            nvidia.com/gpu: 1
----

**Node labels y selectors**

.Añadir labels personalizados:
[source,bash]
----
# Via kubectl
kubectl label nodes worker-1 workload.type=memory-intensive
kubectl label nodes worker-2 workload.type=cpu-intensive
kubectl label nodes worker-3 zone=us-east-1a
kubectl label nodes worker-4 storage=ssd

# Ver labels de un nodo
kubectl get node worker-1 --show-labels

# Usar nodeSelector en pod
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  nodeSelector:
    workload.type: memory-intensive
    storage: ssd
  containers:
  - name: redis
    image: redis:7
----

==== Personalización de versiones de Kubernetes

Rancher soporta múltiples versiones de Kubernetes simultáneamente.

**Versiones disponibles**

.Listar versiones disponibles:
[source,bash]
----
# Via Rancher UI:
# Cluster Management → Create → Ver dropdown de Kubernetes Version

# Via API:
curl -sk -X GET "${RANCHER_URL}/v1/management.cattle.io.k8sversions" \
  -H "Authorization: Bearer ${API_TOKEN}" | \
  jq -r '.data[] | "\(.id) - \(.serverVersion)"'

# Output ejemplo:
# v1.28.15+rke2r1 - v1.28.15
# v1.27.16+rke2r1 - v1.27.16
# v1.26.15+rke2r1 - v1.26.15
----

**Matriz de compatibilidad**

.Versiones soportadas por Rancher 2.9.x:
[cols="1,1,1,1", options="header"]
|===
|Rancher Version |Kubernetes Min |Kubernetes Max |Recomendada

|2.9.3
|1.26.x
|1.30.x
|1.28.15

|2.9.2
|1.25.x
|1.29.x
|1.28.14

|2.9.1
|1.25.x
|1.29.x
|1.27.16

|2.8.x
|1.24.x
|1.28.x
|1.27.x
|===

**Seleccionar versión específica**

.Durante creación de clúster:
[source,text]
----
1. Rancher UI → Create Cluster
2. Kubernetes Version dropdown:
   - v1.28.15+rke2r1 (Recomendado)
   - v1.27.16+rke2r1 (Stable, LTS)
   - v1.29.10+rke2r1 (Latest)
   
Consideraciones:
- Usar versiones N-1 o N-2 para estabilidad
- Verificar compatibilidad con aplicaciones
- Revisar changelog de Kubernetes
- Testing en staging antes de producción
----

.Con Terraform:
[source,hcl]
----
resource "rancher2_cluster_v2" "cluster" {
  name               = "production"
  kubernetes_version = "v1.28.15+rke2r1"
  # ...
}
----

**Actualización de versión de Kubernetes**

.Actualizar clúster existente:
[source,text]
----
1. Cluster Management → Select cluster → ⋮ → Edit Config
2. Kubernetes Version → Seleccionar nueva versión
3. Upgrade Strategy:
   - Control Plane Concurrency: 1 (uno a la vez)
   - Worker Concurrency: 10% (10% de workers en paralelo)
   - Drain nodes: Yes
   - Max Unavailable Worker: 10%

4. Save → Upgrade will start automatically

Proceso automático:
1. Upgrade control plane nodes (uno por uno)
2. Upgrade etcd nodes (uno por uno)
3. Upgrade workers (en batches según concurrency)
4. Cada nodo es drenado antes de upgrade
5. Validación de health después de cada nodo
----

.Actualizar con kubectl:
[source,bash]
----
# Ver versión actual
kubectl version

# Rancher maneja el upgrade, pero puedes monitorearlo
kubectl get nodes -w

# Ver estado de upgrade en cada nodo
kubectl describe node <node-name> | grep -A 5 "System Info"
----

==== Configuración de networking y CNI

El CNI (Container Network Interface) es crítico para la comunicación entre pods.

**CNI disponibles en Rancher**

.Comparación de CNI:
[cols="1,2,1,1", options="header"]
|===
|CNI |Características |Rendimiento |Uso recomendado

|Calico
|Network policies, IPAM, BGP routing
|Muy bueno
|Producción, security-focused

|Cilium
|eBPF-based, observability, security
|Excelente
|Moderno, alta performance

|Canal
|Calico + Flannel (policies + simple networking)
|Bueno
|Balance simplicidad/features

|Flannel
|Simple overlay network, VXLAN
|Bueno
|Desarrollo, simple setups

|Multus
|Multiple NICs, SR-IOV
|Variable
|Telecom, NFV

|Weave
|Simple mesh network, encryption
|Aceptable
|Legacy, simplicidad
|===

**Configurar Cilium (recomendado)**

.Durante creación de clúster:
[source,text]
----
1. Cluster Creation → Network Provider: Cilium

2. Chart Values (opcional, avanzado):
   Click "Edit as YAML"
----

.Configuración personalizada de Cilium:
[source,yaml]
----
# En Cluster Creation → Chart Values → rke2-cilium

rke2-cilium:
  # IPAM
  ipam:
    mode: kubernetes  # o "cluster-pool" para IPAM de Cilium
    operator:
      clusterPoolIPv4PodCIDRList: ["10.42.0.0/16"]
      clusterPoolIPv4MaskSize: 24

  # Tunnel mode
  tunnel: vxlan  # o "disabled" para routing nativo

  # Hubble (observability)
  hubble:
    enabled: true
    relay:
      enabled: true
    ui:
      enabled: true
      service:
        type: LoadBalancer
    metrics:
      enabled:
        - dns
        - drop
        - tcp
        - flow
        - icmp
        - http

  # Security
  encryption:
    enabled: true
    type: wireguard  # o "ipsec"

  # Load Balancer (sin external LB)
  kubeProxyReplacement: true
  k8sServiceHost: <control-plane-vip>
  k8sServicePort: 6443

  # BGP (avanzado)
  bgpControlPlane:
    enabled: false

  # Monitoring
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true

  # Performance
  resources:
    limits:
      cpu: 4000m
      memory: 4Gi
    requests:
      cpu: 100m
      memory: 512Mi
----

**Configurar Calico**

.Configuración de Calico:
[source,yaml]
----
# En Cluster Creation → Chart Values → rke2-calico

rke2-calico:
  # Configuración básica
  installation:
    calicoNetwork:
      ipPools:
      - blockSize: 26
        cidr: 10.42.0.0/16
        encapsulation: VXLAN  # o "IPIP" o "None" (routing directo)
        natOutgoing: Enabled
        nodeSelector: all()

  # Network Policy
  networkPolicy:
    enabled: true

  # Typha (para clústeres grandes >50 nodos)
  typha:
    enabled: true
    replicas: 3

  # Felix (agent en cada nodo)
  felix:
    bpfEnabled: false  # true para eBPF (requiere kernel 5.3+)
    chainInsertMode: Insert
    logSeverityScreen: Info
    prometheusMetricsEnabled: true

  # BGP configuration
  bgp:
    enabled: false  # true si usas BGP peering
    # bgpPeers:
    # - peerIP: 192.168.1.1
    #   asNumber: 64512
----

**Verificar networking después de creación**

.Comandos de verificación:
[source,bash]
----
# 1. Ver CNI instalado
kubectl get pods -n kube-system | grep -E 'calico|cilium|flannel'

# 2. Ver configuración de red
kubectl get nodes -o wide

# 3. Test de conectividad entre pods
kubectl run test-1 --image=busybox --rm -it -- sh
# Dentro del pod:
wget -O- http://kubernetes.default.svc.cluster.local

# 4. Test de DNS
kubectl run test-dns --image=busybox --rm -it -- nslookup kubernetes.default

# 5. Ver IP pools (Calico)
kubectl get ippools -o yaml

# 6. Ver CiliumNodes (Cilium)
kubectl get ciliumnodes

# 7. Test de network policy
# Crear test namespace
kubectl create ns test-netpol
kubectl run web --image=nginx -n test-netpol
kubectl expose pod web --port=80 -n test-netpol

# Crear cliente
kubectl run client --image=busybox -n test-netpol --rm -it -- sh
# Dentro: wget -O- http://web  (debería funcionar)

# Aplicar network policy (deny all)
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: test-netpol
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF

# Reintentar conexión (debería fallar)
kubectl run client --image=busybox -n test-netpol --rm -it -- wget -O- http://web --timeout=5
----

**Configuración avanzada: Multiple CNI con Multus**

Para casos de uso avanzados (telco, NFV, múltiples interfaces).

.Instalar Multus:
[source,yaml]
----
# Multus como CNI adicional (después de crear clúster con CNI primario)
apiVersion: v1
kind: ConfigMap
metadata:
  name: multus-cni-config
  namespace: kube-system
data:
  cni-conf.json: |
    {
      "name": "multus-cni-network",
      "type": "multus",
      "delegates": [
        {
          "cniVersion": "0.3.1",
          "name": "default-cni",
          "type": "cilium"
        }
      ],
      "kubeconfig": "/etc/cni/net.d/multus.d/multus.kubeconfig"
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-multus-ds
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: multus
  template:
    metadata:
      labels:
        name: multus
    spec:
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      containers:
      - name: kube-multus
        image: ghcr.io/k8snetworkplumbingwg/multus-cni:stable
        command: ["/entrypoint.sh"]
        args: ["--multus-conf-file=auto"]
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
          limits:
            cpu: 200m
            memory: 100Mi
        securityContext:
          privileged: true
        volumeMounts:
        - name: cni
          mountPath: /host/etc/cni/net.d
        - name: cnibin
          mountPath: /host/opt/cni/bin
      volumes:
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: cnibin
        hostPath:
          path: /opt/cni/bin
----

**Troubleshooting networking**

.Diagnóstico de problemas comunes:
[source,bash]
----
#!/bin/bash
echo "=== Diagnóstico de Networking Kubernetes ==="

# 1. Estado de pods CNI
echo "1. Estado de CNI pods:"
kubectl get pods -n kube-system -l k8s-app=cilium -o wide
kubectl get pods -n kube-system -l k8s-app=calico-node -o wide

# 2. Ver logs de CNI
echo "2. Logs de CNI:"
kubectl logs -n kube-system -l k8s-app=cilium --tail=50

# 3. Verificar IP forwarding en nodos
echo "3. IP forwarding:"
kubectl get nodes -o name | while read node; do
  echo "$node:"
  kubectl debug $node -it --image=busybox -- sysctl net.ipv4.ip_forward
done

# 4. Verificar firewall rules
echo "4. Firewall rules:"
kubectl get nodes -o name | while read node; do
  echo "$node:"
  kubectl debug $node -it --image=nicolaka/netshoot -- iptables -L -n | grep -i forward
done

# 5. Test de conectividad cluster
echo "5. Test de conectividad:"
kubectl run nettest --image=nicolaka/netshoot --rm -it -- bash -c "
  ping -c 3 8.8.8.8
  nslookup kubernetes.default
  curl -I https://kubernetes.default.svc.cluster.local:443 -k
"

# 6. Ver rutas en nodos
echo "6. Rutas:"
kubectl get nodes -o name | head -1 | while read node; do
  kubectl debug $node -it --image=nicolaka/netshoot -- ip route
done

echo "=== Fin diagnóstico ==="
----

==== Backup y restauración de clústeres

El backup regular de clústeres es crítico para disaster recovery y debe cubrir tanto datos del cluster (etcd) como datos de aplicaciones.

**Componentes a respaldar en un clúster**

.Elementos críticos:
[cols="1,2,1", options="header"]
|===
|Componente |Contenido |Método

|etcd
|Estado del clúster Kubernetes (pods, services, configs)
|Snapshots automáticos/manuales

|Persistent Volumes
|Datos de aplicaciones
|Volume snapshots, Velero

|ConfigMaps/Secrets
|Configuraciones y credenciales
|Velero, kubectl export

|CRDs
|Custom Resource Definitions
|Velero, kubectl export

|Namespaces
|Objetos de aplicaciones
|Velero

|RBAC
|Roles, RoleBindings, ServiceAccounts
|Velero, kubectl export
|===

**Método 1: Backup de etcd (incluido en RKE/RKE2)**

RKE2 y RKE1 incluyen backup automático de etcd.

.Configurar backup automático de etcd (RKE2):
[source,yaml]
----
# Al crear clúster en Rancher, en Advanced Options → etcd

# O editar clúster existente:
# Cluster → Edit Config → Advanced → etcd
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: production-cluster
spec:
  rkeConfig:
    etcd:
      # Backup local en cada nodo etcd
      snapshotScheduleCron: "0 */6 * * *"  # Cada 6 horas
      snapshotRetention: 14  # Mantener 14 snapshots
      
      # Backup a S3 (recomendado)
      s3:
        bucket: rancher-cluster-backups
        cloudCredentialName: aws-creds  # Cloud credential en Rancher
        endpoint: s3.amazonaws.com
        region: us-east-1
        folder: production-cluster
        skipSSLVerify: false
----

.Verificar backups de etcd:
[source,bash]
----
# Listar snapshots locales (en nodos etcd)
# SSH al nodo etcd
ls -lh /var/lib/rancher/rke2/server/db/snapshots/

# Listar snapshots en S3
aws s3 ls s3://rancher-cluster-backups/production-cluster/ --recursive

# Ver configuración de backup del clúster
kubectl get cluster.provisioning.cattle.io production-cluster -o yaml | grep -A 10 etcd

# Forzar snapshot manual (via Rancher UI)
# Cluster → ⋮ → Take Snapshot

# O via API
curl -k -X POST "${RANCHER_URL}/v1/provisioning.cattle.io.clusters/${CLUSTER_ID}?action=backupEtcd" \
  -H "Authorization: Bearer ${API_TOKEN}"
----

**Método 2: Backup completo con Velero**

Velero es el estándar de facto para backup de clústeres Kubernetes.

.Instalar Velero en el clúster:
[source,bash]
----
# 1. Instalar Velero CLI
wget https://github.com/vmware-tanzu/velero/releases/download/v1.14.0/velero-v1.14.0-linux-amd64.tar.gz
tar -xvf velero-v1.14.0-linux-amd64.tar.gz
sudo mv velero-v1.14.0-linux-amd64/velero /usr/local/bin/
velero version

# 2. Configurar credenciales S3
cat > credentials-velero <<EOF
[default]
aws_access_key_id = <AWS_ACCESS_KEY_ID>
aws_secret_access_key = <AWS_SECRET_ACCESS_KEY>
EOF

# 3. Instalar Velero en el clúster
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.10.0 \
  --bucket rancher-velero-backups \
  --secret-file ./credentials-velero \
  --backup-location-config region=us-east-1 \
  --snapshot-location-config region=us-east-1 \
  --use-volume-snapshots=true \
  --use-node-agent \
  --uploader-type=restic

# 4. Verificar instalación
kubectl get pods -n velero
velero version
----

.Configurar backups automáticos con Velero:
[source,yaml]
----
# Backup schedule diario de todo el clúster
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-cluster-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  template:
    includedNamespaces:
    - '*'
    excludedNamespaces:
    - kube-system
    - kube-public
    - velero
    includedResources:
    - '*'
    includeClusterResources: true
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 720h0m0s  # Mantener 30 días
    snapshotVolumes: true
    defaultVolumesToFsBackup: false
----

.Backup manual con Velero:
[source,bash]
----
# Backup completo del clúster
velero backup create full-backup-$(date +%Y%m%d) \
  --include-namespaces '*' \
  --include-cluster-resources=true

# Backup de namespace específico
velero backup create app-backup \
  --include-namespaces production \
  --include-cluster-resources=false

# Backup con volúmenes (usando restic)
velero backup create app-with-data \
  --include-namespaces production \
  --default-volumes-to-fs-backup

# Ver estado del backup
velero backup describe full-backup-20251030

# Ver logs del backup
velero backup logs full-backup-20251030

# Listar todos los backups
velero backup get
----

**Restauración desde backup de etcd**

.Restaurar clúster desde snapshot de etcd:
[source,bash]
----
# IMPORTANTE: Esto recrea completamente el clúster
# Todos los nodos deben estar disponibles

# Método 1: Via Rancher UI
# 1. Cluster → ⋮ → Restore Snapshot
# 2. Seleccionar snapshot
# 3. Confirm

# Método 2: Via API
curl -k -X POST "${RANCHER_URL}/v1/provisioning.cattle.io.clusters/${CLUSTER_ID}?action=restoreSnapshot" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "snapshotName": "on-demand-production-cluster-20251030-120000"
  }'

# Método 3: Restauración manual en RKE2 (disaster recovery completo)
# En el primer nodo etcd:
systemctl stop rke2-server

# Restaurar snapshot
rke2 server \
  --cluster-reset \
  --cluster-reset-restore-path=/var/lib/rancher/rke2/server/db/snapshots/etcd-snapshot-production-20251030-120000 \
  --token <existing-cluster-token>

# Esperar a que complete
# Iniciar RKE2
systemctl start rke2-server

# Verificar
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
kubectl get nodes

# En los nodos adicionales (si es clúster multi-master):
# Eliminar datos viejos y rejoin
systemctl stop rke2-server
rm -rf /var/lib/rancher/rke2/server/db
systemctl start rke2-server
----

**Restauración con Velero**

.Restaurar desde backup de Velero:
[source,bash]
----
# Listar backups disponibles
velero backup get

# Restauración completa
velero restore create --from-backup full-backup-20251030

# Restaurar solo un namespace
velero restore create --from-backup full-backup-20251030 \
  --include-namespaces production

# Restaurar con mapeo de namespace (restaurar a diferente namespace)
velero restore create --from-backup full-backup-20251030 \
  --namespace-mappings production:production-restored

# Restaurar solo recursos específicos
velero restore create --from-backup full-backup-20251030 \
  --include-resources deployments,services,configmaps

# Ver estado de restauración
velero restore describe <restore-name>

# Ver logs de restauración
velero restore logs <restore-name>

# Listar todas las restauraciones
velero restore get
----

**Backup de aplicaciones específicas**

.Ejemplo: Backup de base de datos PostgreSQL:
[source,yaml]
----
# Backup con pre/post hooks para consistency
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: postgres-backup
  namespace: velero
spec:
  includedNamespaces:
  - database
  includedResources:
  - '*'
  hooks:
    resources:
    - name: postgres-backup-hook
      includedNamespaces:
      - database
      labelSelector:
        matchLabels:
          app: postgresql
      pre:
      - exec:
          container: postgresql
          command:
          - /bin/bash
          - -c
          - 'PGPASSWORD=$POSTGRES_PASSWORD pg_dump -U postgres -h localhost mydb > /tmp/backup.sql'
          timeout: 3m
      post:
      - exec:
          container: postgresql
          command:
          - /bin/bash
          - -c
          - 'rm -f /tmp/backup.sql'
  volumeSnapshotLocations:
  - default
  storageLocation: default
  ttl: 720h0m0s
----

**Disaster Recovery completo**

.Procedimiento DR completo de clúster:
[source,bash]
----
#!/bin/bash
# Script de Disaster Recovery completo

set -e

echo "=== Disaster Recovery de Clúster Kubernetes ==="

# Variables
CLUSTER_NAME="production-cluster"
BACKUP_DATE="20251030"
S3_BUCKET="rancher-cluster-backups"

# 1. Crear nuevo clúster en Rancher (o usar existente)
echo "1. Asegurarse de tener clúster destino creado en Rancher"

# 2. Restaurar etcd snapshot (si aplicable)
echo "2. Restaurando snapshot de etcd..."
# Via Rancher UI o API (ver sección anterior)

# 3. Instalar Velero en clúster destino
echo "3. Instalando Velero..."
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.10.0 \
  --bucket ${S3_BUCKET} \
  --secret-file ./credentials-velero \
  --backup-location-config region=us-east-1

# 4. Esperar a que Velero esté listo
kubectl wait --for=condition=Available --timeout=300s -n velero deployment/velero

# 5. Listar backups disponibles
echo "5. Backups disponibles:"
velero backup get

# 6. Restaurar desde backup más reciente
echo "6. Restaurando desde backup..."
BACKUP_NAME=$(velero backup get -o json | jq -r '.items | sort_by(.status.completionTimestamp) | last | .metadata.name')
velero restore create dr-restore-$(date +%s) --from-backup ${BACKUP_NAME} --wait

# 7. Verificar restauración
echo "7. Verificando recursos restaurados..."
kubectl get all -A

# 8. Verificar PVCs
echo "8. Verificando Persistent Volume Claims..."
kubectl get pvc -A

# 9. Verificar aplicaciones críticas
echo "9. Verificando aplicaciones..."
kubectl get pods -A | grep -v "Running\|Completed"

# 10. Verificar services y endpoints
echo "10. Verificando servicios..."
kubectl get svc,ep -A

# 11. Test de conectividad
echo "11. Testing conectividad..."
kubectl run test-dr --image=busybox --rm -it -- wget -O- http://myapp.production.svc.cluster.local

echo "=== DR completado ==="
echo "Verificar manualmente:"
echo "- Aplicaciones accesibles"
echo "- Datos de bases de datos"
echo "- Certificados y secrets"
echo "- Configuración de DNS"
----

**Best practices para backup**

.Recomendaciones:
[source,text]
----
1. Estrategia 3-2-1:
   - 3 copias de los datos
   - 2 medios de almacenamiento diferentes
   - 1 copia offsite (S3 en otra región)

2. Frecuencia de backups:
   - etcd: Cada 6-12 horas
   - Aplicaciones críticas: Cada hora
   - Aplicaciones normales: Diario
   - Backup manual antes de cambios grandes

3. Retención:
   - Diarios: 7 días
   - Semanales: 4 semanas
   - Mensuales: 12 meses
   - Antes de upgrades: Permanente (marcar)

4. Testing:
   - Test de restauración mensual en clúster de prueba
   - Documentar RTO (Recovery Time Objective): < 1 hora
   - Documentar RPO (Recovery Point Objective): < 6 horas
   - Automatizar proceso DR

5. Seguridad:
   - Cifrar backups en reposo (S3 encryption)
   - Cifrar backups en tránsito (TLS)
   - Rotar credenciales de backup
   - Restricir acceso a backups (IAM policies)
   - Audit logging de acceso a backups

6. Monitoreo:
   - Alertar si backup falla
   - Verificar integridad de backups
   - Monitorear tamaño de backups
   - Dashboard con última fecha de backup exitoso
----

**Verificación de backups**

.Script de verificación de backups:
[source,bash]
----
#!/bin/bash
echo "=== Verificación de Backups ==="

# 1. Verificar último snapshot de etcd
echo "1. Último snapshot etcd:"
aws s3 ls s3://rancher-cluster-backups/production-cluster/ --recursive | \
  sort | tail -1

# 2. Verificar backups de Velero
echo "2. Últimos backups de Velero:"
velero backup get | tail -5

# 3. Verificar estado del último backup
LAST_BACKUP=$(velero backup get -o json | jq -r '.items | sort_by(.status.completionTimestamp) | last | .metadata.name')
echo "3. Estado del último backup: $LAST_BACKUP"
velero backup describe $LAST_BACKUP | grep -E "Phase|Warnings|Errors"

# 4. Verificar tamaño de backups
echo "4. Tamaño de backups en S3:"
aws s3 ls s3://rancher-velero-backups/ --recursive --human-readable --summarize

# 5. Verificar schedule de backups
echo "5. Schedules configurados:"
velero schedule get

# 6. Verificar última ejecución de schedule
echo "6. Última ejecución de schedules:"
velero schedule describe daily-cluster-backup | grep "Last Backup"

# 7. Test de integridad (restaurar en clúster temporal)
echo "7. Para test completo, ejecutar:"
echo "   - Crear clúster temporal"
echo "   - velero restore create test-restore --from-backup $LAST_BACKUP"
echo "   - Verificar recursos"
echo "   - Eliminar clúster temporal"

echo "=== Fin verificación ==="
----

==== Monitoreo del estado de salud

El monitoreo continuo del clúster es esencial para detectar problemas antes de que afecten a producción.

**Componentes a monitorear**

.Métricas críticas:
[cols="1,2,1", options="header"]
|===
|Categoría |Métricas |Umbral crítico

|Nodos
|CPU, memoria, disco, red
|CPU > 80%, Memoria > 85%

|Control Plane
|API server latency, etcd latency
|Latency > 100ms

|etcd
|DB size, leader changes, proposals
|Size > 8GB, Changes > 3/hour

|Pods
|Restarts, OOMKilled, CrashLoopBackOff
|Restarts > 5/hour

|Red
|Packet loss, latency entre nodos
|Loss > 1%, Latency > 10ms

|Almacenamiento
|PV usage, I/O wait
|Usage > 85%, I/O wait > 20%
|===

**Método 1: Rancher Monitoring (Prometheus + Grafana)**

Solución integrada y recomendada.

.Instalar Rancher Monitoring:
[source,text]
----
# Via Rancher UI:
1. Cluster → Apps → Charts
2. Buscar "Rancher Monitoring"
3. Install

Configuración:
- Prometheus Storage: 50Gi (PVC)
- Retention: 15d
- Grafana Enabled: Yes
- Grafana Storage: 10Gi (PVC)
- AlertManager Enabled: Yes

Resources:
- Prometheus:
  - CPU: 1000m
  - Memory: 3000Mi
- Grafana:
  - CPU: 200m
  - Memory: 200Mi

4. Install
----

.Configuración avanzada de Monitoring:
[source,yaml]
----
# Cluster → Apps → Installed Apps → rancher-monitoring → Edit YAML

prometheus:
  prometheusSpec:
    # Recursos
    resources:
      requests:
        cpu: 1000m
        memory: 3000Mi
      limits:
        cpu: 2000m
        memory: 6000Mi
    
    # Storage
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: fast-ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    
    # Retención
    retention: 30d
    retentionSize: 90GB
    
    # Scrape interval
    scrapeInterval: 30s
    evaluationInterval: 30s
    
    # External labels para federación
    externalLabels:
      cluster: production
      region: us-east-1
      environment: prod
    
    # Remote write (para long-term storage)
    remoteWrite:
    - url: https://prometheus-remote.example.com/api/v1/write
      basicAuth:
        username:
          name: remote-write-secret
          key: username
        password:
          name: remote-write-secret
          key: password

grafana:
  enabled: true
  adminPassword: SecurePassword123!
  
  persistence:
    enabled: true
    storageClassName: fast-ssd
    size: 10Gi
  
  # Ingress para acceso externo
  ingress:
    enabled: true
    hosts:
    - grafana.example.com
    tls:
    - secretName: grafana-tls
      hosts:
      - grafana.example.com

alertmanager:
  enabled: true
  
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: fast-ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
  
  # Configuración de alertas
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXX'
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
      - match:
          severity: critical
        receiver: critical
        continue: true
      - match:
          severity: warning
        receiver: warning
    
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'critical'
      slack_configs:
      - channel: '#critical-alerts'
        title: '🚨 CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
      pagerduty_configs:
      - service_key: <pagerduty-integration-key>
    
    - name: 'warning'
      slack_configs:
      - channel: '#warning-alerts'
        title: '⚠️  Warning: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
----

**Acceder a Grafana**

.Acceso a dashboards:
[source,bash]
----
# Obtener contraseña de Grafana admin
kubectl get secret -n cattle-monitoring-system rancher-monitoring-grafana \
  -o jsonpath='{.data.admin-password}' | base64 -d

# Port-forward si no hay ingress
kubectl port-forward -n cattle-monitoring-system \
  svc/rancher-monitoring-grafana 3000:80

# Abrir navegador
open http://localhost:3000
# Usuario: admin
# Contraseña: <obtenida arriba>

# Dashboards incluidos:
# - Kubernetes / Compute Resources / Cluster
# - Kubernetes / Compute Resources / Namespace (Pods)
# - Kubernetes / Compute Resources / Node (Pods)
# - Kubernetes / Networking / Cluster
# - etcd
# - Kubernetes / API server
# - Kubernetes / Kubelet
----

**Crear alertas personalizadas**

.PrometheusRule para alertas custom:
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: custom-cluster-alerts
  namespace: cattle-monitoring-system
  labels:
    release: rancher-monitoring
spec:
  groups:
  - name: cluster-health
    interval: 30s
    rules:
    
    # Alerta: CPU alto en nodos
    - alert: NodeHighCPU
      expr: |
        (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} has high CPU usage"
        description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"
    
    # Alerta: Memoria alta en nodos
    - alert: NodeHighMemory
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} has high memory usage"
        description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"
    
    # Alerta: Disco casi lleno
    - alert: NodeDiskAlmostFull
      expr: |
        (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Disk almost full on {{ $labels.instance }}"
        description: "Disk usage is {{ $value }}% on {{ $labels.instance }}"
    
    # Alerta: Nodo no disponible
    - alert: NodeNotReady
      expr: |
        kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} is not ready"
        description: "Node {{ $labels.node }} has been unready for more than 5 minutes"
    
    # Alerta: Pod en CrashLoopBackOff
    - alert: PodCrashLooping
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod has restarted {{ $value }} times in the last 15 minutes"
    
    # Alerta: etcd DB size grande
    - alert: EtcdDatabaseSizeLarge
      expr: |
        etcd_mvcc_db_total_size_in_bytes / 1024 / 1024 / 1024 > 8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "etcd database size is large"
        description: "etcd DB size is {{ $value }}GB"
    
    # Alerta: API server latency alto
    - alert: APIServerHighLatency
      expr: |
        histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!="WATCH"}[5m])) by (verb, le)) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "API server has high latency"
        description: "API server 99th percentile latency is {{ $value }}s"
    
    # Alerta: PVC casi lleno
    - alert: PVCAlmostFull
      expr: |
        (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is almost full"
        description: "PVC usage is {{ $value }}%"
----

**Método 2: Monitoreo básico con kubectl**

Para clústeres sin Prometheus.

.Script de monitoreo básico:
[source,bash]
----
#!/bin/bash
# Script de health check básico

echo "=== Cluster Health Check ==="
echo "Timestamp: $(date)"
echo ""

# 1. Estado de nodos
echo "1. Node Status:"
kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory,VERSION:.status.nodeInfo.kubeletVersion
echo ""

# 2. Pods no saludables
echo "2. Unhealthy Pods:"
kubectl get pods -A --field-selector status.phase!=Running,status.phase!=Succeeded | tail -n +2
echo ""

# 3. Resource usage de nodos
echo "3. Node Resource Usage:"
kubectl top nodes
echo ""

# 4. Pods con más uso de recursos
echo "4. Top 10 Pods by CPU:"
kubectl top pods -A --sort-by=cpu | head -11
echo ""

echo "5. Top 10 Pods by Memory:"
kubectl top pods -A --sort-by=memory | head -11
echo ""

# 6. PVC usage
echo "6. PVC Status:"
kubectl get pvc -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase,CAPACITY:.spec.resources.requests.storage,STORAGECLASS:.spec.storageClassName
echo ""

# 7. Events recientes (errores/warnings)
echo "7. Recent Warning/Error Events:"
kubectl get events -A --field-selector type!=Normal --sort-by='.lastTimestamp' | tail -20
echo ""

# 8. Control plane components
echo "8. Control Plane Components:"
kubectl get componentstatuses 2>/dev/null || kubectl get pods -n kube-system -l tier=control-plane
echo ""

# 9. Critical pods
echo "9. Critical System Pods:"
kubectl get pods -n kube-system -o custom-columns=NAME:.metadata.name,STATUS:.status.phase,RESTARTS:.status.containerStatuses[0].restartCount,AGE:.metadata.creationTimestamp
echo ""

# 10. Cluster-wide resource requests
echo "10. Cluster-wide Resource Requests:"
kubectl describe nodes | grep -A 5 "Allocated resources"

echo ""
echo "=== Health Check Complete ==="
----

**Método 3: Rancher Cluster Explorer**

Interface visual integrada en Rancher.

.Usar Cluster Explorer:
[source,text]
----
1. Rancher UI → Cluster Management → Select cluster → Explore

2. Overview:
   - Cluster status
   - Node count y health
   - Resource usage (CPU, Memory)
   - Pod count por namespace
   - Events recientes

3. Nodes:
   - Lista de nodos con estado
   - CPU/Memory usage por nodo
   - Conditions (Ready, DiskPressure, MemoryPressure)
   - Taints y labels

4. Monitoring (si está instalado):
   - Gráficas de CPU, Memory, Network, Disk
   - Dashboards predefinidos
   - Métricas en tiempo real

5. Events:
   - Eventos del clúster en tiempo real
   - Filtrar por tipo (Warning, Error, Normal)
   - Ver logs de eventos

6. Workloads:
   - Estado de Deployments, StatefulSets, DaemonSets
   - Pod status y health
   - Resource usage por workload
----

**Exportar métricas para sistemas externos**

.Integración con Datadog:
[source,yaml]
----
# Instalar Datadog agent
apiVersion: v1
kind: Secret
metadata:
  name: datadog-secret
  namespace: datadog
type: Opaque
data:
  api-key: <BASE64_ENCODED_DATADOG_API_KEY>
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: datadog-agent
  namespace: datadog
spec:
  selector:
    matchLabels:
      app: datadog-agent
  template:
    metadata:
      labels:
        app: datadog-agent
    spec:
      serviceAccountName: datadog-agent
      containers:
      - name: agent
        image: gcr.io/datadoghq/agent:latest
        env:
        - name: DD_API_KEY
          valueFrom:
            secretKeyRef:
              name: datadog-secret
              key: api-key
        - name: DD_SITE
          value: "datadoghq.com"
        - name: DD_KUBERNETES_KUBELET_NODENAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: DD_COLLECT_KUBERNETES_EVENTS
          value: "true"
        - name: DD_APM_ENABLED
          value: "true"
        - name: DD_LOGS_ENABLED
          value: "true"
        - name: DD_PROCESS_AGENT_ENABLED
          value: "true"
        volumeMounts:
        - name: dockersocket
          mountPath: /var/run/docker.sock
          readOnly: true
        - name: procdir
          mountPath: /host/proc
          readOnly: true
        - name: cgroups
          mountPath: /host/sys/fs/cgroup
          readOnly: true
      volumes:
      - name: dockersocket
        hostPath:
          path: /var/run/docker.sock
      - name: procdir
        hostPath:
          path: /proc
      - name: cgroups
        hostPath:
          path: /sys/fs/cgroup
----

**Dashboard de salud general**

.Crear dashboard custom en Grafana:
[source,json]
----
{
  "dashboard": {
    "title": "Cluster Health Overview",
    "panels": [
      {
        "title": "Cluster Status",
        "type": "stat",
        "targets": [
          {
            "expr": "count(kube_node_status_condition{condition=\"Ready\",status=\"true\"})"
          }
        ]
      },
      {
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)"
          }
        ]
      },
      {
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100"
          }
        ]
      },
      {
        "title": "Pod Restarts (Last Hour)",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(increase(kube_pod_container_status_restarts_total[1h]))"
          }
        ]
      },
      {
        "title": "API Server Requests",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(apiserver_request_total[5m])) by (verb)"
          }
        ]
      },
      {
        "title": "etcd Size",
        "type": "gauge",
        "targets": [
          {
            "expr": "etcd_mvcc_db_total_size_in_bytes / 1024 / 1024 / 1024"
          }
        ]
      }
    ]
  }
}
----

**Mejores prácticas de monitoreo**

.Recomendaciones:
[source,text]
----
1. Definir SLIs (Service Level Indicators):
   - Latencia API: p99 < 100ms
   - Disponibilidad: 99.9% uptime
   - Tasa de error: < 0.1%

2. Configurar alertas inteligentes:
   - Evitar alert fatigue (no alertar por todo)
   - Usar umbrales basados en baseline
   - Agrupar alertas relacionadas
   - Definir severity levels (critical/warning/info)

3. Retención de métricas:
   - Alta resolución: 15 días
   - Resolución media: 90 días
   - Resolución baja: 1 año
   - Agregar datos viejos (downsampling)

4. Dashboards por rol:
   - Executive: Uptime, SLA, costs
   - Operations: Node health, resource usage
   - Developers: Application metrics, logs
   - Security: Audit logs, anomalies

5. Automatización:
   - Auto-scaling basado en métricas
   - Auto-remediation para problemas conocidos
   - Integración con ChatOps (Slack/Teams)
   - Runbooks automáticos
----

== Módulo 4: Seguridad y Gobernanza

=== 4.1 Gestión de usuarios y roles

Rancher proporciona un sistema completo de gestión de identidad y acceso (IAM) que integra autenticación externa con autorización granular basada en RBAC de Kubernetes.

==== Sistema de autenticación y autorización

**Arquitectura de autenticación en Rancher**

.Flujo de autenticación y autorización:
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

actor Usuario
participant "Rancher UI/API" as Rancher
participant "Auth Provider\n(LDAP/SAML/OIDC)" as Auth
participant "Rancher RBAC" as RBAC
participant "Kubernetes API" as K8s

Usuario -> Rancher: Login request
Rancher -> Auth: Authenticate user
Auth -> Auth: Verify credentials
Auth --> Rancher: User identity + groups
Rancher -> RBAC: Check user permissions
RBAC -> RBAC: Evaluate roles\n(Global/Cluster/Project)
RBAC --> Rancher: Permissions granted
Rancher -> K8s: Proxy request with\nRancher service account
K8s --> Rancher: Response
Rancher --> Usuario: Access granted

note right of RBAC
  Multi-level authorization:
  1. Global (Rancher level)
  2. Cluster (K8s cluster level)
  3. Project (Namespace group level)
end note

@enduml
----

**Niveles de autenticación**

.Métodos de autenticación soportados:
[cols="1,2,1,1", options="header"]
|===
|Método |Tipo |Enterprise |MFA Support

|Local
|Username/Password
|No
|No

|Active Directory
|LDAP
|Sí
|Via AD

|FreeIPA
|LDAP
|Sí
|Via FreeIPA

|Azure AD
|SAML/OIDC
|Sí
|Sí

|Keycloak
|OIDC
|Sí
|Sí

|Okta
|SAML
|Sí
|Sí

|GitHub
|OAuth
|No
|Via GitHub

|Google OAuth
|OAuth
|No
|Via Google

|SAML Generic
|SAML 2.0
|Sí
|Depends on IdP

|OpenLDAP
|LDAP
|Sí
|No
|===

**Configurar autenticación (revisión)**

Ya cubierto en módulo 2.3, pero recordatorio de configuración:

[source,bash]
----
# Via Rancher API
curl -k -X POST "${RANCHER_URL}/v3/activeDirectoryConfig" \
  -H "Authorization: Bearer ${API_TOKEN}" \
  -H 'Content-Type: application/json' \
  -d '{
    "enabled": true,
    "servers": ["ad.example.com"],
    "port": 636,
    "tls": true,
    "serviceAccountDistinguishedName": "CN=rancher,OU=Service Accounts,DC=example,DC=com",
    "serviceAccountPassword": "password",
    "userSearchBase": "OU=Users,DC=example,DC=com",
    "groupSearchBase": "OU=Groups,DC=example,DC=com"
  }'
----

**Autorización multi-nivel**

Rancher implementa autorización en tres niveles jerárquicos:

.Jerarquía de autorización:
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

RECTANGLE "Global Scope" as global {
  note
    - Acceso a Rancher completo
    - Gestión de usuarios
    - Gestión de auth providers
    - Gestión de clusters
  end note
}

RECTANGLE "Cluster Scope" as cluster {
  note
    - Acceso a un cluster específico
    - Gestión de nodos
    - Gestión de namespaces
    - Gestión de proyectos
    - Gestión de apps
  end note
}

RECTANGLE "Project Scope" as project {
  note
    - Acceso a grupo de namespaces
    - Gestión de workloads
    - Gestión de servicios
    - Gestión de secrets
  end note
}

RECTANGLE "Namespace Scope" as ns {
  note
    - RBAC nativo de Kubernetes
    - Roles y RoleBindings
  end note
}

global -down-> cluster : contiene
cluster -down-> project : contiene
project -down-> ns : agrupa

@enduml
----

==== Roles globales vs roles de clúster

**Roles Globales (Global Roles)**

Aplican a nivel de Rancher, controlando acceso a funcionalidades administrativas.

.Roles globales predefinidos:
[cols="1,2,2", options="header"]
|===
|Rol |Permisos |Uso recomendado

|Administrator
|Acceso completo a Rancher y todos los recursos
|Administradores de plataforma

|Standard User
|Crear clústeres, gestionar propios recursos
|Usuarios normales, developers

|User-Base
|Ver dashboard, acceso mínimo
|Usuarios con permisos específicos por cluster/project

|Restricted Admin
|Admin sin capacidad de modificar auth settings
|Admins delegados
|===

.Ver roles globales:
[source,bash]
----
# Listar roles globales
kubectl get globalroles

# Ver detalles de un rol
kubectl get globalrole admin -o yaml

# Ver bindings (asignaciones) de roles globales
kubectl get globalrolebindings

# Ver quién tiene rol de administrator
kubectl get globalrolebindings -o json | \
  jq '.items[] | select(.globalRoleName=="admin") | {user: .userName, group: .groupPrincipalName}'
----

.Crear rol global personalizado:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: GlobalRole
metadata:
  name: cluster-creator
spec:
  displayName: "Cluster Creator"
  description: "Can create and manage clusters but not modify global settings"
  rules:
  # Puede gestionar clusters
  - apiGroups:
    - "management.cattle.io"
    resources:
    - "clusters"
    verbs:
    - "*"
  
  # Puede ver cloud credentials
  - apiGroups:
    - "management.cattle.io"
    resources:
    - "cloudcredentials"
    verbs:
    - "get"
    - "list"
  
  # Puede ver node templates
  - apiGroups:
    - "management.cattle.io"
    resources:
    - "nodetemplates"
    verbs:
    - "*"
  
  # No puede modificar auth settings
  - apiGroups:
    - "management.cattle.io"
    resources:
    - "authconfigs"
    verbs:
    - "get"
    - "list"
----

.Asignar rol global a usuario:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: GlobalRoleBinding
metadata:
  name: user-john-cluster-creator
spec:
  globalRoleName: cluster-creator
  userName: "local://u-xxxxx"  # User ID de Rancher
---
# O asignar a grupo
apiVersion: management.cattle.io/v3
kind: GlobalRoleBinding
metadata:
  name: group-devops-cluster-creator
spec:
  globalRoleName: cluster-creator
  groupPrincipalName: "activedirectory_group://CN=DevOps,OU=Groups,DC=example,DC=com"
----

**Roles de Clúster (Cluster Roles)**

Controlan acceso a recursos dentro de un clúster específico.

.Roles de clúster predefinidos:
[cols="1,2,2", options="header"]
|===
|Rol |Permisos |Uso recomendado

|Cluster Owner
|Acceso completo al clúster
|Administradores del clúster

|Cluster Member
|Acceso lectura a la mayoría de recursos
|Usuarios que necesitan visibilidad del cluster

|View Only
|Solo lectura a recursos básicos
|Auditors, monitoring teams

|Create Projects
|Crear y gestionar proyectos
|Team leads

|Manage Cluster Members
|Gestionar usuarios del clúster
|Administradores delegados

|Manage Nodes
|Gestionar nodos del clúster
|Operations team
|===

.Ver roles de clúster:
[source,bash]
----
# Listar cluster role templates (templates de Rancher)
kubectl get clusterroletemplates

# Ver detalles
kubectl get clusterroletemplate cluster-owner -o yaml

# Ver asignaciones de roles en un clúster
kubectl get clusterroletemplatebindings -n <cluster-id>

# Ejemplo: Ver quién tiene acceso al clúster
kubectl get clusterroletemplatebindings -n c-m-xxxxx -o json | \
  jq '.items[] | {role: .roleTemplateName, user: .userName, group: .groupPrincipalName}'
----

.Crear rol de clúster personalizado:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: RoleTemplate
metadata:
  name: readonly-cluster-viewer
spec:
  builtin: false
  clusterCreatorDefault: false
  context: cluster
  displayName: "Read-Only Cluster Viewer"
  description: "Can view all resources but cannot modify anything"
  rules:
  # Ver nodos
  - apiGroups:
    - ""
    resources:
    - "nodes"
    verbs:
    - "get"
    - "list"
    - "watch"
  
  # Ver namespaces
  - apiGroups:
    - ""
    resources:
    - "namespaces"
    verbs:
    - "get"
    - "list"
    - "watch"
  
  # Ver todos los recursos de apps
  - apiGroups:
    - "apps"
    resources:
    - "*"
    verbs:
    - "get"
    - "list"
    - "watch"
  
  # Ver servicios
  - apiGroups:
    - ""
    resources:
    - "services"
    - "endpoints"
    verbs:
    - "get"
    - "list"
    - "watch"
  
  # Ver eventos
  - apiGroups:
    - ""
    resources:
    - "events"
    verbs:
    - "get"
    - "list"
    - "watch"
----

.Asignar rol de clúster a usuario:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: ClusterRoleTemplateBinding
metadata:
  name: john-viewer-binding
  namespace: c-m-xxxxx  # Cluster ID
spec:
  clusterName: c-m-xxxxx
  roleTemplateName: readonly-cluster-viewer
  userName: "local://u-yyyyy"
---
# O asignar a grupo de AD
apiVersion: management.cattle.io/v3
kind: ClusterRoleTemplateBinding
metadata:
  name: ops-team-viewer
  namespace: c-m-xxxxx
spec:
  clusterName: c-m-xxxxx
  roleTemplateName: readonly-cluster-viewer
  groupPrincipalName: "activedirectory_group://CN=OpsTeam,OU=Groups,DC=example,DC=com"
----

**Roles de Proyecto (Project Roles)**

Los proyectos son grupos lógicos de namespaces. Roles de proyecto controlan acceso a estos grupos.

.Roles de proyecto predefinidos:
[cols="1,2,2", options="header"]
|===
|Rol |Permisos |Uso recomendado

|Project Owner
|Acceso completo al proyecto y sus namespaces
|Product owners, team leads

|Project Member
|Gestionar workloads, no cambiar configuración proyecto
|Developers, SREs

|Read Only
|Solo lectura de recursos del proyecto
|QA, support teams

|Create Namespaces
|Crear namespaces dentro del proyecto
|Platform engineers
|===

.Ver roles de proyecto:
[source,bash]
----
# Listar project role templates
kubectl get projectroletemplates

# Ver asignaciones en un proyecto
kubectl get projectroletemplatebindings -n <project-id>

# Ejemplo
kubectl get projectroletemplatebindings -n p-xxxxx -o json | \
  jq '.items[] | {role: .roleTemplateName, user: .userName}'
----

.Crear rol de proyecto personalizado:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: RoleTemplate
metadata:
  name: deployer
spec:
  builtin: false
  context: project
  displayName: "Application Deployer"
  description: "Can deploy and manage applications but not modify project settings"
  rules:
  # Gestionar deployments
  - apiGroups:
    - "apps"
    resources:
    - "deployments"
    - "replicasets"
    - "statefulsets"
    - "daemonsets"
    verbs:
    - "*"
  
  # Gestionar pods
  - apiGroups:
    - ""
    resources:
    - "pods"
    - "pods/log"
    - "pods/exec"
    verbs:
    - "*"
  
  # Gestionar servicios
  - apiGroups:
    - ""
    resources:
    - "services"
    verbs:
    - "*"
  
  # Gestionar configmaps y secrets
  - apiGroups:
    - ""
    resources:
    - "configmaps"
    - "secrets"
    verbs:
    - "*"
  
  # Ver ingress (no modificar)
  - apiGroups:
    - "networking.k8s.io"
    resources:
    - "ingresses"
    verbs:
    - "get"
    - "list"
    - "watch"
  
  # Gestionar HPA
  - apiGroups:
    - "autoscaling"
    resources:
    - "horizontalpodautoscalers"
    verbs:
    - "*"
----

==== Políticas de RBAC avanzadas

**Herencia de roles**

Rancher soporta herencia de roles para simplificar gestión.

.Ejemplo de rol con herencia:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: RoleTemplate
metadata:
  name: senior-developer
spec:
  builtin: false
  context: project
  displayName: "Senior Developer"
  
  # Heredar permisos de developer básico
  roleTemplateNames:
  - "deployer"  # Rol definido anteriormente
  
  # Añadir permisos adicionales
  rules:
  # Puede gestionar ingress (además de los permisos heredados)
  - apiGroups:
    - "networking.k8s.io"
    resources:
    - "ingresses"
    verbs:
    - "*"
  
  # Puede gestionar PVCs
  - apiGroups:
    - ""
    resources:
    - "persistentvolumeclaims"
    verbs:
    - "*"
  
  # Puede ver logs de auditoría
  - apiGroups:
    - "management.cattle.io"
    resources:
    - "auditlogs"
    verbs:
    - "get"
    - "list"
----

**RBAC condicional con atributos**

Usar labels y annotations para control granular.

.Ejemplo: Restringir acceso solo a ciertos namespaces:
[source,yaml]
----
# Rol que solo puede acceder a namespaces con label team=frontend
apiVersion: management.cattle.io/v3
kind: RoleTemplate
metadata:
  name: frontend-team-member
spec:
  context: cluster
  displayName: "Frontend Team Member"
  rules:
  - apiGroups:
    - ""
    resources:
    - "namespaces"
    verbs:
    - "get"
    - "list"
    resourceNames: []  # Vacío para permitir listar
  
  # Acceso completo en namespaces con label correcto
  # (implementado via Project membership en Rancher)
  - apiGroups:
    - "*"
    resources:
    - "*"
    verbs:
    - "*"
    # Limitado por Project que agrupa solo namespaces con team=frontend

# Crear proyecto que agrupa namespaces frontend
---
apiVersion: management.cattle.io/v3
kind: Project
metadata:
  name: frontend-project
  namespace: c-m-xxxxx
spec:
  clusterName: c-m-xxxxx
  displayName: "Frontend Project"
  namespaceDefaultResourceQuota:
    limit:
      limitsCpu: "4000m"
      limitsMemory: "8Gi"
  
  # Namespace selector (solo namespaces con este label)
  namespaceLabels:
    team: frontend
----

**Service Accounts con RBAC**

Para automatización y CI/CD.

.Crear service account con permisos específicos:
[source,yaml]
----
# 1. Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ci-deployer
  namespace: production

---
# 2. Role con permisos necesarios
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: deployment-manager
  namespace: production
rules:
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "replicasets"
  verbs:
  - "get"
  - "list"
  - "create"
  - "update"
  - "patch"

- apiGroups:
  - ""
  resources:
  - "pods"
  - "pods/log"
  verbs:
  - "get"
  - "list"

- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "secrets"
  verbs:
  - "get"
  - "list"

---
# 3. RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ci-deployer-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: ci-deployer
  namespace: production
roleRef:
  kind: Role
  name: deployment-manager
  apiGroup: rbac.authorization.k8s.io

---
# 4. Obtener token del service account
# kubectl create token ci-deployer -n production --duration=87600h
# O extraer del secret (K8s < 1.24)
# kubectl get secret -n production ci-deployer-token-xxxxx -o jsonpath='{.data.token}' | base64 -d
----

**RBAC para APIs específicas**

Controlar acceso a APIs custom y CRDs.

[source,yaml]
----
# Rol para gestionar solo recursos de Rancher Apps
apiVersion: management.cattle.io/v3
kind: RoleTemplate
metadata:
  name: app-catalog-manager
spec:
  context: cluster
  displayName: "App Catalog Manager"
  rules:
  # Gestionar apps del catálogo
  - apiGroups:
    - "catalog.cattle.io"
    resources:
    - "apps"
    - "clusterrepos"
    verbs:
    - "*"
  
  # Ver releases de Helm
  - apiGroups:
    - "helm.cattle.io"
    resources:
    - "releases"
    verbs:
    - "get"
    - "list"
    - "watch"
  
  # No puede modificar releases directamente
  - apiGroups:
    - "helm.cattle.io"
    resources:
    - "releases"
    verbs:
    - "create"
    - "update"
    - "delete"
    resourceNames: []  # Vacío = ninguno
----

**Auditoría de permisos**

Verificar qué puede hacer un usuario.

.Comandos de auditoría:
[source,bash]
----
# Verificar qué puede hacer un usuario (kubectl auth can-i)
kubectl auth can-i --list --as=user@example.com

# Verificar permiso específico
kubectl auth can-i create deployments -n production --as=user@example.com

# Ver todos los permisos de un usuario en Rancher
# Via API
curl -k "${RANCHER_URL}/v3/users/u-xxxxx/globalrolebindings" \
  -H "Authorization: Bearer ${API_TOKEN}"

curl -k "${RANCHER_URL}/v3/clusterroletemplatebindings?userId=u-xxxxx" \
  -H "Authorization: Bearer ${API_TOKEN}"

# Herramienta para auditar RBAC
# https://github.com/FairwindsOps/rbac-lookup
kubectl rbac-lookup user@example.com

# O instalar rbac-manager
kubectl krew install rbac-lookup
kubectl rbac-lookup --kind user --name user@example.com
----

==== Gestión de proyectos y namespaces

**Proyectos en Rancher**

Los proyectos son una abstracción de Rancher que agrupa múltiples namespaces para facilitar gestión de permisos y cuotas.

.Arquitectura de proyectos:
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

RECTANGLE "Cluster" as cluster {
  RECTANGLE "Project: Frontend" as pf {
    RECTANGLE "Namespace: frontend-prod" as nfp
    RECTANGLE "Namespace: frontend-staging" as nfs
    RECTANGLE "Namespace: frontend-dev" as nfd
  }
  
  RECTANGLE "Project: Backend" as pb {
    RECTANGLE "Namespace: backend-api" as nba
    RECTANGLE "Namespace: backend-worker" as nbw
  }
  
  RECTANGLE "Project: Database" as pdb {
    RECTANGLE "Namespace: postgres" as npg
    RECTANGLE "Namespace: redis" as nrd
  }
  
  RECTANGLE "System Namespaces" as sys {
    RECTANGLE "kube-system"
    RECTANGLE "cattle-system"
  }
}

note right of pf
  Project = Grupo lógico
  - RBAC unificado
  - Resource quotas
  - Network policies
  - Pod Security Policies
end note

@enduml
----

**Crear proyecto**

.Via Rancher UI:
[source,text]
----
1. Cluster → Projects/Namespaces → Create Project
2. Configurar:
   Name: frontend-project
   Description: Frontend applications
   
3. Resource Quotas:
   - CPU Limit: 10 cores
   - Memory Limit: 20Gi
   - Pods: 100
   - Services: 50
   - ConfigMaps: 50
   - Secrets: 50
   - PVCs: 20

4. Container Default Resource Limits:
   - CPU Request: 100m
   - CPU Limit: 500m
   - Memory Request: 128Mi
   - Memory Limit: 512Mi

5. Pod Security Policy:
   - Restricted (recomendado para apps)
   - O Baseline

6. Network Isolation:
   - Enable Network Policy
   - Default deny all ingress

7. Members:
   - Añadir usuarios/grupos con roles (Owner/Member/Read-Only)

8. Create
----

.Via YAML:
[source,yaml]
----
apiVersion: management.cattle.io/v3
kind: Project
metadata:
  name: frontend-project
  namespace: c-m-xxxxx  # Cluster ID
spec:
  clusterName: c-m-xxxxx
  displayName: "Frontend Project"
  description: "Frontend applications and services"
  
  # Resource quotas a nivel de proyecto
  resourceQuota:
    limit:
      limitsCpu: "10000m"
      limitsMemory: "20Gi"
      requestsCpu: "5000m"
      requestsMemory: "10Gi"
      pods: "100"
      services: "50"
      configmaps: "50"
      secrets: "50"
      persistentvolumeclaims: "20"
  
  # Default resource limits para containers
  namespaceDefaultResourceQuota:
    limit:
      limitsCpu: "500m"
      limitsMemory: "512Mi"
      requestsCpu: "100m"
      requestsMemory: "128Mi"
  
  # Container default limits
  containerDefaultResourceLimit:
    limitsCpu: "500m"
    limitsMemory: "512Mi"
    requestsCpu: "100m"
    requestsMemory: "128Mi"
  
  # Pod Security Policy
  podSecurityPolicyTemplateId: "restricted"
  
  # Enable network policy
  enableProjectNetworkIsolation: true
----

**Gestionar namespaces en proyectos**

.Crear namespace en un proyecto:
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: frontend-prod
  annotations:
    # Asociar a proyecto
    field.cattle.io/projectId: c-m-xxxxx:p-yyyyy
  labels:
    # Labels automáticos de Rancher
    field.cattle.io/projectId: p-yyyyy
    # Labels custom
    environment: production
    team: frontend
spec:
  finalizers:
  - kubernetes
----

.Mover namespace entre proyectos:
[source,bash]
----
# Via Rancher UI:
# Cluster → Projects/Namespaces → Select namespace → Move

# Via kubectl:
kubectl annotate namespace frontend-staging \
  field.cattle.io/projectId=c-m-xxxxx:p-zzzzz --overwrite

kubectl label namespace frontend-staging \
  field.cattle.io/projectId=p-zzzzz --overwrite
----

**Resource Quotas por namespace**

Rancher hereda quotas del proyecto pero se pueden personalizar.

[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: frontend-prod
spec:
  hard:
    # Compute
    requests.cpu: "4"
    requests.memory: "8Gi"
    limits.cpu: "8"
    limits.memory: "16Gi"
    
    # Storage
    requests.storage: "100Gi"
    persistentvolumeclaims: "10"
    
    # Objects
    pods: "50"
    services: "20"
    services.loadbalancers: "2"
    services.nodeports: "5"
    configmaps: "30"
    secrets: "30"
    replicationcontrollers: "20"
    
    # Extended resources
    requests.nvidia.com/gpu: "2"
----

**Limit Ranges por namespace**

Definir límites default para containers.

[source,yaml]
----
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: frontend-prod
spec:
  limits:
  # Limits para containers
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
  
  # Limits para pods
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"
  
  # Limits para PVCs
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"
----

**Network Policies por proyecto**

Rancher puede crear network policies automáticas para aislar proyectos.

.Network Policy automática de proyecto:
[source,yaml]
----
# Generada automáticamente por Rancher cuando
# enableProjectNetworkIsolation: true

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np-default
  namespace: frontend-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # Permitir desde mismo namespace
  - from:
    - podSelector: {}
  
  # Permitir desde otros namespaces del mismo proyecto
  - from:
    - namespaceSelector:
        matchLabels:
          field.cattle.io/projectId: p-yyyyy
  
  egress:
  # Permitir a mismo namespace
  - to:
    - podSelector: {}
  
  # Permitir a otros namespaces del mismo proyecto
  - to:
    - namespaceSelector:
        matchLabels:
          field.cattle.io/projectId: p-yyyyy
  
  # Permitir DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53
  
  # Permitir tráfico externo (internet)
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80
----

**Best practices para gestión de usuarios**

.Recomendaciones:
[source,text]
----
1. Autenticación:
   - Usar IdP externo (LDAP/SAML/OIDC) en producción
   - Nunca usar usuarios locales en producción (excepto admin de emergencia)
   - Habilitar MFA cuando sea posible
   - Rotar contraseñas regularmente

2. Autorización:
   - Principio de menor privilegio (least privilege)
   - Usar grupos de AD/IdP en lugar de usuarios individuales
   - Documentar qué grupos tienen qué accesos
   - Revisar permisos trimestralmente

3. Roles:
   - Crear roles custom para necesidades específicas
   - Usar herencia para reducir duplicación
   - Nombrar roles descriptivamente
   - Documentar propósito de cada rol

4. Proyectos:
   - Un proyecto por aplicación/equipo
   - Configurar resource quotas apropiadas
   - Habilitar network isolation
   - Usar naming convention: <team>-<app>-<env>

5. Auditoría:
   - Habilitar audit logging (ver módulo 4.3)
   - Revisar logs de acceso regularmente
   - Monitorear cambios en permisos
   - Alertar sobre acciones privilegiadas

6. Service Accounts:
   - Usar SA para CI/CD, no usuarios
   - Limitar permisos de SA al mínimo necesario
   - Rotar tokens de SA
   - No usar SA por defecto
----

**Script de auditoría de usuarios**

.Herramienta para auditar accesos:
[source,bash]
----
#!/bin/bash
# Auditoría de usuarios y permisos en Rancher

echo "=== Rancher User Audit ==="
echo "Timestamp: $(date)"
echo ""

# 1. Usuarios con rol global de admin
echo "1. Global Administrators:"
kubectl get globalrolebindings -o json | \
  jq -r '.items[] | select(.globalRoleName=="admin") | 
  "  - User: \(.userName // "N/A"), Group: \(.groupPrincipalName // "N/A")"'
echo ""

# 2. Usuarios por clúster
echo "2. Cluster Access Summary:"
for cluster in $(kubectl get clusters.management.cattle.io -o json | jq -r '.items[].metadata.name'); do
  echo "  Cluster: $cluster"
  kubectl get clusterroletemplatebindings -n $cluster -o json | \
    jq -r '.items[] | "    - \(.roleTemplateName): \(.userName // .groupPrincipalName)"' | \
    sort | uniq
  echo ""
done

# 3. Proyectos y sus miembros
echo "3. Project Membership:"
for project in $(kubectl get projects.management.cattle.io -A -o json | jq -r '.items[].metadata.name'); do
  ns=$(kubectl get projects.management.cattle.io -A -o json | \
    jq -r ".items[] | select(.metadata.name==\"$project\") | .metadata.namespace")
  echo "  Project: $project (Cluster: $ns)"
  kubectl get projectroletemplatebindings -n $project -o json 2>/dev/null | \
    jq -r '.items[]? | "    - \(.roleTemplateName): \(.userName // .groupPrincipalName)"' | \
    sort | uniq || echo "    No members"
  echo ""
done

# 4. Service Accounts con ClusterRoleBindings
echo "4. Service Accounts with Cluster Roles:"
kubectl get clusterrolebindings -o json | \
  jq -r '.items[] | select(.subjects[]?.kind=="ServiceAccount") | 
  "  - SA: \(.subjects[0].name) in namespace \(.subjects[0].namespace) -> Role: \(.roleRef.name)"'
echo ""

# 5. Usuarios sin actividad reciente (requiere audit logs)
echo "5. Inactive Users (last 30 days):"
echo "  (Requires audit log analysis - not implemented in basic script)"

echo ""
echo "=== Audit Complete ==="
----

=== 4.2 Políticas de seguridad

La seguridad en Kubernetes requiere múltiples capas de protección. Rancher facilita la implementación de políticas de seguridad a través de Pod Security Standards, Network Policies, gestión de secrets y admission controllers como OPA Gatekeeper.

==== Pod Security Standards

Los Pod Security Standards (PSS) reemplazan las Pod Security Policies (PSP) deprecadas desde Kubernetes 1.25.

**Niveles de Pod Security**

.Pod Security Standards levels:
[cols="1,2,2", options="header"]
|===
|Nivel |Restricciones |Uso recomendado

|Privileged
|Sin restricciones, permite cualquier configuración
|Pods de sistema, CNI, storage drivers

|Baseline
|Previene escalaciones de privilegios conocidas
|Aplicaciones que necesitan algunas capacidades elevadas

|Restricted
|Fuertemente restringido, hardening completo
|Aplicaciones stateless, microservicios (recomendado)
|===

**Comparación detallada de niveles**

.Restricciones por nivel:
[cols="2,1,1,1", options="header"]
|===
|Característica |Privileged |Baseline |Restricted

|Host namespaces (hostNetwork, hostPID, hostIPC)
|✓ Permitido
|✗ No permitido
|✗ No permitido

|Privileged containers
|✓ Permitido
|✗ No permitido
|✗ No permitido

|Capabilities (ALL)
|✓ Permitido
|✗ Solo lista limitada
|✗ Solo NET_BIND_SERVICE

|HostPath volumes
|✓ Permitido
|✗ No permitido
|✗ No permitido

|Host ports
|✓ Permitido
|✗ No permitido
|✗ No permitido

|AppArmor
|✓ Cualquiera
|✓ Cualquiera
|✗ Solo runtime/default, localhost/*

|SELinux
|✓ Cualquiera
|✓ Cualquiera
|✗ Solo predefinidos

|/proc mount
|✓ Cualquiera
|✓ Cualquiera
|✗ Solo default

|Seccomp
|✓ Sin restricción
|✓ Sin restricción
|✗ Obligatorio (RuntimeDefault, Localhost)

|Run as user
|✓ Cualquier UID
|✓ Cualquier UID
|✗ No puede ser root (UID != 0)

|Volume types
|✓ Todos
|✗ Solo seguros
|✗ Solo seguros + restricciones adicionales
|===

**Configurar PSS en Rancher**

.Habilitar PSS a nivel de namespace:
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    # Enforce restricted en este namespace
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: latest
    
    # Warn si viola baseline
    pod-security.kubernetes.io/warn: baseline
    pod-security.kubernetes.io/warn-version: latest
    
    # Audit si viola privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/audit-version: latest
----

.Configurar PSS a nivel de clúster (Rancher):
[source,yaml]
----
# Via Rancher Cluster Config
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: production-cluster
spec:
  rkeConfig:
    machineSelectorConfig:
      - config:
          # Habilitar admission controller
          kube-apiserver-arg:
            - "admission-control-config-file=/etc/rancher/rke2/admission-config.yaml"
          
          # Archivo de configuración
          kube-apiserver-extra-mount:
            - "/etc/rancher/rke2/admission-config.yaml:/etc/rancher/rke2/admission-config.yaml:ro"

# Contenido de admission-config.yaml
---
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: PodSecurity
  configuration:
    apiVersion: pod-security.admission.config.k8s.io/v1
    kind: PodSecurityConfiguration
    defaults:
      enforce: "restricted"
      enforce-version: "latest"
      audit: "restricted"
      audit-version: "latest"
      warn: "restricted"
      warn-version: "latest"
    exemptions:
      usernames: []
      runtimeClasses: []
      namespaces:
      - kube-system
      - kube-public
      - kube-node-lease
      - cattle-system
      - cattle-fleet-system
      - cattle-monitoring-system
----

**Ejemplos de pods según nivel de seguridad**

.Pod Restricted (más seguro):
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: production
spec:
  securityContext:
    # Prohibir escalación de privilegios
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault
  
  containers:
  - name: app
    image: myapp:1.0
    
    securityContext:
      # No privilegios
      privileged: false
      # No permitir escalación
      allowPrivilegeEscalation: false
      # Eliminar todas las capabilities
      capabilities:
        drop:
        - ALL
      # Filesystem read-only
      readOnlyRootFilesystem: true
      # Run as non-root
      runAsNonRoot: true
      runAsUser: 1000
    
    # Usar volúmenes seguros
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /app/cache
  
  volumes:
  - name: tmp
    emptyDir: {}
  - name: cache
    emptyDir: {}
----

.Pod Baseline (menos restricciones):
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: baseline-app
  namespace: staging
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 1000
  
  containers:
  - name: app
    image: myapp:1.0
    
    securityContext:
      privileged: false
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE  # Permitido en baseline
        - CHOWN            # Permitido en baseline
    
    volumeMounts:
    - name: data
      mountPath: /data
  
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: app-data
----

.Pod Privileged (sistema):
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: system-pod
  namespace: kube-system
spec:
  hostNetwork: true
  hostPID: true
  
  containers:
  - name: system-agent
    image: system-agent:1.0
    
    securityContext:
      privileged: true
      capabilities:
        add:
        - ALL
    
    volumeMounts:
    - name: host-root
      mountPath: /host
  
  volumes:
  - name: host-root
    hostPath:
      path: /
      type: Directory
----

==== Network Policies

Las Network Policies controlan el tráfico de red entre pods y servicios.

**Conceptos básicos**

.Tipos de Network Policies:
[cols="1,2,2", options="header"]
|===
|Tipo |Función |Ejemplo

|Ingress
|Controla tráfico entrante al pod
|Solo permitir desde frontend a backend

|Egress
|Controla tráfico saliente del pod
|Permitir a database, bloquear internet

|Combined
|Controla ambos direcciones
|Microsegmentación completa
|===

**Policy 1: Deny All (default deny)**

Base recomendada para zero-trust networking.

[source,yaml]
----
# Denegar TODO el tráfico ingress en el namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: production
spec:
  podSelector: {}  # Aplica a todos los pods
  policyTypes:
  - Ingress
  # No hay regla ingress = denegar todo

---
# Denegar TODO el tráfico egress en el namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  # No hay regla egress = denegar todo
----

**Policy 2: Allow desde frontend a backend**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-allow-frontend
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
      tier: api
  
  policyTypes:
  - Ingress
  
  ingress:
  # Permitir desde frontend
  - from:
    - podSelector:
        matchLabels:
          app: frontend
          tier: web
    ports:
    - protocol: TCP
      port: 8080
  
  # Permitir health checks desde ingress controller
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
      path: /health
----

**Policy 3: Allow a database**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-to-database
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  
  policyTypes:
  - Egress
  
  egress:
  # Permitir a PostgreSQL
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  
  # Permitir a Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  
  # Permitir DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
----

**Policy 4: Allow internet egress selectivo**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external-apis
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
      internet-access: "true"
  
  policyTypes:
  - Egress
  
  egress:
  # Permitir HTTPS a internet (excepto rangos privados)
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
    ports:
    - protocol: TCP
      port: 443
  
  # Permitir HTTP (solo si es necesario)
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
    ports:
    - protocol: TCP
      port: 80
  
  # Permitir DNS
  - to:
    - namespaceSelector: {}
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
----

**Policy 5: Microsegmentación por namespace**

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: namespace-isolation
  namespace: production
spec:
  podSelector: {}
  
  policyTypes:
  - Ingress
  - Egress
  
  ingress:
  # Solo permitir desde mismo namespace
  - from:
    - podSelector: {}
  
  # Permitir desde ingress namespace
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
  
  egress:
  # Permitir a mismo namespace
  - to:
    - podSelector: {}
  
  # Permitir a shared services namespace
  - to:
    - namespaceSelector:
        matchLabels:
          name: shared-services
  
  # Permitir DNS y metrics
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 443  # metrics server
----

**Verificar Network Policies**

.Comandos de verificación:
[source,bash]
----
# Listar network policies
kubectl get networkpolicies -A

# Ver detalles de una policy
kubectl describe networkpolicy backend-allow-frontend -n production

# Test de conectividad
# Crear pod de test
kubectl run test-source --image=nicolaka/netshoot -n production --rm -it -- bash

# Dentro del pod:
# Test conectividad a backend (debería fallar si no hay policy)
curl http://backend-service:8080

# Test conectividad a internet
curl -I https://google.com

# Test DNS
nslookup backend-service

# Tool para visualizar network policies
# https://github.com/containership/networkpolicytool
kubectl create -f https://raw.githubusercontent.com/containership/networkpolicytool/master/deploy.yaml
----

**Rancher Network Policy en UI**

.Crear policy via Rancher UI:
[source,text]
----
1. Cluster → Projects → Select Project → Resources → Network Policies
2. Add Policy
3. Configurar:
   Name: backend-ingress-policy
   Target Pods: app=backend
   
   Ingress Rules:
   - Add Rule
     - Source Type: Pod Selector
     - Label: app=frontend
     - Port: 8080
     - Protocol: TCP
   
   Egress Rules:
   - Add Rule
     - Destination Type: Pod Selector
     - Label: app=postgres
     - Port: 5432
     - Protocol: TCP

4. Create
----

==== Gestión de secrets y certificados

**Tipos de Secrets en Kubernetes**

.Tipos de secrets:
[cols="1,2,2", options="header"]
|===
|Tipo |Uso |Ejemplo

|Opaque
|Datos genéricos (default)
|API keys, passwords

|kubernetes.io/tls
|Certificados TLS
|Ingress certificates

|kubernetes.io/dockerconfigjson
|Credenciales de registry
|Pull images privadas

|kubernetes.io/service-account-token
|Tokens de service account
|Autenticación API

|kubernetes.io/basic-auth
|Autenticación básica
|HTTP basic auth

|kubernetes.io/ssh-auth
|Claves SSH
|Git over SSH
|===

**Crear secrets**

.Secret genérico (Opaque):
[source,bash]
----
# Desde literales
kubectl create secret generic app-secrets \
  --from-literal=db-password='SuperSecure123!' \
  --from-literal=api-key='sk-1234567890abcdef' \
  -n production

# Desde archivos
kubectl create secret generic app-config \
  --from-file=config.json=./config.json \
  --from-file=credentials=./creds.txt \
  -n production

# Desde YAML
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: production
type: Opaque
stringData:  # stringData no necesita base64
  db-password: "SuperSecure123!"
  api-key: "sk-1234567890abcdef"
data:  # data debe estar en base64
  db-host: cG9zdGdyZXMuc3ZjLmNsdXN0ZXIubG9jYWw=  # postgres.svc.cluster.local
EOF
----

.Secret TLS:
[source,bash]
----
# Desde certificados existentes
kubectl create secret tls myapp-tls \
  --cert=path/to/tls.crt \
  --key=path/to/tls.key \
  -n production

# YAML
apiVersion: v1
kind: Secret
metadata:
  name: myapp-tls
  namespace: production
type: kubernetes.io/tls
data:
  tls.crt: |
    LS0tLS1CRUdJTi...  # Base64 del certificado
  tls.key: |
    LS0tLS1CRUdJTi...  # Base64 de la key privada
----

.Secret de registry:
[source,bash]
----
# Para pull de imágenes privadas
kubectl create secret docker-registry regcred \
  --docker-server=registry.example.com \
  --docker-username=myuser \
  --docker-password=mypassword \
  --docker-email=user@example.com \
  -n production

# Usar en pod
apiVersion: v1
kind: Pod
metadata:
  name: private-app
spec:
  imagePullSecrets:
  - name: regcred
  containers:
  - name: app
    image: registry.example.com/myapp:1.0
----

**Usar secrets en pods**

.Como variables de entorno:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-secrets
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    env:
    # Single secret value
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: app-secrets
          key: db-password
    
    # All secrets as env vars
    envFrom:
    - secretRef:
        name: app-secrets
        prefix: APP_  # Keys prefijadas con APP_
----

.Como volumen montado:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-secret-volume
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    volumeMounts:
    - name: secret-volume
      mountPath: /etc/secrets
      readOnly: true
    
    # Secrets disponibles como archivos:
    # /etc/secrets/db-password
    # /etc/secrets/api-key
  
  volumes:
  - name: secret-volume
    secret:
      secretName: app-secrets
      defaultMode: 0400  # Read-only por owner
      items:  # Opcional: seleccionar keys específicas
      - key: db-password
        path: database/password
        mode: 0400
----

**Cifrado de secrets en etcd**

Por defecto, secrets se almacenan en base64 (no cifrados) en etcd.

.Habilitar encryption at rest:
[source,yaml]
----
# Crear encryption config
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    # AES-CBC encryption
    - aescbc:
        keys:
        - name: key1
          secret: <BASE64_ENCODED_32_BYTE_KEY>
    # Fallback a identity (sin cifrado) para leer secrets viejos
    - identity: {}

# Generar key aleatoria
head -c 32 /dev/urandom | base64

# Configurar en API server (RKE2)
# /etc/rancher/rke2/config.yaml
kube-apiserver-arg:
  - "encryption-provider-config=/var/lib/rancher/rke2/server/encryption-config.yaml"
----

**Rotar secrets**

.Proceso de rotación de secrets:
[source,bash]
----
#!/bin/bash
# Script de rotación de secret

SECRET_NAME="app-secrets"
NAMESPACE="production"

echo "Rotando secret: $SECRET_NAME en namespace: $NAMESPACE"

# 1. Generar nuevo password
NEW_PASSWORD=$(openssl rand -base64 32)

# 2. Crear nuevo secret
kubectl create secret generic ${SECRET_NAME}-new \
  --from-literal=db-password="$NEW_PASSWORD" \
  --dry-run=client -o yaml | kubectl apply -f -

# 3. Actualizar aplicación para usar nuevo secret
kubectl set env deployment/backend \
  --from=secret/${SECRET_NAME}-new \
  -n $NAMESPACE

# 4. Esperar rollout
kubectl rollout status deployment/backend -n $NAMESPACE

# 5. Verificar aplicación funciona
kubectl get pods -n $NAMESPACE -l app=backend

# 6. Eliminar secret viejo
kubectl delete secret $SECRET_NAME -n $NAMESPACE

# 7. Renombrar nuevo secret
kubectl get secret ${SECRET_NAME}-new -n $NAMESPACE -o yaml | \
  sed "s/${SECRET_NAME}-new/${SECRET_NAME}/g" | \
  kubectl apply -f -

kubectl delete secret ${SECRET_NAME}-new -n $NAMESPACE

echo "Rotación completada"
----

**External Secrets Operator**

Para integración con vaults externos (AWS Secrets Manager, HashiCorp Vault, Azure Key Vault).

.Instalar External Secrets Operator:
[source,bash]
----
helm repo add external-secrets https://charts.external-secrets.io
helm repo update

helm install external-secrets \
  external-secrets/external-secrets \
  -n external-secrets-system \
  --create-namespace
----

.Configurar con AWS Secrets Manager:
[source,yaml]
----
# Secret Store
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: aws-secrets-manager
  namespace: production
spec:
  provider:
    aws:
      service: SecretsManager
      region: us-east-1
      auth:
        secretRef:
          accessKeyIDSecretRef:
            name: aws-credentials
            key: access-key-id
          secretAccessKeySecretRef:
            name: aws-credentials
            key: secret-access-key

---
# External Secret
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: app-secrets
  namespace: production
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  
  target:
    name: app-secrets  # Nombre del secret K8s a crear
    creationPolicy: Owner
  
  data:
  - secretKey: db-password
    remoteRef:
      key: production/database
      property: password
  
  - secretKey: api-key
    remoteRef:
      key: production/api
      property: key
----

**Cert-manager para certificados**

Gestión automática de certificados TLS.

.Ejemplo de Certificate:
[source,yaml]
----
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: myapp-tls
  namespace: production
spec:
  secretName: myapp-tls-secret
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
  - myapp.example.com
  - api.myapp.example.com
  
  # Renovación automática
  renewBefore: 720h  # 30 días antes

---
# ClusterIssuer para Let's Encrypt
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod-key
    solvers:
    - http01:
        ingress:
          class: nginx
----

==== Configuración de OPA Gatekeeper

OPA (Open Policy Agent) Gatekeeper permite definir políticas de seguridad y compliance como código.

**Instalar Gatekeeper**

.Instalación via Helm:
[source,bash]
----
helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts
helm repo update

helm install gatekeeper gatekeeper/gatekeeper \
  --namespace gatekeeper-system \
  --create-namespace \
  --set replicas=3 \
  --set auditInterval=60

# Verificar instalación
kubectl get pods -n gatekeeper-system
kubectl get crd | grep gatekeeper
----

**Conceptos de Gatekeeper**

.Componentes principales:
[cols="1,2,2", options="header"]
|===
|Componente |Función |Ejemplo

|ConstraintTemplate
|Define la lógica de la política (Rego)
|Template para validar labels

|Constraint
|Instancia de una template con parámetros
|Requerir label "owner" en todos los namespaces

|Config
|Configuración de sincronización de datos
|Sincronizar lista de namespaces

|Assign/Mutation
|Modificar recursos automáticamente
|Añadir labels por defecto
|===

**Policy 1: Requerir labels obligatorios**

.ConstraintTemplate:
[source,yaml]
----
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequiredlabels
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredLabels
      validation:
        openAPIV3Schema:
          type: object
          properties:
            labels:
              type: array
              items:
                type: string
  
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredlabels

        violation[{"msg": msg, "details": {"missing_labels": missing}}] {
          provided := {label | input.review.object.metadata.labels[label]}
          required := {label | label := input.parameters.labels[_]}
          missing := required - provided
          count(missing) > 0
          msg := sprintf("Missing required labels: %v", [missing])
        }
----

.Constraint (usar la template):
[source,yaml]
----
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredLabels
metadata:
  name: namespace-must-have-owner
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Namespace"]
  
  parameters:
    labels:
    - "owner"
    - "environment"
    - "cost-center"
----

**Policy 2: Prohibir contenedores privilegiados**

.ConstraintTemplate:
[source,yaml]
----
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8spsprivilegedcontainer
spec:
  crd:
    spec:
      names:
        kind: K8sPSPPrivilegedContainer
  
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8spsprivilegedcontainer

        violation[{"msg": msg, "details": {}}] {
          c := input_containers[_]
          c.securityContext.privileged
          msg := sprintf("Privileged container is not allowed: %v", [c.name])
        }

        input_containers[c] {
          c := input.review.object.spec.containers[_]
        }
        
        input_containers[c] {
          c := input.review.object.spec.initContainers[_]
        }
----

.Constraint:
[source,yaml]
----
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sPSPPrivilegedContainer
metadata:
  name: no-privileged-containers
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Pod"]
    excludedNamespaces:
    - kube-system
    - cattle-system
----

**Policy 3: Limitar registries permitidos**

.ConstraintTemplate:
[source,yaml]
----
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
      validation:
        openAPIV3Schema:
          type: object
          properties:
            repos:
              type: array
              items:
                type: string
  
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sallowedrepos

        violation[{"msg": msg}] {
          container := input_containers[_]
          satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("Container image %v does not come from approved registry", [container.image])
        }

        input_containers[c] {
          c := input.review.object.spec.containers[_]
        }
        
        input_containers[c] {
          c := input.review.object.spec.initContainers[_]
        }
----

.Constraint:
[source,yaml]
----
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sAllowedRepos
metadata:
  name: allowed-docker-registries
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Pod"]
    namespaces:
    - production
  
  parameters:
    repos:
    - "registry.example.com/"
    - "docker.io/library/"
    - "gcr.io/myproject/"
----

**Policy 4: Requerir resource limits**

.ConstraintTemplate y Constraint:
[source,yaml]
----
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8srequireresourcelimits
spec:
  crd:
    spec:
      names:
        kind: K8sRequireResourceLimits
  
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequireresourcelimits

        violation[{"msg": msg}] {
          container := input_containers[_]
          not container.resources.limits.cpu
          msg := sprintf("Container %v does not have CPU limit set", [container.name])
        }

        violation[{"msg": msg}] {
          container := input_containers[_]
          not container.resources.limits.memory
          msg := sprintf("Container %v does not have memory limit set", [container.name])
        }

        input_containers[c] {
          c := input.review.object.spec.containers[_]
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequireResourceLimits
metadata:
  name: containers-must-have-limits
spec:
  match:
    kinds:
    - apiGroups: [""]
      kinds: ["Pod"]
    namespaces:
    - production
    - staging
----

**Mutation: Añadir labels automáticamente**

[source,yaml]
----
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: Assign
metadata:
  name: add-default-labels
spec:
  applyTo:
  - groups: [""]
    kinds: ["Pod"]
    versions: ["v1"]
  
  match:
    scope: Namespaced
    namespaces:
    - production
  
  location: "metadata.labels.managed-by"
  parameters:
    assign:
      value: "gatekeeper"

---
# Añadir annotations
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: Assign
metadata:
  name: add-security-annotation
spec:
  applyTo:
  - groups: [""]
    kinds: ["Pod"]
    versions: ["v1"]
  
  location: "metadata.annotations.security-scan"
  parameters:
    assign:
      value: "required"
----

**Verificar políticas de Gatekeeper**

.Comandos de verificación:
[source,bash]
----
# Listar constraint templates
kubectl get constrainttemplates

# Listar constraints
kubectl get constraints

# Ver violaciones de una constraint
kubectl get k8srequiredlabels namespace-must-have-owner -o yaml

# Ver audit de violaciones
kubectl get constraints -o json | \
  jq '.items[] | {name: .metadata.name, violations: .status.totalViolations}'

# Test de política (dry-run)
cat <<EOF | kubectl apply --dry-run=server -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-privileged
spec:
  containers:
  - name: nginx
    image: nginx
    securityContext:
      privileged: true
EOF
# Debería ser bloqueado si hay policy

# Ver logs de Gatekeeper
kubectl logs -n gatekeeper-system -l control-plane=controller-manager

# Métricas de Gatekeeper
kubectl port-forward -n gatekeeper-system svc/gatekeeper-webhook-service 8888:443
curl -k https://localhost:8888/metrics
----

**Biblioteca de políticas predefinidas**

Gatekeeper tiene una biblioteca de políticas comunes:

[source,bash]
----
# Clonar repositorio de políticas
git clone https://github.com/open-policy-agent/gatekeeper-library.git

# Instalar políticas comunes
cd gatekeeper-library/library

# Ejemplo: Instalar política de replica limits
kubectl apply -f pod-security-policy/replica-limits/

# Ejemplo: Instalar política de allowed repos
kubectl apply -f general/allowedrepos/
----

**Best practices para políticas de seguridad**

.Recomendaciones:
[source,text]
----
1. Pod Security Standards:
   - Usar "restricted" por defecto
   - Solo "privileged" para system namespaces
   - Documentar excepciones
   - Auditar pods que no cumplen

2. Network Policies:
   - Comenzar con deny-all
   - Whitelist explícito (zero-trust)
   - Una policy por microservicio
   - Documentar flujos de tráfico
   - Testing regular

3. Secrets:
   - Nunca hardcodear en código
   - Usar external secrets manager (Vault, AWS SM)
   - Habilitar encryption at rest
   - Rotar regularmente (90 días)
   - Auditar accesos
   - Limitar scope (namespace-level)

4. Certificados:
   - Usar cert-manager para automatización
   - Renovación automática (30 días antes)
   - Wildcard certs solo si es necesario
   - Monitorear expiración
   - Backup de keys privadas

5. OPA Gatekeeper:
   - Empezar en modo audit (no enforce)
   - Políticas progresivas (warn → enforce)
   - Testing en staging primero
   - Documentar cada política
   - Revisar violaciones regularmente
   - Biblioteca de políticas estándar
----

== Módulo 5: Gestión de Aplicaciones

=== 5.1 Rancher Apps & Marketplace

Rancher Apps & Marketplace simplifica el despliegue de aplicaciones mediante un catálogo integrado de Helm charts. Proporciona una experiencia unificada para descubrir, instalar y gestionar aplicaciones en clústeres Kubernetes.

==== Catálogo de aplicaciones integrado

Rancher incluye varios catálogos predefinidos con aplicaciones populares y certificadas.

**Catálogos disponibles en Rancher**

.Catálogos predeterminados:
[cols="1,2,1,2", options="header"]
|===
|Catálogo |Descripción |Tipo |Apps destacadas

|Rancher Charts
|Aplicaciones curadas y probadas por Rancher
|Built-in
|Monitoring, Logging, Longhorn, Istio

|Partner Charts
|Apps de partners certificados de Rancher
|Built-in
|Portworx, Kasten, Sysdig

|Helm Stable (legacy)
|Charts estables de Helm 2
|Legacy
|Deprecado, migrar a nuevos repos

|Artifact Hub
|Catálogo público de Helm charts
|External
|Cualquier chart público
|===

**Arquitectura del marketplace**

.Flujo de instalación de apps:
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

actor Usuario
participant "Rancher UI" as UI
participant "Rancher Apps" as Apps
database "Chart Repository\n(Helm Repo)" as Repo
participant "Helm Controller" as Helm
participant "Kubernetes API" as K8s

Usuario -> UI: Seleccionar app del catálogo
UI -> Apps: Request install
Apps -> Repo: Download chart
Repo --> Apps: Chart files
Apps -> Helm: Install release
Helm -> K8s: Create resources
K8s --> Helm: Resources created
Helm --> Apps: Release installed
Apps --> UI: Installation complete
UI --> Usuario: App ready

note right of Helm
  Helm v3 controller
  Release namespace
  Custom values
  Version tracking
end note

@enduml
----

**Explorar el marketplace**

.Via Rancher UI:
[source,text]
----
1. Cluster → Apps → Charts
2. Ver categorías:
   - Featured: Apps destacadas
   - Database: PostgreSQL, MySQL, MongoDB, Redis
   - Monitoring: Prometheus, Grafana, Rancher Monitoring
   - Logging: Loki, Elasticsearch, Fluentd
   - Storage: Longhorn, Rook, MinIO
   - Networking: Traefik, NGINX, Istio
   - Security: Falco, Vault, Cert-Manager
   - CI/CD: ArgoCD, Jenkins, GitLab Runner

3. Buscar apps:
   - Barra de búsqueda
   - Filtros por categoría
   - Ordenar por popularidad/nombre

4. Ver detalles de app:
   - README
   - Versions disponibles
   - Requirements
   - Values (configuración)
   - Dependencies
----

**Añadir repositorios de charts**

.Añadir repo via UI:
[source,text]
----
1. Cluster → Apps → Repositories
2. Create
3. Configurar:
   Name: my-charts
   Index URL: https://charts.example.com/index.yaml
   
   Opciones:
   - Git Repo: Para charts en Git
   - HTTP(S): Para charts en servidor web
   - OCI: Para charts en registry OCI (Harbor, GHCR)

4. Authentication (si necesario):
   - HTTP Basic Auth
   - Bearer Token
   - SSH Key (Git)

5. Create
----

.Añadir repo via YAML:
[source,yaml]
----
apiVersion: catalog.cattle.io/v1
kind: ClusterRepo
metadata:
  name: my-charts
spec:
  # HTTP(S) repository
  url: https://charts.example.com
  
  # O Git repository
  gitRepo: https://github.com/myorg/charts.git
  gitBranch: main
  
  # Autenticación HTTP Basic
  clientSecret:
    name: my-charts-auth
    namespace: cattle-system
  
  # Refresh interval
  forceUpdate: false
  
  # Insecure skip TLS verify (no recomendado)
  insecure: false
  
  # CA cert (si es necesario)
  caBundle: |
    -----BEGIN CERTIFICATE-----
    MIIDXTCCAkWgAwIBAgIJAKJ...
    -----END CERTIFICATE-----

---
# Secret para autenticación
apiVersion: v1
kind: Secret
metadata:
  name: my-charts-auth
  namespace: cattle-system
type: kubernetes.io/basic-auth
stringData:
  username: myuser
  password: mypassword
----

.Añadir repo OCI (Harbor/GHCR):
[source,yaml]
----
apiVersion: catalog.cattle.io/v1
kind: ClusterRepo
metadata:
  name: harbor-charts
spec:
  # OCI registry
  url: oci://harbor.example.com/charts
  
  # Autenticación
  clientSecret:
    name: harbor-registry-creds
    namespace: cattle-system

---
apiVersion: v1
kind: Secret
metadata:
  name: harbor-registry-creds
  namespace: cattle-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: eyJhdXRocyI6eyJoYXJib3IuZXhhbXBsZS5jb20iOnsidXNlcm5hbWUiOiJhZG1pbiIsInBhc3N3b3JkIjoiSGFyYm9yMTIzNDUiLCJhdXRoIjoiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9In19fQ==
----

==== Instalación de aplicaciones desde Helm charts

**Instalar app desde el marketplace**

.Proceso de instalación paso a paso:
[source,text]
----
Ejemplo: Instalar PostgreSQL

1. Cluster → Apps → Charts
2. Buscar "postgresql"
3. Seleccionar "PostgreSQL" (Bitnami)
4. Click "Install"

5. Configuración básica:
   - Name: postgres-prod
   - Namespace: database (crear si no existe)
   - Project: Database Project
   - Description: Production PostgreSQL database

6. Chart Options:
   Tab "Values YAML":
   Editar configuración en YAML o usar form

7. Configuración importante:
   
   Global:
     postgresql.postgresqlUsername: postgres
     postgresql.postgresqlPassword: <secure-password>
     postgresql.postgresqlDatabase: myapp
   
   Persistence:
     persistence.enabled: true
     persistence.storageClass: fast-ssd
     persistence.size: 100Gi
   
   Resources:
     resources.requests.memory: 2Gi
     resources.requests.cpu: 1000m
     resources.limits.memory: 4Gi
     resources.limits.cpu: 2000m
   
   Replication:
     replication.enabled: true
     replication.readReplicas: 2
   
   Metrics:
     metrics.enabled: true
     metrics.serviceMonitor.enabled: true

8. Review and Install
9. Ver progreso en Apps → Installed Apps
----

**Configuración avanzada con values.yaml**

.Ejemplo completo de values para PostgreSQL:
[source,yaml]
----
# Global
global:
  postgresql:
    postgresqlUsername: postgres
    postgresqlPassword: "SuperSecurePassword123!"
    postgresqlDatabase: myapp
  
# Image
image:
  registry: docker.io
  repository: bitnami/postgresql
  tag: 15.5.0
  pullPolicy: IfNotPresent

# Replication
replication:
  enabled: true
  readReplicas: 2
  synchronousCommit: "on"
  numSynchronousReplicas: 1

# Persistence
persistence:
  enabled: true
  storageClass: "fast-ssd"
  accessModes:
  - ReadWriteOnce
  size: 100Gi
  annotations:
    volume.beta.kubernetes.io/storage-class: fast-ssd

# Resources
resources:
  requests:
    memory: 2Gi
    cpu: 1000m
  limits:
    memory: 4Gi
    cpu: 2000m

# PostgreSQL configuration
postgresqlConfiguration:
  max_connections: "200"
  shared_buffers: "512MB"
  effective_cache_size: "2GB"
  maintenance_work_mem: "256MB"
  checkpoint_completion_target: "0.9"
  wal_buffers: "16MB"
  default_statistics_target: "100"
  random_page_cost: "1.1"
  work_mem: "10MB"

# Backup
backup:
  enabled: true
  schedule: "0 2 * * *"  # Daily at 2 AM
  retention: 7
  storageClass: "standard"

# Metrics
metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    namespace: cattle-monitoring-system
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      memory: 512Mi
      cpu: 200m

# Security
securityContext:
  enabled: true
  fsGroup: 1001
  runAsUser: 1001

containerSecurityContext:
  enabled: true
  runAsUser: 1001
  runAsNonRoot: true

# Network policy
networkPolicy:
  enabled: true
  allowExternal: false
  explicitNamespacesSelector:
    matchLabels:
      name: backend

# Service
service:
  type: ClusterIP
  port: 5432
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"

# Readiness and liveness probes
livenessProbe:
  enabled: true
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

readinessProbe:
  enabled: true
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
----

**Instalar con CLI (kubectl)**

.Usando Helm directamente:
[source,bash]
----
# Añadir repositorio
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# Buscar charts
helm search repo postgresql

# Inspeccionar chart
helm show chart bitnami/postgresql
helm show values bitnami/postgresql > postgresql-values.yaml

# Editar valores
vim postgresql-values.yaml

# Dry-run (test)
helm install postgres-prod bitnami/postgresql \
  --namespace database \
  --create-namespace \
  --values postgresql-values.yaml \
  --dry-run --debug

# Instalar
helm install postgres-prod bitnami/postgresql \
  --namespace database \
  --create-namespace \
  --values postgresql-values.yaml \
  --version 12.12.10

# Verificar instalación
helm list -n database
helm status postgres-prod -n database
kubectl get all -n database
----

**Instalar apps con dependencias**

Algunas apps requieren otras apps instaladas primero.

.Ejemplo: Instalar app que requiere cert-manager:
[source,bash]
----
# 1. Instalar cert-manager primero
helm repo add jetstack https://charts.jetstack.io
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.14.0 \
  --set installCRDs=true

# Esperar a que esté listo
kubectl wait --for=condition=Available --timeout=300s \
  -n cert-manager deployment --all

# 2. Ahora instalar app que depende de cert-manager
helm install myapp myrepo/myapp \
  --namespace myapp \
  --create-namespace \
  --set certManager.enabled=true
----

**Troubleshooting instalación**

.Comandos de diagnóstico:
[source,bash]
----
# Ver estado de release
helm status myapp -n myapp

# Ver historial de releases
helm history myapp -n myapp

# Ver valores actuales
helm get values myapp -n myapp

# Ver todos los valores (incluidos defaults)
helm get values myapp -n myapp --all

# Ver manifest generado
helm get manifest myapp -n myapp

# Ver hooks de pre/post install
helm get hooks myapp -n myapp

# Ver notas de instalación
helm get notes myapp -n myapp

# Ver logs de pods de la app
kubectl logs -n myapp -l app=myapp --tail=100

# Ver eventos del namespace
kubectl get events -n myapp --sort-by='.lastTimestamp'

# Describe pods con problemas
kubectl describe pod -n myapp <pod-name>
----

==== Gestión de versiones y actualizaciones

**Ver versiones disponibles**

.Listar versiones de un chart:
[source,bash]
----
# Via Helm
helm search repo postgresql --versions | head -20

# Via Rancher UI
# Apps → Charts → PostgreSQL → Versions dropdown

# Ver changelog entre versiones
helm show readme bitnami/postgresql --version 12.12.10
----

**Actualizar aplicación**

.Update via Rancher UI:
[source,text]
----
1. Apps → Installed Apps
2. Seleccionar app (postgres-prod)
3. Click "Upgrade"
4. Opciones:
   - Change Version: Seleccionar nueva versión
   - Edit Values: Modificar configuración
   - Review Diff: Ver cambios en valores

5. Upgrade Strategy:
   - Rolling Update (default)
   - Recreate (downtime)

6. Click "Upgrade"
7. Monitorear rollout
----

.Update via Helm CLI:
[source,bash]
----
# Ver versión actual
helm list -n database

# Actualizar a nueva versión (mantener valores)
helm upgrade postgres-prod bitnami/postgresql \
  --namespace database \
  --version 13.0.0 \
  --reuse-values

# Actualizar con nuevos valores
helm upgrade postgres-prod bitnami/postgresql \
  --namespace database \
  --version 13.0.0 \
  --values postgresql-values-updated.yaml

# Dry-run para ver cambios
helm upgrade postgres-prod bitnami/postgresql \
  --namespace database \
  --version 13.0.0 \
  --reuse-values \
  --dry-run --debug

# Actualizar solo valores (misma versión)
helm upgrade postgres-prod bitnami/postgresql \
  --namespace database \
  --reuse-values \
  --set resources.requests.memory=4Gi

# Forzar upgrade (aunque no hay cambios)
helm upgrade postgres-prod bitnami/postgresql \
  --namespace database \
  --reuse-values \
  --force

# Wait hasta que esté ready
helm upgrade postgres-prod bitnami/postgresql \
  --namespace database \
  --version 13.0.0 \
  --reuse-values \
  --wait --timeout 10m
----

**Rollback a versión anterior**

.Rollback via Rancher UI:
[source,text]
----
1. Apps → Installed Apps
2. Seleccionar app
3. Click en menú (⋮) → Rollback
4. Seleccionar revisión:
   - Ver historial de revisiones
   - Ver diff de valores
5. Confirm Rollback
----

.Rollback via Helm:
[source,bash]
----
# Ver historial
helm history postgres-prod -n database

# Output:
# REVISION  UPDATED                   STATUS      CHART              DESCRIPTION
# 1         Mon Oct 30 10:00:00 2025  superseded  postgresql-12.12.10 Install complete
# 2         Mon Oct 30 14:00:00 2025  superseded  postgresql-13.0.0   Upgrade complete
# 3         Mon Oct 30 16:00:00 2025  deployed    postgresql-13.1.0   Upgrade complete

# Rollback a revisión anterior
helm rollback postgres-prod -n database

# Rollback a revisión específica
helm rollback postgres-prod 2 -n database

# Dry-run
helm rollback postgres-prod 2 -n database --dry-run

# Forzar rollback
helm rollback postgres-prod 2 -n database --force

# Wait hasta completar
helm rollback postgres-prod 2 -n database --wait --timeout 5m
----

**Estrategias de actualización**

.Tabla de estrategias:
[cols="1,2,1,2", options="header"]
|===
|Estrategia |Descripción |Downtime |Uso

|Rolling Update
|Actualizar pods gradualmente
|No
|Aplicaciones stateless (recomendado)

|Recreate
|Eliminar todos los pods, crear nuevos
|Sí
|Aplicaciones stateful con constraints

|Blue-Green
|Desplegar versión nueva, switchear tráfico
|No
|Rollback instantáneo requerido

|Canary
|Desplegar a % de usuarios, incrementar gradualmente
|No
|Testing en producción, apps críticas
|===

**Políticas de actualización automática**

.Auto-update con Rancher Fleet:
[source,yaml]
----
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: apps-repo
  namespace: fleet-default
spec:
  repo: https://github.com/myorg/fleet-apps
  branch: main
  
  # Auto-update cada 15 minutos
  pollingInterval: 15m
  
  targets:
  - name: production
    clusterSelector:
      matchLabels:
        environment: production
  
  # Helm chart options
  helmRepoURLRegex: "https://charts.bitnami.com/.*"
  
  # Auto-update strategy
  forceNamespace: ""
  
  # Diff options
  diff:
    comparePatches:
    - apiVersion: apps/v1
      kind: Deployment
      operations:
      - {"op":"remove", "path":"/spec/template/metadata/annotations"}
----

==== Creación de repositorios personalizados

**Estructura de un repositorio Helm**

.Estructura de directorios:
[source,text]
----
my-charts/
├── index.yaml              # Índice del repositorio
├── myapp-1.0.0.tgz        # Chart empaquetado
├── myapp-1.1.0.tgz
├── postgres-custom-1.0.0.tgz
└── charts/                 # Charts fuente (opcional)
    ├── myapp/
    │   ├── Chart.yaml
    │   ├── values.yaml
    │   ├── templates/
    │   │   ├── deployment.yaml
    │   │   ├── service.yaml
    │   │   └── ingress.yaml
    │   └── README.md
    └── postgres-custom/
        ├── Chart.yaml
        ├── values.yaml
        └── templates/
----

**Crear un chart desde cero**

.Crear estructura de chart:
[source,bash]
----
# Crear esqueleto del chart
helm create myapp

# Estructura generada
myapp/
├── Chart.yaml          # Metadata del chart
├── values.yaml         # Valores por defecto
├── charts/             # Dependencias
├── templates/          # Templates de K8s
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── ingress.yaml
│   ├── serviceaccount.yaml
│   ├── _helpers.tpl   # Helpers de template
│   ├── hpa.yaml
│   ├── NOTES.txt      # Notas post-instalación
│   └── tests/         # Tests del chart
│       └── test-connection.yaml
└── .helmignore        # Archivos a ignorar

# Editar Chart.yaml
cat > myapp/Chart.yaml <<EOF
apiVersion: v2
name: myapp
description: A Helm chart for MyApp microservice
type: application
version: 1.0.0
appVersion: "1.0.0"

maintainers:
- name: Platform Team
  email: platform@example.com

keywords:
- microservice
- api
- backend

home: https://github.com/myorg/myapp
sources:
- https://github.com/myorg/myapp

icon: https://example.com/logo.png

dependencies:
- name: postgresql
  version: "12.x.x"
  repository: https://charts.bitnami.com/bitnami
  condition: postgresql.enabled
- name: redis
  version: "17.x.x"
  repository: https://charts.bitnami.com/bitnami
  condition: redis.enabled
EOF
----

.Editar values.yaml:
[source,yaml]
----
# Default values for myapp
replicaCount: 3

image:
  repository: myregistry.example.com/myapp
  pullPolicy: IfNotPresent
  tag: ""  # Overrides the image tag (default: Chart appVersion)

imagePullSecrets:
- name: registry-secret

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
  - host: myapp.example.com
    paths:
    - path: /
      pathType: Prefix
  tls:
  - secretName: myapp-tls
    hosts:
    - myapp.example.com

resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - myapp
        topologyKey: kubernetes.io/hostname

# Application config
config:
  logLevel: info
  database:
    host: postgres
    port: 5432
    name: myapp
  redis:
    host: redis
    port: 6379

# PostgreSQL dependency
postgresql:
  enabled: true
  auth:
    username: myapp
    password: ""  # Will be generated
    database: myapp
  primary:
    persistence:
      size: 10Gi

# Redis dependency
redis:
  enabled: true
  auth:
    enabled: true
    password: ""  # Will be generated
  master:
    persistence:
      size: 8Gi
----

**Empaquetar y publicar chart**

.Empaquetar chart:
[source,bash]
----
# Validar chart
helm lint myapp/

# Empaquetar
helm package myapp/
# Output: myapp-1.0.0.tgz

# Generar índice del repositorio
helm repo index . --url https://charts.example.com

# Ver index.yaml generado
cat index.yaml
----

.Ejemplo de index.yaml:
[source,yaml]
----
apiVersion: v1
entries:
  myapp:
  - apiVersion: v2
    appVersion: 1.0.0
    created: "2025-10-30T10:00:00.000000000Z"
    description: A Helm chart for MyApp microservice
    digest: 3a2b1c4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2d3e4f5a6b7c8d9e0f1a2b
    home: https://github.com/myorg/myapp
    keywords:
    - microservice
    - api
    - backend
    maintainers:
    - email: platform@example.com
      name: Platform Team
    name: myapp
    sources:
    - https://github.com/myorg/myapp
    urls:
    - https://charts.example.com/myapp-1.0.0.tgz
    version: 1.0.0
----

**Opción 1: Publicar en servidor web estático**

.Usando nginx:
[source,bash]
----
# Subir charts a servidor web
scp *.tgz index.yaml user@charts.example.com:/var/www/charts/

# Configurar nginx
cat > /etc/nginx/sites-available/charts <<EOF
server {
    listen 80;
    server_name charts.example.com;
    
    root /var/www/charts;
    
    location / {
        autoindex on;
        try_files \$uri \$uri/ =404;
    }
    
    # CORS headers (opcional)
    add_header Access-Control-Allow-Origin *;
}
EOF

ln -s /etc/nginx/sites-available/charts /etc/nginx/sites-enabled/
nginx -t
systemctl reload nginx

# Añadir a Rancher
helm repo add mycompany https://charts.example.com
----

**Opción 2: Publicar en Harbor**

Harbor soporta Helm charts nativamente.

[source,bash]
----
# Login a Harbor
helm registry login harbor.example.com -u admin

# Push chart
helm push myapp-1.0.0.tgz oci://harbor.example.com/charts

# Añadir repo en Rancher
# Apps → Repositories → Create
# Type: OCI
# URL: oci://harbor.example.com/charts
----

**Opción 3: Publicar en GitHub Pages**

[source,bash]
----
# Estructura del repositorio Git
my-charts/
├── charts/           # Charts fuente
│   └── myapp/
├── docs/            # GitHub Pages (publicado)
│   ├── index.yaml
│   ├── myapp-1.0.0.tgz
│   └── myapp-1.1.0.tgz
└── .github/
    └── workflows/
        └── release.yaml

# GitHub Action para automatizar release
cat > .github/workflows/release.yaml <<EOF
name: Release Charts

on:
  push:
    branches:
      - main
    paths:
      - 'charts/**'

jobs:
  release:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v3
      with:
        fetch-depth: 0

    - name: Configure Git
      run: |
        git config user.name "\$GITHUB_ACTOR"
        git config user.email "\$GITHUB_ACTOR@users.noreply.github.com"

    - name: Install Helm
      uses: azure/setup-helm@v3

    - name: Run chart-releaser
      uses: helm/chart-releaser-action@v1.5.0
      env:
        CR_TOKEN: "\${{ secrets.GITHUB_TOKEN }}"
EOF

# Habilitar GitHub Pages en settings del repo
# Source: gh-pages branch

# URL del repo: https://myorg.github.io/my-charts
----

**Opción 4: ChartMuseum (servidor dedicado)**

[source,bash]
----
# Instalar ChartMuseum
helm repo add chartmuseum https://chartmuseum.github.io/charts
helm install chartmuseum chartmuseum/chartmuseum \
  --namespace chartmuseum \
  --create-namespace \
  --set env.open.DISABLE_API=false \
  --set env.open.ALLOW_OVERWRITE=true \
  --set persistence.enabled=true \
  --set persistence.size=10Gi \
  --set ingress.enabled=true \
  --set ingress.hosts[0].name=charts.example.com

# Push chart a ChartMuseum
curl --data-binary "@myapp-1.0.0.tgz" \
  https://charts.example.com/api/charts

# Añadir repo
helm repo add mycompany https://charts.example.com
----

**CI/CD para charts**

.Pipeline GitLab CI completo:
[source,yaml]
----
# .gitlab-ci.yml
stages:
  - validate
  - package
  - publish

variables:
  CHART_NAME: myapp
  CHART_REPO: https://charts.example.com

validate:
  stage: validate
  image: alpine/helm:latest
  script:
    - helm lint charts/$CHART_NAME/
    - helm template charts/$CHART_NAME/ > /dev/null
  rules:
    - changes:
      - charts/**/*

package:
  stage: package
  image: alpine/helm:latest
  script:
    - helm package charts/$CHART_NAME/
    - helm repo index . --url $CHART_REPO
  artifacts:
    paths:
      - "*.tgz"
      - index.yaml
    expire_in: 1 week
  rules:
    - changes:
      - charts/**/*
      if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH

publish:
  stage: publish
  image: curlimages/curl:latest
  script:
    - |
      for chart in *.tgz; do
        curl --fail -u $CHART_USER:$CHART_PASSWORD \
          --data-binary "@$chart" \
          $CHART_REPO/api/charts
      done
  rules:
    - changes:
      - charts/**/*
      if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
  dependencies:
    - package
----

**Best practices para charts**

.Recomendaciones:
[source,text]
----
1. Versionado:
   - Seguir Semantic Versioning (MAJOR.MINOR.PATCH)
   - Version en Chart.yaml != appVersion
   - Changelog en cada release
   - Tags Git para cada versión

2. Values:
   - Valores sensatos por defecto
   - Documentar cada valor en values.yaml
   - values-production.yaml para overrides
   - No hardcodear secrets

3. Templates:
   - Usar _helpers.tpl para código reutilizable
   - Validar inputs con required/fail
   - Labels consistentes (app.kubernetes.io/*)
   - Annotations para metadata

4. Testing:
   - helm lint en CI
   - helm template para validar
   - Tests en templates/tests/
   - Probar en staging antes de prod

5. Documentación:
   - README.md completo
   - NOTES.txt con instrucciones post-install
   - Ejemplos de values
   - Troubleshooting guide

6. Seguridad:
   - No incluir secrets en values
   - Usar external secrets
   - SecurityContext en pods
   - NetworkPolicy por defecto
   - Escaneo de vulnerabilidades
----

=== 5.2 Workloads y servicios

Rancher proporciona una interfaz unificada para gestionar todos los tipos de workloads de Kubernetes (Deployments, StatefulSets, DaemonSets), services, ingress y configuración (ConfigMaps y Secrets).

==== Gestión de Deployments, StatefulSets y DaemonSets

**Conceptos generales**

.Tabla comparativa de workload types:
[cols="1,1,1,1,1", options="header"]
|===
|Workload |Stateless |Replicas |Permanencia |Uso

|Deployment
|Sí
|Múltiples
|No
|Aplicaciones web, APIs, backends stateless

|StatefulSet
|No
|Múltiples
|Sí
|Bases de datos, cache, sistemas de ficheros

|DaemonSet
|Depende
|1 por nodo
|Sí
|Logging, monitoring, networking

|Job
|Depende
|1+
|Temporal
|Tareas batch, backups, scripts

|CronJob
|Depende
|Temporal
|Temporal
|Tareas programadas
|===

**Crear y gestionar Deployments**

.Via Rancher UI:
[source,text]
----
1. Cluster → Workloads → Deployments
2. Create
3. Configurar:
   Name: nginx-app
   Namespace: default
   Replicas: 3
   
   Container Image:
   - Image: nginx:1.25
   - Pull Policy: IfNotPresent
   
   Port Mapping:
   - Name: http
   - Port: 8080
   - Container Port: 80

   Resources:
   - Requests CPU: 100m
   - Requests Memory: 128Mi
   - Limits CPU: 500m
   - Limits Memory: 512Mi

4. Security:
   - Security Context
   - Read-only filesystem: true
   - Run as non-root: true
   
5. Health:
   - Liveness Probe
   - Readiness Probe
   - Startup Probe

6. Scheduling:
   - Node selectors
   - Affinity rules
   - Tolerations

7. Create
----

.YAML Deployment completo:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app
  namespace: default
  labels:
    app.kubernetes.io/name: nginx
    app.kubernetes.io/version: "1.25"
    app.kubernetes.io/component: web
  annotations:
    description: "Production NGINX web server"
spec:
  replicas: 3
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  selector:
    matchLabels:
      app: nginx
      tier: web
  
  template:
    metadata:
      labels:
        app: nginx
        tier: web
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    
    spec:
      serviceAccountName: nginx-sa
      
      # Pod security
      securityContext:
        runAsNonRoot: true
        runAsUser: 101
        fsGroup: 101
      
      # Pod affinity para distribuir replicas
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nginx
              topologyKey: kubernetes.io/hostname
      
      # Node selectors
      nodeSelector:
        workload: web
        disk: fast-ssd
      
      # Tolerations
      tolerations:
      - key: gpu
        operator: Equal
        value: "true"
        effect: NoSchedule
      
      # Init containers
      initContainers:
      - name: init-permissions
        image: busybox:1.35
        command: ['sh', '-c', 'chmod 777 /var/cache/nginx']
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
      
      # Contenedores
      containers:
      - name: nginx
        image: nginx:1.25
        imagePullPolicy: IfNotPresent
        
        # Ports
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - name: metrics
          containerPort: 8080
          protocol: TCP
        
        # Environment variables
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "info"
        
        # Env desde ConfigMap
        envFrom:
        - configMapRef:
            name: nginx-config
        
        # Env desde Secret
        - secretRef:
            name: nginx-secrets
        
        # Recursos
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        # Probes (ver sección de Health Checks)
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        
        startupProbe:
          httpGet:
            path: /startup
            port: http
          failureThreshold: 30
          periodSeconds: 1
        
        # Security context del contenedor
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        
        # Volúmenes
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d
          readOnly: true
        - name: cache
          mountPath: /var/cache/nginx
        - name: tmp
          mountPath: /var/run
        - name: logs
          mountPath: /var/log/nginx
      
      # Volúmenes
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-conf
          defaultMode: 0644
      - name: cache
        emptyDir: {}
      - name: tmp
        emptyDir: {}
      - name: logs
        emptyDir:
          sizeLimit: 1Gi
      
      # Image pull secrets
      imagePullSecrets:
      - name: docker-secret
      
      # DNS config
      dnsPolicy: ClusterFirst
      
      # Termination grace period
      terminationGracePeriodSeconds: 30
----

**Actualizar Deployment**

.Via Rancher UI:
[source,text]
----
1. Cluster → Workloads → Deployments
2. Seleccionar deployment: nginx-app
3. Click en menú (⋮) → Edit YAML
4. Modificar especificación
5. Save

O Edit Config:
1. Seleccionar deployment
2. Click "Edit"
3. Cambiar imagen, replicas, etc.
4. Save
----

.Via kubectl:
[source,bash]
----
# Actualizar imagen
kubectl set image deployment/nginx-app \
  nginx=nginx:1.26 \
  -n default \
  --record

# Ver rollout status
kubectl rollout status deployment/nginx-app -n default

# Ver historial de rollouts
kubectl rollout history deployment/nginx-app -n default

# Rollback
kubectl rollout undo deployment/nginx-app -n default

# Rollback a revisión específica
kubectl rollout undo deployment/nginx-app --to-revision=2 -n default

# Pausar rollout
kubectl rollout pause deployment/nginx-app -n default

# Reanudar rollout
kubectl rollout resume deployment/nginx-app -n default

# Escalar replicas
kubectl scale deployment/nginx-app --replicas=5 -n default
----

**StatefulSets - Aplicaciones con estado**

StatefulSets son para aplicaciones que requieren:
- Identificadores únicos y estables
- Almacenamiento persistente
- Orden de deployment y escalado
- Acceso de red estable

.YAML StatefulSet (PostgreSQL):
[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-cluster
  namespace: databases
spec:
  serviceName: postgres-svc  # Requerido para StatefulSets
  replicas: 3
  
  selector:
    matchLabels:
      app: postgres
      cluster: primary
  
  template:
    metadata:
      labels:
        app: postgres
        cluster: primary
    spec:
      securityContext:
        fsGroup: 999
      
      terminationGracePeriodSeconds: 60
      
      containers:
      - name: postgres
        image: postgres:15.5-bullseye
        
        ports:
        - name: postgresql
          containerPort: 5432
          protocol: TCP
        
        env:
        - name: POSTGRES_DB
          value: "app_db"
        
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        
        # Initial data setup
        - name: POSTGRES_INITDB_ARGS
          value: |
            --max_connections=200
            --shared_buffers=512MB
            --effective_cache_size=2GB
        
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 2
        
        # Volúmenes para cada réplica
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
          subPath: postgres
        - name: config
          mountPath: /etc/postgresql
          readOnly: true
      
      # Volúmenes de configuración
      volumes:
      - name: config
        configMap:
          name: postgres-config
  
  # Volúmenes persistentes (uno por réplica)
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "fast-ssd"
      resources:
        requests:
          storage: 100Gi

---
# Headless Service requerido para StatefulSet
apiVersion: v1
kind: Service
metadata:
  name: postgres-svc
  namespace: databases
spec:
  clusterIP: None  # Headless
  selector:
    app: postgres
  ports:
  - name: postgresql
    port: 5432
    targetPort: postgresql
  publishNotReadyAddresses: true  # Para init
----

**DaemonSets - Ejecutar en cada nodo**

DaemonSets se usan para:
- Node monitoring (Prometheus, Datadog)
- Log collection (Fluentd, Filebeat)
- Network plugins (Calico, Flannel)
- Storage agents (Ceph, Longhorn)

.YAML DaemonSet (Node exporter):
[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  
  template:
    metadata:
      labels:
        app: node-exporter
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
    
    spec:
      hostNetwork: true
      hostPID: true
      hostIPC: true
      
      serviceAccountName: node-exporter
      
      # Ejecutar en todos los nodos (incluyendo master)
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      
      containers:
      - name: node-exporter
        image: quay.io/prometheus/node-exporter:v1.7.0
        
        args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/rootfs
        - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
        - --web.listen-address=:9100
        
        ports:
        - name: metrics
          containerPort: 9100
          hostPort: 9100
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        
        securityContext:
          privileged: true
        
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: rootfs
          mountPath: /rootfs
          readOnly: true
      
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: rootfs
        hostPath:
          path: /
----

==== Configuración de Services e Ingress

**Services - Exponiendo aplicaciones**

Existen 4 tipos de Services:

.Tabla de tipos de Services:
[cols="1,1,1,2", options="header"]
|===
|Tipo |Acceso |IP |Uso

|ClusterIP
|Interno
|Sí
|Comunicación entre pods

|NodePort
|Externo
|Nodo:Puerto
|Acceso externo simple

|LoadBalancer
|Externo
|LB IP
|Cloud providers, acceso externo escalable

|ExternalName
|Externo
|CNAME
|Servicios externos
|===

.YAML Service (ClusterIP):
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  namespace: default
  labels:
    app: nginx
  annotations:
    description: "Internal nginx service"
spec:
  type: ClusterIP
  
  # Puede ser None (headless) para StatefulSets
  clusterIP: 10.96.0.10
  
  selector:
    app: nginx
    tier: web
  
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
  - name: https
    port: 443
    targetPort: 8443
    protocol: TCP
  
  # Sticky sessions
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  
  # IP internas adicionales
  clusterIPs:
  - 10.96.0.10
  
  # Pub IP (si es needed)
  externalIPs:
  - 192.168.1.100
  
  # Protocolos IPv4/IPv6
  ipFamilyPolicy: SingleStack
  ipFamilies:
  - IPv4
  
  # Health check externo
  externalTrafficPolicy: Local
  healthCheckNodePort: 30000
----

.YAML Service (NodePort):
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: app-nodeport
  namespace: default
spec:
  type: NodePort
  
  selector:
    app: myapp
  
  ports:
  - name: http
    port: 80            # Puerto interno del cluster
    targetPort: 8080    # Puerto en el pod
    nodePort: 30080     # Puerto en el nodo (30000-32767)
    protocol: TCP
----

.YAML Service (LoadBalancer):
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: app-lb
  namespace: default
  annotations:
    # Para AWS
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
spec:
  type: LoadBalancer
  
  selector:
    app: myapp
  
  ports:
  - name: http
    port: 80
    targetPort: 8080
  
  # Para retener IP del cliente
  externalTrafficPolicy: Local
  
  # Load balancer IP (si es soportado)
  loadBalancerIP: "10.0.0.100"
  
  # Restricción de acceso
  loadBalancerSourceRanges:
  - 10.0.0.0/8
  - 192.168.0.0/16
----

**Ingress - Enrutamiento HTTP(S)**

.YAML Ingress (NGINX Ingress Controller):
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: default
  annotations:
    # NGINX annotations
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    
    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    
    # Auth básico
    nginx.ingress.kubernetes.io/auth-type: "basic"
    nginx.ingress.kubernetes.io/auth-secret: "basic-auth"
    nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
spec:
  ingressClassName: nginx
  
  # TLS/SSL
  tls:
  - hosts:
    - app.example.com
    - api.example.com
    secretName: app-tls-cert  # Generado por cert-manager
  
  - hosts:
    - admin.example.com
    secretName: admin-tls-cert
  
  rules:
  # Host principal
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-svc
            port:
              number: 80
      
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-svc
            port:
              number: 8080
  
  # Subdomain api
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-svc
            port:
              number: 8080
      
      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: api-v2-svc
            port:
              number: 8080
  
  # Admin panel
  - host: admin.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: admin-svc
            port:
              number: 3000
  
  # Wildcard
  - host: "*.apps.example.com"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app-router-svc
            port:
              number: 8080

---
# Certificate para HTTPS (cert-manager)
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: app-tls-cert
  namespace: default
spec:
  secretName: app-tls-cert
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
  - app.example.com
  - api.example.com

---
# ClusterIssuer para Let's Encrypt
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
----

**Gestionar Services e Ingress via Rancher UI**

.Crear Service:
[source,text]
----
1. Cluster → Services → Services (ClusterIP)
2. Create

Configurar:
- Name: nginx-svc
- Namespace: default
- Ports:
  * Port: 80
  * Target Port: 8080
  * Protocol: TCP

- Selectors:
  * app: nginx
  * tier: web

3. Create
----

.Crear Ingress:
[source,text]
----
1. Cluster → Services → Ingress
2. Create

Configurar:
- Name: app-ingress
- Namespace: default
- Ingress Class: nginx
- Rules:
  * Host: app.example.com
  * Path: /
  * Path Type: Prefix
  * Target Service: app-svc
  * Port: 80

- SSL/TLS:
  * Create Certificate
  * Domain: app.example.com
  * Issuer: letsencrypt-prod

3. Create
----

==== Gestión de ConfigMaps y Secrets

**ConfigMaps - Datos de configuración no sensibles**

ConfigMaps almacenan datos de configuración en pares clave-valor.

.YAML ConfigMap (aplicación web):
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: default
  labels:
    app: myapp
data:
  # Valores simples
  environment: production
  log_level: info
  max_connections: "200"
  
  # Ficheros
  app.conf: |
    server {
      listen 8080;
      server_name app.example.com;
      
      location / {
        proxy_pass http://backend:8000;
        proxy_set_header X-Real-IP $remote_addr;
      }
    }
  
  database.properties: |
    db.host=postgres
    db.port=5432
    db.pool.size=20
    db.timeout=30000
  
  settings.json: |
    {
      "api": {
        "endpoint": "https://api.example.com",
        "timeout": 30000,
        "retries": 3
      },
      "features": {
        "cache": true,
        "cdn": true
      }
    }

---
# Uso en Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        # Opción 1: Montar como volumen
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
      
      volumes:
      - name: config
        configMap:
          name: app-config
          defaultMode: 0644
          items:
          - key: app.conf
            path: nginx.conf
          - key: database.properties
            path: db.properties

---
# O inyectar como variables de entorno
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        # Opción 2: Como env vars
        envFrom:
        - configMapRef:
            name: app-config
        
        # O selectivamente
        env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: log_level
        
        - name: ENVIRONMENT
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: environment
----

**Secrets - Datos sensibles**

Secrets almacenan información sensible (contraseñas, tokens, certificados).

.Tipos de Secrets:
[cols="1,1,2", options="header"]
|===
|Tipo |Clave |Contenido

|Opaque
|string
|Datos arbitrarios (default)

|kubernetes.io/service-account-token
|token, ca.crt
|Service account token

|kubernetes.io/dockercfg
|.dockercfg
|Credenciales Docker (deprecated)

|kubernetes.io/dockerconfigjson
|.dockerconfigjson
|Credenciales Docker JSON

|kubernetes.io/basic-auth
|username, password
|Basic HTTP authentication

|kubernetes.io/ssh-auth
|ssh-privatekey
|SSH authentication

|kubernetes.io/tls
|tls.crt, tls.key
|Certificados TLS

|bootstrap.kubernetes.io/token
|token-id, token-secret
|Bootstrap token
|===

.YAML Secret (aplicación):
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: default
type: Opaque
stringData:
  # Datos en texto plano (convertido a base64)
  database_password: "SuperSecurePassword123!"
  api_key: "sk-1234567890abcdef"
  jwt_secret: "your-jwt-secret-key-here"

---
# Secret de Docker (pull images)
apiVersion: v1
kind: Secret
metadata:
  name: docker-registry
  namespace: default
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: eyJhdXRocyI6eyJyZWdpc3RyeS5leGFtcGxlLmNvbSI6eyJ1c2VybmFtZSI6Im15dXNlciIsInBhc3N3b3JkIjoibXlwYXNzd29yZCIsImF1dGgiOiJibVY1Ymk1bGVHRnRjR3hsTG1SdmJRPT0ifX19

---
# Secret TLS
apiVersion: v1
kind: Secret
metadata:
  name: app-tls
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: |
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN1akNDQWFvQ0NRQ3lZRzBVWklCRURUQU5CZ2txaGtpRzl3MEJBUXNGQURBdU1Rc3dDUVlEVlFRR0V3SkRCVEVOTUFzR0ExVUVDaE1FVW1GdVkyaHZNQ0FYRFRFNU1EWXpPVEV5TXpRME9Wb1hEVEV4TURZek9URXlNelEwT1Zvd0x6RUxNQWtHQTFVRUJoTUNRMFV4RHpBTkJnTlZCQWNUQm1GdFkyaHBiakNDQVNJd0RRWUpLb1pJaHZjTkFRRUZCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFLQzFTZFhwbjFrSHJDMU5TNWhKYjBiaFJlYzBZRjhpQ25reEd6a3dISEJTVWxGQ1pXNWpiR0ZzYkdWeVEzQjVRV2RCT1UxWWFuSkpha0Y1VGs1UmFGRnNRbEZqTW1SbVYyMDFUMFYzUkhSaGJucFRWRmMyTkU0NVNreE5SMk13V1cxT2Fsa3lSbmxrVjBFNVZsRlpNMHhEU205SE9XRkZhMjl5UjJkaGJHRlRkV2N4V1MweGEzUnVZMEZUYTBKb1VXdDNSRlJZU1hSV2JraDNXVEI0YjBSVFlVMUZlbU5zYkZSWFlqRlJlVGd3V1cxR2FGUnNVbkJrU0VFOVBRMENnZ0VCQUw2dElQSkY2dWdHRDVhZGd4VWFteFJaMUoxY25ORwp8IGRvdHMgZm9yIGJyZXZpdHkgfApjSFJ2T21GdGMyaHZjMlV1YjNKbk9rbHVSMlY1SWl3aVpHVm1ZWFZzZENJNkluSnZiM1E2T1RZMElpd2lSV1poYkhWbElqb2lRbXhoZG1WeUlpd2lSbU56VW1WeWNtOXlJams1TlN3aWRISjFjbk51YzJOaGJHaHZjMlVpT2pCOVhYMD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: |
    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

---
# Uso en Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: database_password
        
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: api_key
      
      # Para pull de imágenes privadas
      imagePullSecrets:
      - name: docker-registry
----

**Gestionar ConfigMaps y Secrets via Rancher UI**

.Crear ConfigMap:
[source,text]
----
1. Cluster → Config → ConfigMaps
2. Create
3. Configurar:
   - Name: app-config
   - Namespace: default
   - Data:
     * Key: environment
       Value: production
     * Key: log_level
       Value: info

4. Create
----

.Crear Secret:
[source,text]
----
1. Cluster → Config → Secrets
2. Create
3. Configurar:
   - Name: app-secrets
   - Namespace: default
   - Type: Opaque
   - Data:
     * Key: database_password
       Value: SecurePassword123
     * Key: api_key
       Value: sk-1234567890

4. Create
----

**Mejores prácticas para Secrets**

[source,text]
----
1. Usar Secret Management externo:
   - Hashicorp Vault
   - AWS Secrets Manager
   - Azure Key Vault
   - Google Secret Manager

2. Encriptación:
   - Encriptar secrets en etcd
   - RBAC para limitar acceso
   - Audit logs para cambios

3. Rotación:
   - Rotar secrets regularmente
   - Automatizar con operadores
   - Sin downtime

4. No commitear secrets:
   - .gitignore para secrets
   - Usar sealed-secrets o external-secrets
   - Renovación frecuente
----

==== Health checks y probes

**Tipos de probes**

Kubernetes proporciona 3 tipos de health checks:

.Tabla de probes:
[cols="1,1,1,2", options="header"]
|===
|Probe |Propósito |Acción |Timing

|Liveness
|¿El contenedor está vivo?
|Reiniciar si fallan N veces
|Periódicamente durante toda la vida

|Readiness
|¿Puede recibir tráfico?
|Remover de endpoints si fallan
|Periódicamente durante toda la vida

|Startup
|¿La app está iniciando?
|Falla si no inicia en tiempo
|Solo al inicio
|===

**Liveness Probe - Detectar contenedores muertos**

Reinicia un contenedor que está colgado pero sigue ejecutando.

.YAML con Liveness Probe:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    # Opción 1: HTTP GET
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
        httpHeaders:
        - name: X-Custom-Header
          value: Awesome
      
      initialDelaySeconds: 30  # Esperar 30s antes de empezar
      periodSeconds: 10        # Cada 10s
      timeoutSeconds: 5        # Timeout de respuesta
      successThreshold: 1      # 1 éxito = OK
      failureThreshold: 3      # 3 fallos = fallo del probe

---
# Opción 2: TCP Socket check
apiVersion: v1
kind: Pod
metadata:
  name: db-pod
spec:
  containers:
  - name: postgres
    image: postgres:15
    
    livenessProbe:
      tcpSocket:
        port: 5432
      initialDelaySeconds: 15
      periodSeconds: 20

---
# Opción 3: Exec (comando)
apiVersion: v1
kind: Pod
metadata:
  name: redis-pod
spec:
  containers:
  - name: redis
    image: redis:7
    
    livenessProbe:
      exec:
        command:
        - redis-cli
        - ping
      initialDelaySeconds: 5
      periodSeconds: 5
----

**Readiness Probe - Preparado para tráfico**

Permite que el pod reciba tráfico solo cuando está completamente listo.

.YAML con Readiness Probe:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: nginx:1.25
        
        # Readiness: ¿Puedo recibir tráfico?
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5    # Más rápido que liveness
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 2       # Menos fallos permitidos
        
        # Liveness: ¿Estoy vivo?
        livenessProbe:
          httpGet:
            path: /health
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
----

**Startup Probe - Esperar a que inicie**

Para aplicaciones que tardan mucho en iniciar.

.YAML con Startup Probe:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: slow-startup-app
spec:
  containers:
  - name: app
    image: myapp:slowstart
    
    # Startup: ¿Ya está iniciado? (timeout corto, muchos intentos)
    startupProbe:
      httpGet:
        path: /startup
        port: 8080
      failureThreshold: 30      # 30 intentos
      periodSeconds: 1          # Cada 1 segundo
      # Total: 30 segundos máximo para iniciar
    
    # Readiness: ¿Puedo recibir tráfico? (solo si startup pasó)
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 0    # Sin delay (startup ya pasó)
      periodSeconds: 5
      failureThreshold: 2
    
    # Liveness: ¿Estoy vivo? (solo si startup pasó)
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 0
      periodSeconds: 10
      failureThreshold: 3
----

**Probes avanzados**

.YAML con configuración avanzada:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: advanced-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        # Health check con headers personalizados
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
            scheme: HTTPS  # Para HTTPS
            httpHeaders:
            - name: X-Health-Check
              value: "true"
            - name: Authorization
              value: "Bearer token123"
          
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness con múltiples chequeos
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              curl -f http://localhost:8080/ready || exit 1
              # Y verificar dependencias
              [ -f /tmp/dependencies-ready ] || exit 1
          
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        
        # Lifecycle hooks (ejecución de comandos)
        lifecycle:
          # Pre-stop: Prepararse antes de terminar
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
          
          # Post-start: Ejecutar después de iniciar
          postStart:
            exec:
              command: 
              - /bin/sh
              - -c
              - |
                # Esperar a que la app esté lista
                until curl -f http://localhost:8080/health; do
                  sleep 1
                done
                echo "App started successfully"
----

**Monitoreo de probes en Rancher**

.Ver estado de probes via Rancher UI:
[source,text]
----
1. Cluster → Workloads → Deployments
2. Seleccionar deployment
3. Ver sección "Pod Status":
   - Pod name: app-7d4f5d7d8d-abc12
   - Status: Running
   - Ready: 1/1
   
4. Click en pod para ver detalles:
   - Conditions:
     * Initialized: True
     * Ready: True
     * ContainersReady: True
     * PodScheduled: True
   
   - Container Status:
     * ready: true
     * restartCount: 0
     * state: Running

5. Ver eventos:
   - Click en pod → Events tab
   - Ver cambios de estado
   - Detectar problemas de probes
----

.Debugging de probes con kubectl:
[source,bash]
----
# Ver condiciones del pod
kubectl describe pod <pod-name> -n <namespace>
# Buscar:
# - Conditions
# - Events (últimos cambios)
# - Probe failure messages

# Ver logs del contenedor
kubectl logs <pod-name> -n <namespace>

# Ver logs anteriores (si crashed)
kubectl logs <pod-name> -n <namespace> --previous

# Ejecutar comandos en el contenedor
kubectl exec -it <pod-name> -n <namespace> -- /bin/sh

# Dentro del contenedor, probar probes manualmente:
curl http://localhost:8080/health
curl http://localhost:8080/ready

# Ver cambios en tiempo real
kubectl get pod <pod-name> -n <namespace> -w

# Ver métricas
kubectl top pod <pod-name> -n <namespace>
kubectl top node
----

**Best practices para probes**

[source,text]
----
1. Startup Probe:
   - Usar para apps con inicio lento
   - failureThreshold: 30 (típico)
   - periodSeconds: 1
   - Combinar con readiness/liveness

2. Readiness Probe:
   - Verificar si está listo para servir
   - Más sensible que liveness
   - Usar HTTP 200 como success
   - Timeout corto (3-5s)

3. Liveness Probe:
   - Detectar deadlocks o infinitos loops
   - Menos sensible que readiness
   - initialDelaySeconds >= 30 para apps lentas
   - failureThreshold: 3

4. Endpoints de salud:
   - /health: Basic liveness
   - /ready: Readiness check
   - /live: Detailed liveness status
   - Retornar datos mínimos

5. Evitar:
   - No usar probes muy agresivos
   - No dejarlos sin timeout
   - No ignorar failureThreshold
   - No conectar a bases de datos en readiness
----

=== 5.3 Almacenamiento y networking

Rancher proporciona herramientas avanzadas para gestionar almacenamiento persistente y configurar networking en Kubernetes.

==== Configuración de Storage Classes

StorageClasses automatizan el aprovisionamiento de volúmenes persistentes basado en requerimientos de aplicación.

**Concepto de StorageClass**

.Tabla de StorageClasses comunes:
[cols="1,1,2,2", options="header"]
|===
|Nombre |Provisioner |Características |Uso

|fast-ssd
|ebs.csi.aws.com
|SSD GP3, IOPS hasta 16,000
|Bases de datos, aplicaciones críticas

|standard
|kubernetes.io/aws-ebs
|EBS GP2, IOPS 3,000
|Almacenamiento general

|local
|kubernetes.io/local
|Local node storage (no replicado)
|Caches, datos temporales

|longhorn
|longhorn-system
|Distributed storage Kubernetes-native
|Backup, replicación, snapshots

|cephfs
|ceph.csi.ceph.io
|Ceph distributed storage
|Enterprise storage
|===

**Crear StorageClass**

.YAML StorageClass (AWS EBS):
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete          # Delete | Retain | Recycle
allowVolumeExpansion: true     # Permitir expansión después de creación
volumeBindingMode: WaitForFirstConsumer  # WaitForFirstConsumer | Immediate

parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
  kms_key_id: "arn:aws:kms:us-east-1:123456789012:key/abc123"

---
# StorageClass estándar
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate

parameters:
  type: gp2
  iops: "1000"
  encrypted: "false"

---
# StorageClass local (high-performance)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-nvme
provisioner: kubernetes.io/local
reclaimPolicy: Delete
allowVolumeExpansion: false
volumeBindingMode: WaitForFirstConsumer

---
# StorageClass Longhorn (replicado)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-replicated
provisioner: driver.longhorn.io
reclaimPolicy: Delete
allowVolumeExpansion: true

parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"  # 48 horas
  fromBackup: ""
  fstype: "ext4"
  migratable: "false"
  repeatedFailureTTL: "604800"  # 7 días
----

**Via Rancher UI**

.Crear StorageClass:
[source,text]
----
1. Cluster → Storage → Storage Classes
2. Create
3. Configurar:
   - Name: fast-ssd
   - Provisioner: ebs.csi.aws.com
   - Reclaim Policy: Delete
   - Allow Volume Expansion: Checked
   - Volume Binding Mode: WaitForFirstConsumer
   
4. Parameters:
   - type: gp3
   - iops: 3000
   - throughput: 125
   - encrypted: true

5. Create
----

==== Gestión de Persistent Volumes

**Persistent Volumes vs Persistent Volume Claims**

- **PersistentVolume (PV)**: Recurso de storage en el cluster
- **PersistentVolumeClaim (PVC)**: Solicitud de storage por una aplicación

.Flujo de aprovisionamiento:
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

actor Pod
participant "PVC" as PVC
database "StorageClass" as SC
participant "Provisioner" as Prov
database "PV" as PV
database "Physical Storage\n(AWS EBS, etc)" as Storage

Pod -> PVC: Request 100Gi
PVC -> SC: Match StorageClass
SC -> Prov: Create volume
Prov -> Storage: Provision 100Gi volume
Storage --> Prov: Volume created
Prov -> PV: Create PV
PV -> PVC: Bind PV to PVC
PVC --> Pod: Volume ready
Pod -> Storage: Mount volume

note right of SC
  StorageClass defines
  - Provisioner
  - Parameters
  - Reclaim policy
end note

@enduml
----

**Crear Persistent Volume Claim**

.YAML PersistentVolumeClaim:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data
  namespace: databases
spec:
  accessModes:
    - ReadWriteOnce  # Solo un nodo puede leer/escribir
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi

---
# Expandible
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-logs
  namespace: applications
spec:
  accessModes:
    - ReadWriteMany  # Múltiples nodos pueden leer/escribir
  storageClassName: longhorn-replicated
  resources:
    requests:
      storage: 50Gi

---
# Acceso de solo lectura
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-data
  namespace: applications
spec:
  accessModes:
    - ReadOnlyMany  # Múltiples nodos solo lectura
  storageClassName: standard
  resources:
    requests:
      storage: 200Gi

---
# Usar PVC en Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: databases
spec:
  template:
    spec:
      containers:
      - name: postgres
        image: postgres:15
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
      
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: postgres-data

---
# Expandir PVC (si allowVolumeExpansion: true)
# Editar el PVC y cambiar storage
# kubectl patch pvc postgres-data -p '{"spec":{"resources":{"requests":{"storage":"200Gi"}}}}'
----

**Monitoreo de PVCs**

.Via Rancher UI:
[source,text]
----
1. Cluster → Storage → Persistent Volumes
2. Ver lista de PVCs:
   - Name
   - Namespace
   - Status (Bound | Pending | Lost)
   - Capacity
   - Access Modes
   - StorageClass
   - Age

3. Click en PVC para ver detalles:
   - Bound to: Qué PV está usando
   - Used: Espacio utilizado (aproximado)
   - Events: Cambios recientes

4. Acciones:
   - Delete PVC (libera espacio)
   - Edit YAML
----

.Monitoreo con kubectl:
[source,bash]
----
# Ver PVCs
kubectl get pvc -A

# Ver PVs
kubectl get pv

# Describe PVC (detalles completos)
kubectl describe pvc postgres-data -n databases

# Ver uso actual de espacio (si está soportado)
kubectl exec <pod-name> -n databases -- df -h

# Ver PVs y PVCs asociados
kubectl get pv,pvc -n databases

# Expandir PVC
kubectl patch pvc postgres-data -p '{"spec":{"resources":{"requests":{"storage":"200Gi"}}}}'

# Ver eventos de PVC
kubectl get events -n databases --field-selector involvedObject.name=postgres-data
----

**Backup de volúmenes (Longhorn)**

Si usa Longhorn StorageClass, puede hacer backups de volúmenes.

.YAML para backup de Longhorn:
[source,yaml]
----
apiVersion: longhorn.io/v1beta1
kind: Backup
metadata:
  name: postgres-backup-001
  namespace: longhorn-system
spec:
  backupTarget: s3://backups.example.com/longhorn/
  snapshotName: postgres-data-snapshot
  volumeName: postgres-data
  storageClassName: longhorn-replicated

---
# Restore desde backup
apiVersion: longhorn.io/v1beta1
kind: Restore
metadata:
  name: postgres-restore-001
  namespace: longhorn-system
spec:
  backupURL: s3://backups.example.com/longhorn/postgres-data-snapshot
  backupTarget: s3://backups.example.com/longhorn/
  volumeName: postgres-data-restored
  storageClassName: longhorn-replicated
----

==== Configuración de networking avanzada

**Network Policies - Restricción de tráfico**

Network Policies controlan el tráfico ingress/egress entre pods.

.YAML NetworkPolicy (deny-all + allow específicos):
[source,yaml]
----
# Denegar todo tráfico
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}  # Aplica a todos los pods
  policyTypes:
  - Ingress
  - Egress

---
# Permitir DNS (requerido para que funcione cualquier cosa)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53

---
# Permitir tráfico frontend -> backend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080

---
# Permitir ingress externo -> frontend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-to-frontend
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443

---
# Backend -> Base de datos
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-to-db
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: postgres
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: backend
    ports:
    - protocol: TCP
      port: 5432

---
# Permitir egress a APIs externas
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-external-apis
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  
  # HTTPS to external APIs
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32  # AWS metadata (bloquear)
        - 10.0.0.0/8          # Internal (bloquear)
    ports:
    - protocol: TCP
      port: 443
  
  # Allow to database (internal)
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
----

**Service Mesh con Istio - Observabilidad de networking**

Service Mesh proporciona control granular del tráfico, observabilidad y seguridad.

.Instalar Istio (via Rancher):
[source,bash]
----
# 1. Instalar desde Rancher UI
# Cluster → Apps → Charts
# Buscar "Istio"
# Configurar:
# - Enable monitoring
# - Tracing (Jaeger)
# - Observability

# O via Helm
helm repo add istio https://istio-release.storage.googleapis.com/charts
helm install istio-base istio/base \
  --namespace istio-system --create-namespace
helm install istiod istio/istiod \
  --namespace istio-system

# Verificar instalación
kubectl get ns -L istio-injection
kubectl get pods -n istio-system
----

.Habilitar Istio en namespace:
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    istio-injection: enabled  # Inyectar Envoy sidecars automáticamente
----

.YAML Istio VirtualService (enrutamiento avanzado):
[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: app-routes
  namespace: production
spec:
  hosts:
  - app.example.com
  - app-internal
  
  http:
  # Enrutar por header
  - match:
    - headers:
        user-type:
          exact: admin
    route:
    - destination:
        host: backend
        port:
          number: 8080
      weight: 100
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 2s
  
  # Canary deployment (90% stable, 10% new version)
  - match:
    - uri:
        prefix: /api
    route:
    - destination:
        host: backend
        subset: v1
      weight: 90
    - destination:
        host: backend
        subset: v2
      weight: 10
    timeout: 5s
  
  # Default route
  - route:
    - destination:
        host: backend
        port:
          number: 8080
      weight: 100

---
# Destination Rule (definir subsets)
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: backend-dr
  namespace: production
spec:
  host: backend
  trafficPolicy:
    connectionPool:
      http:
        http1MaxPendingRequests: 100
        http2MaxRequests: 1000
        maxRequestsPerConnection: 2
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minEjectionDuration: 30s
  
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

---
# Observabilidad: ServiceEntry (servicios externos)
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: external-api
  namespace: production
spec:
  hosts:
  - api.example.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS

---
# PeerAuthentication (mTLS - mutual TLS)
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: production
spec:
  mtls:
    mode: STRICT  # Requerir mTLS entre todos los pods

---
# AuthorizationPolicy (control de acceso)
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: backend-policy
  namespace: production
spec:
  selector:
    matchLabels:
      app: backend
  
  rules:
  # Permitir frontend -> backend
  - from:
    - source:
        principals: ["cluster.local/ns/production/sa/frontend"]
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/*"]
  
  # Permitir Prometheus -> metrics
  - from:
    - source:
        principals: ["cluster.local/ns/monitoring/sa/prometheus"]
    to:
    - operation:
        methods: ["GET"]
        paths: ["/metrics"]
----

**Observabilidad de Istio**

Istio proporciona observabilidad completa del tráfico entre servicios.

.Herramientas de observabilidad:
[source,text]
----
1. Kiali (visualización de service mesh):
   - Grafo de servicios en tiempo real
   - Métricas de tráfico
   - Rastreo de errores
   - Validación de configuración
   
   URL: http://kiali.example.com
   Usuarios/Passwords: admin/admin (por defecto)

2. Jaeger (distributed tracing):
   - Rastreo de requestsa través de servicios
   - Latencia entre servicios
   - Análisis de errores
   
   URL: http://jaeger.example.com

3. Prometheus + Grafana:
   - Métricas de Istio
   - Dashboards personalizados
   - Alertas

4. Grafana Dashboards:
   - Istio Service Dashboard
   - Istio Workload Dashboard
   - Mesh Dashboard
----

.Verificar instalación de observabilidad:
[source,bash]
----
# Verificar servicios de observabilidad
kubectl get svc -n istio-system | grep -E 'kiali|jaeger|prometheus|grafana'

# Port-forward a Kiali
kubectl port-forward -n istio-system svc/kiali 20000:20000
# Acceder: http://localhost:20000

# Port-forward a Jaeger
kubectl port-forward -n istio-system svc/jaeger 16686:16686
# Acceder: http://localhost:16686

# Port-forward a Grafana
kubectl port-forward -n istio-system svc/grafana 3000:3000
# Acceder: http://localhost:3000

# Ver logs de Envoy (sidecar de Istio)
kubectl logs -n production <pod-name> -c istio-proxy

# Ejecutar debug en Envoy proxy
kubectl exec -it -n production <pod-name> -c istio-proxy -- \
  /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --drain-time-s 5
----

==== Configuración avanzada de networking

**DNS y service discovery**

.Configuración de resolución de nombres:
[source,yaml]
----
# Coredns ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }

---
# ExternalDNS (sincronizar con cloud DNS)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
  namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: external-dns
rules:
- apiGroups: [""]
  resources: ["services","endpoints","pods"]
  verbs: ["get","watch","list"]
- apiGroups: ["extensions","networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get","watch","list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: external-dns-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
- kind: ServiceAccount
  name: external-dns
  namespace: kube-system

---
# Deployment de External DNS
apiVersion: apps/v1
kind: Deployment
metadata:
  name: external-dns
  namespace: kube-system
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: external-dns
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.k8s.io/external-dns/external-dns:v0.13.2
        args:
        - --source=service
        - --source=ingress
        - --provider=aws  # o google, azure, etc.
        - --registry=txt
        - --txt-owner-id=my-cluster
        - --aws-zone-type=public
        - --log-level=debug
----

**CNI Plugins avanzados**

Rancher soporta varios CNI (Container Network Interface) plugins.

.Opciones de CNI:
[cols="1,2,2", options="header"]
|===
|CNI |Características |Caso de uso

|Flannel
|Simple, overlay network
|Desarrollo, pequeños clusters

|Calico
|Network policies, BGP routing
|Seguridad, routing avanzado

|Cilium
|eBPF-based, observabilidad
|Seguridad, performance

|Weave
|Mesh networking, encryption
|Multi-cloud, security
|===

.Configurar Calico (en RKE2):
[source,bash]
----
# Durante creación del cluster
# Set CNI: Calico

# O instalar después
helm repo add projectcalico https://projectcalico.docs.tigera.io/charts
helm install calico projectcalico/tigera-operator \
  --namespace tigera-operator \
  --create-namespace

# Verificar
kubectl get pods -n calico-system
kubectl get nodes -o wide | grep -i calico

# Configurar políticas de red
# Ver sección anterior de NetworkPolicies
----

.Configurar Cilium (para eBPF):
[source,bash]
----
# Instalar Cilium
helm repo add cilium https://helm.cilium.io
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set kubeProxyReplacement=strict \
  --set l7Proxy=true \
  --set encryption.enabled=true

# Verificar instalación
cilium status
cilium connectivity test

# Observabilidad de Cilium
helm install hubble cilium/hubble \
  --namespace kube-system \
  --set ui.enabled=true

# Acceder a Hubble UI
kubectl port-forward -n kube-system svc/hubble-ui 8081:80
# http://localhost:8081
----

**Best practices para storage y networking**

[source,text]
----
Storage:
1. Usar StorageClasses apropiadas por requerimiento
2. Implementar snapshots para backup
3. Monitorear uso de disco
4. Política de expansión automática
5. Encriptación en reposo

Networking:
1. Implementar NetworkPolicies (deny by default)
2. Usar Service Mesh para observabilidad
3. Configurar rate limiting
4. Implementar mTLS
5. Monitoreo de tráfico
6. DNS seguro (DNSSEC)
7. Egress filtering
----

== Módulo 6: Monitoreo y Observabilidad

=== 6.1 Rancher Monitoring

Rancher incluye un stack de monitoring completo basado en Prometheus y Grafana que proporciona visibilidad en tiempo real de clusters, nodos y aplicaciones.

==== Instalación y configuración de monitoring stack

**Arquitectura de Rancher Monitoring**

.Componentes del stack de monitoring:
[cols="1,2,2", options="header"]
|===
|Componente |Propósito |Namespace

|Prometheus
|Recopilación y almacenamiento de métricas
|cattle-monitoring-system

|Grafana
|Visualización y dashboards
|cattle-monitoring-system

|Alertmanager
|Gestión de alertas
|cattle-monitoring-system

|PrometheusOperator
|Automatizar configuración de Prometheus
|cattle-monitoring-system

|kube-state-metrics
|Métricas de objetos Kubernetes
|cattle-monitoring-system

|node-exporter
|Métricas de nodos
|cattle-monitoring-system

|Blackbox-exporter
|Monitoreo de endpoints HTTP(S)
|cattle-monitoring-system
|===

**Instalar Rancher Monitoring**

.Via Rancher UI:
[source,text]
----
1. Cluster → Apps → Charts
2. Buscar "Monitoring"
3. Seleccionar "Rancher Monitoring" (promtail stack)
4. Click "Install"

5. Configuración básica:
   Name: rancher-monitoring
   Namespace: cattle-monitoring-system
   
   Chart Options:
   
   Prometheus:
   - Enable: true
   - Retention: 24h (o personalizado)
   - Storage: 50Gi
   - StorageClass: fast-ssd
   
   Grafana:
   - Enable: true
   - Persistence: 10Gi
   - Admin password: <secure-password>
   
   Alertmanager:
   - Enable: true
   - Storage: 5Gi
   
   kube-state-metrics:
   - Enable: true
   
   node-exporter:
   - Enable: true
   
   prometheus-operator:
   - Enable: true

6. Review and Install
7. Esperar a que todos los pods estén running
----

.Verificar instalación:
[source,bash]
----
# Ver pods
kubectl get pods -n cattle-monitoring-system

# Esperado:
# alertmanager-rancher-monitoring-alertmanager-0          2/2     Running
# grafana-rancher-monitoring-grafana-*                      1/1     Running
# prometheus-rancher-monitoring-prometheus-0               2/2     Running
# rancher-monitoring-kube-state-metrics-*                  1/1     Running
# rancher-monitoring-node-exporter-*                       1/1     Running
# rancher-monitoring-prometheus-operator-*                 1/1     Running

# Ver servicios
kubectl get svc -n cattle-monitoring-system

# Port forward a Prometheus
kubectl port-forward -n cattle-monitoring-system \
  svc/rancher-monitoring-prometheus 9090:9090

# Port forward a Grafana
kubectl port-forward -n cattle-monitoring-system \
  svc/rancher-monitoring-grafana 3000:3000

# Port forward a Alertmanager
kubectl port-forward -n cattle-monitoring-system \
  svc/rancher-monitoring-alertmanager 9093:9093
----

.YAML Helm values personalizado:
[source,yaml]
----
# values-monitoring.yaml
prometheus:
  prometheusSpec:
    # Retention policy
    retention: 30d
    retentionSize: "50GB"
    
    # Storage
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: fast-ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    
    # Resources
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 4Gi
    
    # Scrape interval
    scrapeInterval: 30s
    evaluationInterval: 30s
    
    # External labels
    externalLabels:
      cluster: production
      environment: prod
    
    # Query logging
    queryLogFile: /prometheus/query.log
    
    # Remote storage (opcional)
    remoteWrite:
    - url: https://remote-prometheus.example.com/api/v1/write
      basicAuth:
        username: myuser
        password: mypassword
      writeRelabelConfigs:
      - sourceLabels: [__name__]
        regex: node_.*
        action: keep

grafana:
  replicas: 2
  
  persistence:
    enabled: true
    storageClassName: standard
    size: 10Gi
  
  adminPassword: ${GRAFANA_PASSWORD}
  
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://rancher-monitoring-prometheus:9090
        access: proxy
        isDefault: true
  
  env:
    GF_SECURITY_ADMIN_USER: admin
    GF_SECURITY_DISABLE_GRAVATAR: "true"
    GF_EXPLORE_ENABLED: "true"

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

kubeStateMetrics:
  enabled: true
  replicas: 1

nodeExporter:
  enabled: true
  tolerations:
  - effect: NoSchedule
    operator: Exists
  - effect: NoExecute
    operator: Exists
----

==== Métricas de clúster y aplicaciones

**Métricas predeterminadas**

Rancher Monitoring recopila automáticamente métricas de:
- **Nodos**: CPU, memoria, disco, red
- **Pods**: CPU, memoria, red
- **Kubelet**: API calls, volúmenes
- **API Server**: latencia, requests
- **etcd**: latencia de escritura/lectura

.Queries PromQL comunes:
[source,text]
----
# CPU usage por nodo
node_cpu_seconds_total{job="node-exporter"}

# Memoria disponible
node_memory_MemAvailable_bytes{job="node-exporter"}

# CPU usage por pod
sum(rate(container_cpu_usage_seconds_total[5m])) by (pod_name)

# Memoria usage por pod
sum(container_memory_usage_bytes) by (pod_name)

# Network I/O
rate(container_network_receive_bytes_total[5m])
rate(container_network_transmit_bytes_total[5m])

# Disk I/O
rate(node_disk_io_time_seconds_total[5m])

# Uptime del cluster
process_resident_memory_bytes

# API Server latency
histogram_quantile(0.95, rate(apiserver_request_duration_seconds_bucket[5m]))

# Pod restart count
container_last_seen{pod_name=~".*"}

# PVC usage
kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes
----

**ServiceMonitor - Monitorear aplicaciones personalizadas**

ServiceMonitor permite que Prometheus descubra y scrappee automáticamente aplicaciones.

.YAML ServiceMonitor (aplicación personalizada):
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: myapp-metrics
  namespace: production
  labels:
    app: myapp
spec:
  ports:
  - name: metrics
    port: 8080
    targetPort: 8080
  selector:
    app: myapp

---
# ServiceMonitor para scrappear las métricas
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp-monitor
  namespace: production
  labels:
    release: rancher-monitoring
spec:
  selector:
    matchLabels:
      app: myapp
  
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    
    # Relabeling (opcional)
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
    
    # Scrape relabeling
    metricRelabelings:
    - sourceLabels: [__name__]
      regex: 'myapp_.*'
      action: keep

---
# PrometheusRule para alertas
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: myapp-alerts
  namespace: production
  labels:
    prometheus: kube-prometheus
    release: rancher-monitoring
spec:
  groups:
  - name: myapp.rules
    interval: 30s
    rules:
    - alert: MyAppHighCPU
      expr: sum(rate(myapp_cpu_usage[5m])) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "MyApp CPU usage is high"
        description: "CPU usage: {{ $value }}"
    
    - alert: MyAppDown
      expr: up{job="myapp-metrics"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "MyApp is down"
----

**Exponer métricas desde aplicación**

Ejemplo de aplicación Python con Prometheus client:

.Python + Prometheus:
[source,python]
----
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# Definir métricas
request_count = Counter('myapp_requests_total', 'Total requests', ['method', 'endpoint'])
request_duration = Histogram('myapp_request_duration_seconds', 'Request duration')
active_connections = Gauge('myapp_active_connections', 'Active connections')

# Iniciar server HTTP en puerto 8080
start_http_server(8080)

# En tu aplicación:
@app.route('/api/users')
def get_users():
    request_count.labels(method='GET', endpoint='/users').inc()
    
    with request_duration.time():
        # Tu lógica
        users = fetch_users()
    
    return users

# Actualizar gauge de conexiones activas
active_connections.set(current_connections())

# Las métricas se exponen en /metrics (automático)
# http://app:8080/metrics
----

.Node.js + Prometheus:
[source,javascript]
----
const express = require('express');
const client = require('prom-client');

const app = express();

// Definir métricas
const httpRequestDuration = new client.Histogram({
  name: 'myapp_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.5, 1, 2, 5]
});

const requestCounter = new client.Counter({
  name: 'myapp_requests_total',
  help: 'Total HTTP requests',
  labelNames: ['method', 'route', 'status_code']
});

// Middleware para medir
app.use((req, res, next) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    httpRequestDuration
      .labels(req.method, req.route?.path || req.path, res.statusCode)
      .observe(duration);
    
    requestCounter
      .labels(req.method, req.route?.path || req.path, res.statusCode)
      .inc();
  });
  
  next();
});

// Exponer métricas
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', client.register.contentType);
  res.end(await client.register.metrics());
});

app.listen(8080);
----

==== Alertas y notificaciones

**Configurar Alertmanager**

Alertmanager gestiona alertas, realiza deduplicación y enrutamiento.

.YAML Alertmanager configuration:
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: cattle-monitoring-system
type: Opaque
stringData:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
    
    route:
      # Receptor por defecto
      receiver: 'default-receiver'
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      
      # Sub-rutas
      routes:
      # Alertas críticas -> PagerDuty
      - match:
          severity: critical
        receiver: pagerduty-receiver
        repeat_interval: 1h
      
      # Alertas de backend
      - match:
          component: backend
        receiver: backend-team
        group_wait: 5s
        group_interval: 5s
    
    receivers:
    # Slack default
    - name: 'default-receiver'
      slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
    
    # Backend team Slack
    - name: 'backend-team'
      slack_configs:
      - channel: '#backend-alerts'
        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        text: 'Cluster: {{ .GroupLabels.cluster }}'
    
    # PagerDuty
    - name: 'pagerduty-receiver'
      pagerduty_configs:
      - service_key: 'YOUR-PAGERDUTY-SERVICE-KEY'
        description: '{{ .GroupLabels.alertname }}'
        details:
          firing: '{{ range .Alerts.Firing }}{{ .Labels.instance }},{{ end }}'
    
    # Email
    - name: 'email-receiver'
      email_configs:
      - to: 'ops@example.com'
        from: 'alerts@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alerts@example.com'
        auth_password: 'password'
        headers:
          Subject: '[{{ .GroupLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}'
    
    # Webhook personalizado
    - name: 'webhook-receiver'
      webhook_configs:
      - url: 'https://hooks.example.com/alerts'
        send_resolved: true
    
    inhibit_rules:
    # Suprimir alertas non-críticas si hay críticas
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'instance']
    
    # Suprimir alertas si el nodo está down
    - source_match:
        alertname: NodeDown
      target_match_re:
        alertname: '.*'
      equal: ['instance']
----

**Crear PrometheusRules (alertas personalizadas)**

.YAML PrometheusRule completo:
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cluster-alerts
  namespace: cattle-monitoring-system
  labels:
    prometheus: kube-prometheus
    release: rancher-monitoring
spec:
  groups:
  - name: cluster.rules
    interval: 30s
    rules:
    # Node alerts
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} is not ready"
        description: "Node has been unready for more than 5 minutes"
    
    - alert: NodeHighCPU
      expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} CPU usage is high"
        description: "CPU usage: {{ $value }}%"
    
    - alert: NodeHighMemory
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.instance }} memory usage is high"
        description: "Memory usage: {{ $value }}%"
    
    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} has disk pressure"
    
    # Pod alerts
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping"
        description: "Restart rate: {{ $value }}"
    
    - alert: PodNotHealthy
      expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not healthy"
    
    - alert: ContainerWaiting
      expr: sum by (pod, namespace) (kube_pod_container_status_waiting) > 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "Container in pod {{ $labels.pod }} has been waiting for more than 1 hour"
    
    # Deployment alerts
    - alert: DeploymentGenerationMismatch
      expr: kube_deployment_status_observed_generation{namespace=~".*"} != kube_deployment_metadata_generation{namespace=~".*"}
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "Deployment {{ $labels.deployment }} generation mismatch"
    
    - alert: DeploymentReplicasMismatch
      expr: |
        kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Deployment {{ $labels.deployment }} replicas mismatch"
        description: "Desired: {{ $value }}"
    
    # PVC alerts
    - alert: PersistentVolumeInodeFull
      expr: (kubelet_volume_stats_inodes_used / kubelet_volume_stats_inodes) > 0.95
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} inodes almost full"
        description: "Usage: {{ $value | humanizePercentage }}"
    
    - alert: PersistentVolumeFull
      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} almost full"
        description: "Usage: {{ $value | humanizePercentage }}"
    
    # API Server alerts
    - alert: KubeApiServerLatency
      expr: histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket{job="kube-apiserver"}[5m])) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "API Server latency is high"
        description: "p99 latency: {{ $value }}s"
    
    - alert: KubeControllerManagerDown
      expr: up{job="kube-controller-manager"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kube Controller Manager is down"
    
    - alert: KubeSchedulerDown
      expr: up{job="kube-scheduler"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kube Scheduler is down"
----

**Gestionar alertas via UI**

.En Rancher UI:
[source,text]
----
1. Cluster → Monitoring → Alert Rules
   - Ver todas las alertas activas
   - Estado: Firing | Resolved
   - Severidad: critical | warning | info

2. Filtrar por:
   - Namespace
   - Severidad
   - Estado

3. Ir a Alertmanager:
   - Cluster → Monitoring → Alert Manager
   - Ver rutas de alertas
   - Ver inhibiciones activas

4. Test de alertas:
   kubectl exec -n cattle-monitoring-system \
     prometheus-rancher-monitoring-prometheus-0 -- \
     amtool alert add TestAlert severity=warning
----

==== Dashboards personalizados

**Dashboards predefinidos**

Rancher incluye dashboards preconstruidos:
- Cluster Monitoring
- Node Exporter
- Kubernetes API Server
- Etcd
- Kubelet
- Controller Manager

.Ver dashboards:
[source,text]
----
1. Cluster → Monitoring → Dashboards
2. O en Grafana UI:
   - http://grafana.example.com (port-forward)
   - Login: admin / <password>
   - Home → Dashboards
----

**Crear dashboard personalizado**

.Via Grafana UI:
[source,text]
----
1. Grafana → Create → Dashboard
2. Add new panel
3. Configurar:
   - Title: CPU Usage by Node
   - Data source: Prometheus
   - PromQL Query:
     100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
   
4. Visualization:
   - Type: Graph / Gauge / Stat / Table
   - Color scheme: Green-Yellow-Red
   - Thresholds: 60, 80

5. Alert:
   - Threshold: 80
   - No data: Alerting
   - Execution errors: Alerting

6. Save Dashboard
----

.YAML ConfigMap con dashboard:
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-dashboard
  namespace: cattle-monitoring-system
  labels:
    grafana_dashboard: "1"
data:
  custom-dashboard.json: |
    {
      "dashboard": {
        "title": "Custom Application Monitoring",
        "timezone": "browser",
        "panels": [
          {
            "title": "Request Rate",
            "targets": [
              {
                "expr": "rate(myapp_requests_total[5m])",
                "legendFormat": "{{ method }} {{ endpoint }}"
              }
            ],
            "type": "graph"
          },
          {
            "title": "Request Duration",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(myapp_request_duration_seconds_bucket[5m]))",
                "legendFormat": "p95"
              },
              {
                "expr": "histogram_quantile(0.99, rate(myapp_request_duration_seconds_bucket[5m]))",
                "legendFormat": "p99"
              }
            ],
            "type": "graph"
          },
          {
            "title": "Error Rate",
            "targets": [
              {
                "expr": "rate(myapp_errors_total[5m])",
                "legendFormat": "{{ error_type }}"
              }
            ],
            "type": "graph"
          },
          {
            "title": "Active Connections",
            "targets": [
              {
                "expr": "myapp_active_connections"
              }
            ],
            "type": "gauge"
          }
        ]
      }
    }
----

**Exportar e importar dashboards**

.Export dashboard como JSON:
[source,bash]
----
# En Grafana UI:
1. Dashboard → Settings (gear icon)
2. Click "Save as JSON"
3. Copiar JSON

# O via API:
curl -H "Authorization: Bearer $GRAFANA_TOKEN" \
  http://localhost:3000/api/dashboards/db/cluster-monitoring \
  | jq .dashboard > dashboard.json
----

.Importar dashboard:
[source,bash]
----
# En Grafana UI:
1. Create → Import
2. Pegar JSON o URL
3. Seleccionar data source (Prometheus)
4. Save

# O via API:
curl -X POST \
  -H "Authorization: Bearer $GRAFANA_TOKEN" \
  -H "Content-Type: application/json" \
  -d @dashboard.json \
  http://localhost:3000/api/dashboards/db
----

**Variables en dashboards**

Las variables permiten crear dashboards dinámicos.

.Crear variable de namespace:
[source,text]
----
1. Dashboard → Settings → Variables
2. New Variable
3. Configurar:
   Name: namespace
   Query: label_values(kube_pod_labels, namespace)
   Refresh: On dashboard load
   Selection: Multi-select

4. Usar en queries:
   sum by (pod) (rate(container_cpu_usage_seconds_total{namespace=~"$namespace"}[5m]))

5. En el dashboard, el selector aparecerá en la parte superior
----

**Alerting en Grafana**

Grafana puede enviar alertas directamente sin Alertmanager.

.Configurar alerting:
[source,text]
----
1. Dashboard → Panel → Alert tab
2. Configurar:
   - Alert name: High CPU
   - Evaluate every: 1m
   - For: 5m
   
3. Conditions:
   - When: avg()
   - Of: query A
   - Is above: 80

4. No data and execution failures:
   - Alerting
   - OK

5. Notification channels:
   - Send to: Slack / Email / PagerDuty / Webhook

6. Annotations:
   - Summary: CPU usage is high
   - Runbook: https://wiki.example.com/high-cpu

7. Save
----

**Best practices para monitoring**

[source,text]
----
1. Métricas:
   - Usar labels consistentes
   - No explotar cardinalidad (evitar user IDs como label)
   - Retención apropiada por criticidad
   - Usar histogramas para latencia

2. Alertas:
   - Alertas accionables (no ruido)
   - Incluir runbook en descripción
   - Severidad clara (critical/warning/info)
   - Distinct alerts por problema

3. Dashboards:
   - Una métrica = un panel
   - Titles claros y descriptivos
   - Variables para flexibilidad
   - Documentar en panels

4. Escalado:
   - Retention basado en importancia
   - Remote storage para larga duración
   - Federation para multi-cluster
   - High-availability Prometheus
----

=== 6.2 Logging

Logging centralizado proporciona visibilidad completa de eventos en aplicaciones y el cluster. Rancher integra herramientas como Elasticsearch, Fluentd y Kibana para gestión de logs a escala.

==== Configuración de logging centralizado

**Arquitectura de logging**

.Flujo de logs en Kubernetes:
[source,plantuml]
----
@startuml
!define RECTANGLE rectangle

skinparam backgroundColor transparent

actor "Containers\n(stdout/stderr)" as Containers
participant "Kubelet" as Kubelet
database "Container Logs\n(/var/log/containers)" as Logs
participant "Fluentd/Filebeat" as Forwarder
database "Buffer" as Buffer
database "Elasticsearch\nor S3/GCS" as Store
participant "Kibana\nor UI" as UI
actor "User" as User

Containers -> Kubelet: Write logs
Kubelet -> Logs: Save logs
Forwarder -> Logs: Collect logs
Forwarder -> Buffer: Parse & buffer
Buffer -> Store: Forward logs
Store --> UI: Indexed data
UI --> User: Search & visualize

note right of Forwarder
  Parse, filter, transform
  Add metadata (pod, ns, node)
  Batch & compress
end note

@enduml
----

**Opciones de logging en Rancher**

.Tabla de soluciones de logging:
[cols="1,1,2,2", options="header"]
|===
|Solución |Provisioner |Características |Mejor para

|Elasticsearch + Kibana
|Elastic
|Full-featured, scalable, costoso
|Enterprise, analytics avanzado

|Loki + Promtail
|Grafana
|Lightweight, no indexación completa
|Startups, observabilidad simple

|Splunk
|Splunk
|Enterprise-grade, caro
|Large enterprises

|S3/CloudStorage
|Cloud provider
|Cheap, batch processing
|Compliance, long-term storage

|Datadog/New Relic
|SaaS
|Managed, caro
|Startups con $ disponible
|===

**Instalar stack de logging (Elasticsearch + Fluentd + Kibana)**

.Via Rancher UI - Instalar Elasticsearch:
[source,text]
----
1. Cluster → Apps → Charts
2. Buscar "Elasticsearch"
3. Instalar versión recomendada (por ej., Bitnami)
4. Configurar:
   - Replicas: 3 (para HA)
   - Storage: 100Gi minimum
   - Resources: 2Gi RAM minimum por node

5. Create
----

.YAML Elasticsearch Stack (completo):
[source,yaml]
----
# Namespace para logging
apiVersion: v1
kind: Namespace
metadata:
  name: logging
  labels:
    name: logging

---
# Elasticsearch
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: elasticsearch
  namespace: logging
spec:
  version: 8.11.0
  
  nodeSets:
  - name: data
    count: 3
    config:
      node.store.allow_mmap: false
      node.roles: ["master", "data", "ingest"]
      xpack.security.enabled: true
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 2Gi
              cpu: 500m
            limits:
              memory: 4Gi
              cpu: 2000m
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: elasticsearch.k8s.elastic.co/statefulset-name
                    operator: In
                    values:
                    - elasticsearch-data
                topologyKey: kubernetes.io/hostname
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: fast-ssd

---
# Kibana
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: kibana
  namespace: logging
spec:
  version: 8.11.0
  count: 2
  elasticsearchRef:
    name: elasticsearch
  config:
    xpack.security.enabled: true
    xpack.reporting.enabled: false
    kibana.defaultAppId: discover
  podTemplate:
    spec:
      containers:
      - name: kibana
        resources:
          requests:
            memory: 1Gi
            cpu: 100m
          limits:
            memory: 2Gi
            cpu: 500m
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: kibana.k8s.elastic.co/name
                  operator: In
                  values:
                  - kibana
              topologyKey: kubernetes.io/hostname

---
# Ingress para Kibana
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: logging
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - kibana.example.com
    secretName: kibana-tls
  rules:
  - host: kibana.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana
            port:
              number: 5601
----

==== Integración con Elasticsearch y Fluentd

**Instalar Fluent Bit (alternativa lightweight a Fluentd)**

.YAML Fluent Bit DaemonSet:
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit
rules:
- apiGroups: [""]
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit
subjects:
- kind: ServiceAccount
  name: fluent-bit
  namespace: logging

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Daemon Off
        Flush 5
        Log_Level info
        Parsers_File parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        HTTP_Port 2020
        Health_Check On

    [INPUT]
        Name tail
        Path /var/log/containers/*.log
        Parser docker
        Tag kubernetes.*
        Refresh_Interval 5
        Mem_Buf_Limit 50MB
        Skip_Long_Lines On

    [INPUT]
        Name systemd
        Tag host.*
        Systemd_Filter _SYSTEMD_UNIT=kubelet.service
        Read_From_Tail On

    [FILTER]
        Name kubernetes
        Match kubernetes.*
        Kube_URL https://kubernetes.default.svc:443
        Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix kubernetes.var.log.containers.
        Merge_Log On
        Keep_Log Off
        K8S-Logging.Parser On
        K8S-Logging.Exclude On

    [FILTER]
        Name modify
        Match *
        Add cluster_name production
        Add environment production

    [OUTPUT]
        Name es
        Match kubernetes.*
        Host elasticsearch.logging.svc.cluster.local
        Port 9200
        HTTP_User elastic
        HTTP_Passwd ${ELASTICSEARCH_PASSWORD}
        Logstash_Format On
        Logstash_Prefix kubernetes-${HOSTNAME}
        Time_Key @timestamp
        Type _doc
        Retry_Limit 10
        tls On
        tls.verify Off

    [OUTPUT]
        Name es
        Match host.*
        Host elasticsearch.logging.svc.cluster.local
        Port 9200
        HTTP_User elastic
        HTTP_Passwd ${ELASTICSEARCH_PASSWORD}
        Logstash_Format On
        Logstash_Prefix host-logs
        Time_Key @timestamp
        Type _doc
        tls On
        tls.verify Off

  parsers.conf: |
    [PARSER]
        Name docker
        Format json
        Time_Key time
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z
        Time_Keep On

    [PARSER]
        Name syslog
        Format regex
        Regex ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* [^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
        Time_Key time
        Time_Format %b %d %H:%M:%S

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: logging
  labels:
    app: fluent-bit
spec:
  selector:
    matchLabels:
      app: fluent-bit
  template:
    metadata:
      labels:
        app: fluent-bit
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "2020"
        prometheus.io/path: "/api/v1/metrics/prometheus"
    spec:
      serviceAccountName: fluent-bit
      terminationGracePeriodSeconds: 30
      
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:2.1.8
        
        ports:
        - name: http
          containerPort: 2020
        
        env:
        - name: ELASTICSEARCH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: password
        
        resources:
          requests:
            memory: 100Mi
            cpu: 50m
          limits:
            memory: 500Mi
            cpu: 200m
        
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config
          mountPath: /fluent-bit/etc/
          readOnly: true
      
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluent-bit-config

---
# Service para scraping de Prometheus
apiVersion: v1
kind: Service
metadata:
  name: fluent-bit
  namespace: logging
  labels:
    app: fluent-bit
spec:
  type: ClusterIP
  selector:
    app: fluent-bit
  ports:
  - name: http
    port: 2020
    targetPort: 2020
----

**Alternativa: Fluentd (más flexible)**

.YAML Fluentd StatefulSet:
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      @id input_tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels "#{ENV['FLUENT_FILTER_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
      skip_container_metadata "#{ENV['FLUENT_FILTER_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
      skip_master_url "#{ENV['FLUENT_FILTER_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
    </filter>

    <filter kubernetes.**>
      @type modify
      <replace>
        key cluster_name
        expression production
      </replace>
      <replace>
        key environment
        expression prod
      </replace>
    </filter>

    <match **>
      @type elasticsearch
      @id output_elasticsearch
      @log_level info
      include_tag_key true
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST'] || 'elasticsearch.logging.svc.cluster.local'}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT'] || 9200}"
      username "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
      password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
      logstash_format true
      logstash_prefix kubernetes-${HOSTNAME}
      <buffer tag,time>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
        queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '8'}"
        overflow_action block
      </buffer>
    </match>

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: fluentd
  namespace: logging
spec:
  replicas: 3
  serviceName: fluentd
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      securityContext:
        fsGroup: 0
      
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_USER
          value: "elastic"
        - name: FLUENT_ELASTICSEARCH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: password
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "kubernetes"
        
        resources:
          limits:
            memory: 512Mi
            cpu: 500m
          requests:
            memory: 256Mi
            cpu: 200m
        
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
        - name: buffer
          mountPath: /var/log/fluentd-buffers
      
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluentd-config
  
  volumeClaimTemplates:
  - metadata:
      name: buffer
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 10Gi
----

==== Búsqueda y análisis de logs

**Via Kibana UI**

.Explorar logs en Kibana:
[source,text]
----
1. Acceder a Kibana: https://kibana.example.com

2. Crear Index Pattern:
   - Settings → Index Patterns
   - Create Index Pattern
   - Pattern: kubernetes-*
   - Time field: @timestamp
   - Create

3. Discover:
   - Ver logs en tiempo real
   - Filtrar por:
     * Pod name
     * Namespace
     * Container
     * Log level
     * Message content

4. Búsquedas avanzadas:
   - Kibana Query Language (KQL)
   - Ejemplo: kubernetes.pod_name:"nginx-*" AND log:"error"

5. Crear visualizaciones:
   - Visualize
   - Seleccionar índice
   - Crear gráficos, tablas, mapas

6. Dashboards:
   - Combinar múltiples visualizaciones
   - Filtros globales
   - Actualización en tiempo real
----

**Kibana Query Language (KQL) - Ejemplos**

.Consultas útiles:
[source,text]
----
# Logs de error de un namespace específico
kubernetes.namespace_name:"production" AND log:"ERROR"

# Logs de un pod específico en últimos 15 minutos
kubernetes.pod_name:"my-app-123abc" AND @timestamp > now-15m

# Latencia alta en requests HTTP
http.response_time > 1000 AND http.status >= 500

# Errores en base de datos
log:"connection refused" OR log:"database error"

# Logs de múltiples aplicaciones
app:"frontend" OR app:"backend" OR app:"api"

# Logs sin errores en últimos 5 minutos
NOT log:"ERROR" AND NOT log:"WARN" AND @timestamp > now-5m

# Pods reiniciados
restart_count > 0

# Todos los logs de production
kubernetes.namespace_name:"production"

# Búsqueda por contenedor
kubernetes.container_name:"nginx"

# Logs con metadata específica
kubernetes.labels.environment:"staging"
----

**Análisis con agregaciones**

.Crear agregaciones en Kibana:
[source,text]
----
1. Visualize → Create new visualization
2. Select index pattern: kubernetes-*
3. Aggregation options:

   Metrics:
   - Average response_time
   - Max memory_usage
   - Count of errors
   
   Bucket aggregations:
   - Terms (Top 10 namespaces by request count)
   - Date histogram (errors over time)
   - Nested aggregations (errors per pod per namespace)

4. Visualización automática
5. Guardar visualización
----

**Alertas en Kibana/Elasticsearch**

.Crear alertas:
[source,yaml]
----
# Stack Rules - Alert cuando muchos errores
apiVersion: v1
kind: ConfigMap
metadata:
  name: kibana-alerts
  namespace: logging
data:
  high-error-rate.json: |
    {
      "name": "High Error Rate Alert",
      "ruleTypeId": "logs.alert_conditions",
      "schedule": {
        "interval": "5m"
      },
      "params": {
        "criteria": [
          {
            "comparator": ">",
            "fieldName": "error_count",
            "value": 100
          }
        ],
        "timeSize": 5,
        "timeUnit": "m"
      },
      "actions": [
        {
          "actionTypeId": ".slack",
          "params": {
            "message": "Alert: High error rate detected"
          }
        }
      ]
    }

---
# Vía Elasticsearch Watcher
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-watcher
  namespace: logging
data:
  high-error-alert.json: |
    {
      "trigger": {
        "schedule": {
          "interval": "5m"
        }
      },
      "input": {
        "search": {
          "request": {
            "index": ["kubernetes-*"],
            "body": {
              "query": {
                "bool": {
                  "must": [
                    { "match": { "log": "ERROR" } },
                    { "range": { "@timestamp": { "gte": "now-5m" } } }
                  ]
                }
              },
              "aggs": {
                "error_count": { "value_count": { "field": "_id" } }
              }
            }
          }
        }
      },
      "condition": {
        "compare": {
          "ctx.payload.aggregations.error_count.value": { "gt": 100 }
        }
      },
      "actions": {
        "send_email": {
          "email": {
            "to": "ops-team@example.com",
            "subject": "Alert: High Error Rate",
            "body": "Error count: {{ctx.payload.aggregations.error_count.value}}"
          }
        }
      }
    }
----

==== Políticas de retención

**Índices de Elasticsearch - Cycle y retention**

Por defecto, Fluent Bit/Fluentd crean índices diarios: `kubernetes-2024.10.31`

.YAML Index Lifecycle Management (ILM):
[source,yaml]
----
# Política ILM para rotación de índices
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-ilm-policy
  namespace: logging
data:
  kubernetes-policy.json: |
    {
      "policy": "kubernetes-logs-policy",
      "phases": {
        "hot": {
          "min_age": "0d",
          "actions": {
            "rollover": {
              "max_primary_shard_size": "50GB",
              "max_age": "1d"
            },
            "set_priority": {
              "priority": 100
            }
          }
        },
        "warm": {
          "min_age": "7d",
          "actions": {
            "set_priority": {
              "priority": 50
            },
            "forcemerge": {
              "max_num_segments": 1
            }
          }
        },
        "cold": {
          "min_age": "30d",
          "actions": {
            "set_priority": {
              "priority": 0
            },
            "searchable_snapshot": {
              "snapshot_repository": "backup"
            }
          }
        },
        "delete": {
          "min_age": "90d",
          "actions": {
            "delete": {}
          }
        }
      }
    }

---
# Index Template para aplicar ILM automáticamente
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-index-template
  namespace: logging
data:
  kubernetes-template.json: |
    {
      "index_patterns": ["kubernetes-*"],
      "template": {
        "settings": {
          "number_of_shards": 1,
          "number_of_replicas": 1,
          "index.lifecycle.name": "kubernetes-logs-policy",
          "index.lifecycle.rollover_alias": "kubernetes-logs"
        },
        "mappings": {
          "properties": {
            "@timestamp": { "type": "date" },
            "log": { "type": "text" },
            "kubernetes": {
              "properties": {
                "pod_name": { "type": "keyword" },
                "namespace_name": { "type": "keyword" },
                "container_name": { "type": "keyword" },
                "node_name": { "type": "keyword" }
              }
            }
          }
        }
      }
    }
----

**Políticas de retención según compliance**

.Retención por tipo de log:
[source,text]
----
Logs de aplicación:
- Retención: 30 días
- ILM: Hot (7d) → Warm (23d) → Delete (30d)
- Compresión: Sí (después de 7d)

Logs de auditoría (Kubernetes):
- Retención: 1 año (compliance)
- ILM: Hot (30d) → Warm (335d) → Delete (365d)
- Snapshot: Diarios a S3

Logs de seguridad (RBAC, network):
- Retención: 90 días (regulatory)
- ILM: Hot (14d) → Warm (76d) → Delete (90d)
- Alertas: Críticas (24h retention)

Logs de sistema:
- Retención: 14 días
- ILM: Hot (7d) → Delete (14d)
- Sampling: Después de 3d
----

**Automatizar purga de índices**

.CronJob para eliminar índices antiguos:
[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: elasticsearch-cleanup
  namespace: logging
spec:
  # Ejecutar cada día a las 3 AM
  schedule: "0 3 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: elasticsearch-cleanup
          containers:
          - name: cleanup
            image: curlimages/curl:7.85.0
            command:
            - /bin/sh
            - -c
            - |
              # Eliminar índices más antiguos de 90 días
              curl -X DELETE "elasticsearch.logging.svc.cluster.local:9200/kubernetes-$(date -d '90 days ago' +%Y.%m.%d)" \
                -u elastic:${ELASTICSEARCH_PASSWORD}
              
              # Ejecutar ILM manualmente
              curl -X POST "elasticsearch.logging.svc.cluster.local:9200/_ilm/move" \
                -H "Content-Type: application/json" \
                -u elastic:${ELASTICSEARCH_PASSWORD} \
                -d '{"sequence": 0, "phase": "delete"}'
            
            env:
            - name: ELASTICSEARCH_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elasticsearch-credentials
                  key: password
          
          restartPolicy: OnFailure

---
# ServiceAccount y Role para cleanup
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch-cleanup
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: elasticsearch-cleanup
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
----

**Monitoreo de almacenamiento**

.Verificar uso de disco en Elasticsearch:
[source,bash]
----
# Get cluster health
curl -X GET "elasticsearch.example.com:9200/_cluster/health" \
  -u elastic:password | jq '.'

# Get disk usage
curl -X GET "elasticsearch.example.com:9200/_cat/indices?v" \
  -u elastic:password | head -20

# Get node disk usage
curl -X GET "elasticsearch.example.com:9200/_cat/nodes?v&h=node,disk.total,disk.used,disk.avail,disk.percent" \
  -u elastic:password

# Expand disk threshold alert
# Alert si algún nodo usa > 85% del disco
curl -X PUT "elasticsearch.example.com:9200/_cluster/settings" \
  -u elastic:password \
  -H "Content-Type: application/json" \
  -d '{
    "transient": {
      "cluster.info.update.interval": "1m"
    }
  }'
----

**Best practices para logging**

[source,text]
----
1. Estructura de logs:
   - JSON siempre (más fácil de parsear)
   - Timestamps ISO 8601
   - Log levels: DEBUG, INFO, WARN, ERROR, FATAL
   - Context: pod, namespace, node, environment

2. Retención:
   - Compliance first (GDPR, HIPAA, SOC2)
   - Tier por importancia
   - Sampling para reducir volumen
   - Archive a S3/GCS después de N días

3. Performance:
   - Índices diarios (rollover automático)
   - Sharding apropiado (1-3 réplicas)
   - Disk I/O monitoring
   - Query optimization

4. Seguridad:
   - RBAC para acceso a índices
   - Encriptación en tránsito (TLS)
   - Enmascaramiento de datos sensibles
   - Audit trail de acceso

5. Costo:
   - Sampling de logs verbose
   - Compresión después de N días
   - Tiering a S3 (cheaper)
   - Eliminar logs no necesarios
----

=== 6.3 Troubleshooting

Troubleshooting en Rancher y Kubernetes requiere una metodología sistemática para diagnosticar y resolver problemas rápidamente.

==== Diagnóstico de problemas comunes

**Problemas con Pods**

.Tabla de estados de pods y causas:
[cols="1,1,2,2", options="header"]
|===
|Estado |Causa |Síntomas |Solución

|Pending
|Recursos insuficientes
|Pod no se asigna a nodo
|Ver eventos, escalar nodes, ajustar requests

|ImagePullBackOff
|Imagen no disponible
|No puede descargar imagen
|Verificar nombre imagen, registry, secrets

|CrashLoopBackOff
|Aplicación falla al iniciar
|Pods mueren y reinician
|Ver logs, verificar probes, revisión de código

|NotReady
|Readiness probe falha
|Pod no recibe tráfico
|Ver logs de readiness, ajustar endpoint

|Terminating
|No termina gracefully
|Tarda >30s en terminar
|Aumentar terminationGracePeriodSeconds
|===

**Diagnosticar pod crashloopping**

.Pasos para resolver:
[source,bash]
----
# 1. Ver estado del pod
kubectl describe pod myapp-xyz -n production

# Buscar:
# - Events: qué pasó recientemente
# - State: Current, Last State
# - Restart Count

# 2. Ver logs del contenedor
kubectl logs myapp-xyz -n production

# Si crashed, ver logs anteriores:
kubectl logs myapp-xyz -n production --previous

# Ver últimas N líneas
kubectl logs myapp-xyz -n production --tail=100

# Seguir logs en tiempo real
kubectl logs -f myapp-xyz -n production

# 3. Ejecutar comando en pod (si está running)
kubectl exec -it myapp-xyz -n production -- /bin/sh

# Dentro del pod:
ps aux          # Ver procesos
env             # Ver variables de entorno
cat /var/log/*  # Ver logs del sistema

# 4. Verificar readiness probe endpoint
kubectl exec myapp-xyz -n production -- curl http://localhost:8080/ready

# 5. Aumentar log level y reintentar
kubectl set env deployment/myapp LOG_LEVEL=debug -n production
kubectl rollout restart deployment/myapp -n production

# 6. Si todo falla, debuggear con alpine
kubectl debug pod/myapp-xyz -n production -it --image=alpine
----

**Problemas con Deployments**

.Diagnosticar rollout stuck:
[source,bash]
----
# Ver status del rollout
kubectl rollout status deployment/myapp -n production

# Ver historial de rollouts
kubectl rollout history deployment/myapp -n production

# Ver detalles de una revisión específica
kubectl rollout history deployment/myapp --revision=2 -n production

# Paused rollout?
kubectl get deployment myapp -n production -o yaml | grep -i paused

# Si está pausado, reanudar:
kubectl rollout resume deployment/myapp -n production

# Ver réplicas deseadas vs actuales
kubectl get deployment myapp -n production -o wide

# Buscar eventos de error
kubectl get events -n production --field-selector involvedObject.name=myapp --sort-by='.lastTimestamp'

# Debuggear replicaset
kubectl describe replicaset -l app=myapp -n production
----

**Problemas de networking**

.Diagnosticar conectividad entre pods:
[source,bash]
----
# 1. ¿Pueden comunicarse dos pods?
# Pod A -> Pod B:
kubectl exec -it <pod-a> -n ns-a -- \
  nc -zv <pod-b-ip> 8080

# O con curl:
kubectl exec -it <pod-a> -n ns-a -- \
  curl -v http://<pod-b-ip>:8080

# 2. Verificar Service discovery
kubectl exec -it <pod-a> -n ns-a -- \
  nslookup myservice.default.svc.cluster.local

# 3. Verificar iptables en nodo
kubectl debug node/<node-name> -it --image=ubuntu

# En la imagen ubuntu:
chroot /host
iptables -t nat -L -n | grep myservice
iptables -t filter -L -n

# 4. Verificar NetworkPolicy
kubectl get networkpolicy -A
kubectl describe networkpolicy <name> -n <ns>

# Testear NetworkPolicy:
kubectl exec -it <pod> -- curl http://other-service:8080

# 5. Ver endpoint del Service
kubectl get endpoints myservice -n default

# 6. Verificar CNI (calico, cilium)
kubectl get pods -n kube-system | grep -E 'calico|cilium|flannel'

# Ver logs de CNI
kubectl logs -n kube-system -l k8s-app=calico-node --tail=50
----

**Problemas de Storage**

.Diagnosticar PVC no bound:
[source,bash]
----
# 1. Ver estado de PVC
kubectl describe pvc mydata -n production

# Buscar:
# - Phase: Bound | Pending | Lost
# - Events: por qué no está bound

# 2. Verificar que StorageClass existe
kubectl get storageclass

# 3. Verificar provisioner
kubectl get storageclass fast-ssd -o yaml

# 4. Ver PVs disponibles
kubectl get pv

# 5. Ver provisioner logs
kubectl logs -n default <provisioner-pod> --tail=100

# 6. Comprobar permisos RBAC
kubectl auth can-i create pvc --as=system:serviceaccount:production:default

# 7. Si es EBS/cloud storage, verificar:
# - Créditos de cloud
# - Región/zona correcta
# - Quotas no excedidas

# 8. Crear PVC manualmente para test:
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 1Gi
EOF
----

==== Herramientas de debugging

**kubectl debug - Debuggear pods y nodos**

.Debuggear pod:
[source,bash]
----
# Crear pod de debug con imagen interactiva
kubectl debug pod/myapp -it --image=nicolaka/netshoot

# Dentro del pod debug:
# - curl, wget, telnet: networking
# - tcpdump, netstat, ss: network stats
# - strace, ltrace: system call tracing
# - ps, top: process monitoring
# - dig, nslookup: DNS resolution

# Verificar DNS:
dig myservice.default.svc.cluster.local

# Network connectivity test:
curl -v http://myservice:8080/health

# Check iptables:
iptables -L -n -t nat
----

.Debuggear nodo:
[source,bash]
----
# Crear pod privilegiado en nodo
kubectl debug node/my-node -it --image=ubuntu

# Dentro de la imagen:
chroot /host

# Ver recursos del nodo
free -h              # Memoria
df -h                # Disco
top                  # Procesos
systemctl status kubelet
journalctl -u kubelet -n 100

# Ver networking del nodo
ip link show
ip addr show
ss -tlnp

# Ver cgroups (container limits)
ls /sys/fs/cgroup
cat /sys/fs/cgroup/memory/docker/*/memory.limit_in_bytes
----

**k9s - Dashboard interactivo**

k9s es un terminal UI para Kubernetes que facilita la exploración.

[source,bash]
----
# Instalar k9s
brew install k9s  # macOS
wget https://github.com/derailed/k9s/releases/download/v0.28.4/k9s_Linux_amd64.tar.gz  # Linux
tar xf k9s_Linux_amd64.tar.gz

# Ejecutar
k9s

# Comandos en k9s:
:pods              # Ver pods
:nodes             # Ver nodos
:svc               # Ver servicios
:pvc               # Ver persistent volumes
:events            # Ver eventos

# Navegar:
- Arrow keys: navegar
- Enter: describe
- d: delete
- l: logs
- s: shell (exec)
- Port forward: :portforward

# Ejemplos:
1. Ir a :pods
2. Seleccionar pod
3. Press 'l' para ver logs
4. Press 's' para abrir shell
5. Press 'e' para editar
----

**Lens - IDE for Kubernetes**

Lens es un GUI potente para administrar clusters.

[source,text]
----
1. Descargar desde https://k8slens.dev
2. Conectar cluster:
   - Añadir cluster
   - Seleccionar kubeconfig
   - Conectar

3. Features:
   - Visual pod browser
   - Real-time metrics
   - Pod logs
   - Shell access
   - Resource editor
   - Network visualization
   - Helm integration

4. Pro features:
   - Multi-cluster
   - Cluster backup
   - Terminal access
   - Observability
----

**Prometheus + Grafana para debugging**

.Queries para debuggear:
[source,text]
----
# Pod que consume más CPU
topk(5, sum(rate(container_cpu_usage_seconds_total[5m])) by (pod_name))

# Pod que consume más memoria
topk(5, sum(container_memory_usage_bytes) by (pod_name))

# Pods con más restarts
topk(5, kube_pod_container_status_restarts_total)

# Pods pending
sum by (pod) (kube_pod_status_phase{phase="Pending"})

# Error rate por servicio
sum(rate(myapp_errors_total[5m])) by (service)

# Latencia p95 por endpoint
histogram_quantile(0.95, rate(myapp_request_duration_seconds_bucket[5m]))

# Network I/O por pod
sum(rate(container_network_receive_bytes_total[5m])) by (pod_name)
----

==== Análisis de rendimiento

**Identificar cuello de botella**

.Checklist de performance:
[source,text]
----
1. CPU:
   - Top 5 pods por CPU
   - CPU throttling en containers
   - CPU requests vs usage
   
2. Memoria:
   - Pods con OOM kills
   - Memory pressure en nodos
   - Memory leaks (crecimiento)
   
3. Disco:
   - PVC usage > 80%
   - Inode usage > 90%
   - I/O latency
   
4. Red:
   - High packet loss
   - Network saturation
   - DNS latency > 100ms
   
5. Aplicación:
   - Request latency (p95, p99)
   - Error rate
   - Garbage collection pauses
   - Database query time
----

**Profiling de aplicación**

.Python CPU profiling:
[source,python]
----
import cProfile
import pstats
from io import StringIO

def slow_function():
    total = 0
    for i in range(1000000):
        total += i
    return total

# Profiling
pr = cProfile.Profile()
pr.enable()

slow_function()

pr.disable()
s = StringIO()
ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
ps.print_stats()
print(s.getvalue())

# Output mostrará:
# - ncalls: número de llamadas
# - tottime: tiempo total en función
# - cumtime: tiempo acumulado
# - callers: quién llamó
----

.Node.js CPU profiling:
[source,javascript]
----
const v8Profiler = require('v8-profiler-next');

// Iniciar profiling
v8Profiler.startProfiling('profile', true);

// Tu código
setInterval(() => {
  // workload
}, 100);

// Después de tiempo
setTimeout(() => {
  const profile = v8Profiler.stopProfiling('profile');
  
  profile.export((err, result) => {
    require('fs').writeFileSync('profile.cpuprofile', result);
    profile.delete();
  });
}, 60000);

// Abrir profile.cpuprofile en Chrome DevTools
----

**Memory leak detection**

.Python memory leak:
[source,bash]
----
# Usar memory_profiler
pip install memory-profiler

# Anotar función
@profile
def my_function():
    x = [1] * (10 ** 6)
    y = [2] * (10 ** 6)
    return x + y

# Ejecutar con profiling
python -m memory_profiler script.py

# Output:
# Line #      Hits         Time  Per Hit   % Time  Line Contents
# 1           1       7391.0   7391.0     14.3  x = [1] * (10 ** 6)
# 2           1      14233.0  14233.0     27.6  y = [2] * (10 ** 6)
----

.Docker container memory:
[source,bash]
----
# Monitorear memory en tiempo real
docker stats mycontainer

# Ver memory limit
docker inspect mycontainer | grep -i memory

# Ver memory usage por proceso dentro del container
docker exec mycontainer ps aux --sort -rss

# Si hay memory leak:
# 1. Limitaciones de heap (si es Java)
# 2. Connection pooling (database connections)
# 3. Cache no limpiad
# 4. Event listeners no removidos
----

**Load testing**

.Apache Bench:
[source,bash]
----
# Test simple
ab -n 1000 -c 10 http://myapp.example.com/

# Con custom headers
ab -n 1000 -c 50 -H "Authorization: Bearer token" http://myapp.example.com/api

# Output:
# - Requests per second
# - Time per request
# - Transfer rate
# - Percentiles de latencia
----

.wrk (HTTP benchmarking):
[source,bash]
----
# Instalar
brew install wrk  # macOS
apt-get install build-essential && git clone https://github.com/wg/wrk.git

# Test simple
wrk -t4 -c100 -d30s http://myapp.example.com/

# Con script Lua personalizado
cat > script.lua <<EOF
request = function()
  return wrk.format(nil, "/api/users?id=" .. math.random(1, 100))
end
EOF

wrk -t4 -c100 -d30s -s script.lua http://myapp.example.com/

# Output:
# - Running 30s test
# - 4 threads
# - 100 connections
# - Latency distribution
----

==== Estrategias de resolución de incidentes

**Runbook - Guía de resolución**

.Estructura de runbook:
[source,text]
----
# Incident: PersistentVolume Full

## Symptoms
- Application writes failing
- PVC usage > 95%
- Disk full alerts firing

## Root Cause Possibilities
1. Application bug (unbounded logging)
2. Data not being cleaned up
3. PVC too small
4. Storage provisioning failed

## Diagnosis
1. Check PVC usage:
   kubectl get pvc
   kubectl describe pvc mydata
   
2. See what's using space:
   kubectl exec <pod> -- du -sh /*
   
3. Check logs:
   kubectl logs <pod> --all-containers

4. Check events:
   kubectl get events -n namespace --sort-by=.metadata.creationTimestamp

## Resolution

### Option A: Increase PVC size
- Edit PVC spec.resources.requests.storage
- Verify allowVolumeExpansion: true
- Monitor expansion progress

### Option B: Clean up data
- Identify old logs/files to delete
- Export/backup if needed
- Delete old data
- Monitor disk usage

### Option C: Enable autoscaling
- Set up script to monitor disk
- Trigger cleanup when > 80%
- Or use managed cleanup policies

## Verification
- Check PVC usage dropped
- Verify application working
- Confirm alerts resolved

## Post-mortem
- Why wasn't this caught earlier?
- Need better monitoring?
- Need larger PVC?
- Need auto-cleanup?
----

**War room process**

.Durante un incident:
[source,text]
----
1. INITIAL RESPONSE (primeros 5 min)
   - Page on-call engineer
   - Create war room channel (#incidents)
   - Gather incident details
   - Timeline: When did it start?
   - Impact: What's affected?
   - Severity: P1 (critical) | P2 (major) | P3 (minor)

2. INVESTIGATION (siguiente 15 min)
   - Incident commander:
     * Leads investigation
     * Coordinates communication
     * Makes decisions
   
   - Technical leads:
     * Investigate specific areas
     * Update commander regularly
     * Try fixes under guidance

3. MITIGATION (15-60 min)
   - Options:
     a) Quick fix (rollback, restart)
     b) Workaround (scale down, traffic shift)
     c) Degraded service (partial functionality)
     d) Investigation (deeper, more time)
   
   - Execute fix:
     * Clear communication
     * Only one person makes changes
     * Monitor impact

4. RECOVERY (post-fix)
   - Verify impact resolved
   - Monitor for side effects
   - Check metrics normalizing

5. COMMUNICATION
   - Status page updates every 10 min
   - Slack updates hourly
   - Post-incident summary

6. POST-MORTEM (24-48 horas después)
   - What happened?
   - What was the root cause?
   - How do we prevent it?
   - Action items with owners
   - Timeline (when each happened)
   - Lessons learned
----

**Status page updates**

.Ejemplo de update:
[source,text]
----
INCIDENT: API service degradation
SEVERITY: P1 - Critical

2025-10-31 15:00 UTC
INVESTIGATING
- Users experiencing 500 errors on API
- Approximately 40% of traffic affected
- Investigation ongoing

2025-10-31 15:15 UTC
IDENTIFIED
- Root cause: Database connection pool exhausted
- Queries backing up in queue
- Connections not being released

2025-10-31 15:25 UTC
MITIGATING
- Restarting affected pods to clear connections
- Increasing connection pool size
- Reducing query timeout

2025-10-31 15:40 UTC
MONITORING
- 95% of traffic recovered
- Continuing to monitor
- Will send update in 30 min

2025-10-31 16:00 UTC
RESOLVED
- All traffic normalized
- Monitoring for recurrence
- Post-mortem scheduled for 2025-11-01 14:00 UTC
----

**Escalation policy**

[source,text]
----
Level 1: Initial on-call
- 30 min to engage L2
- 60 min to declare P1 if unresolved

Level 2: Senior engineer
- 30 min to engage L3
- 60 min to declare major issue

Level 3: Staff/Architecture
- Final escalation
- Customer communication
- Major decisions

Criteria for escalation:
- P1: Revenue impact, security, widespread
- P2: Significant impact to subsystem
- P3: Minor issues, workarounds available

Response times (SLA):
- P1: 15 min acknowledge, 30 min mitigation
- P2: 1 hour acknowledge, 4 hours resolution
- P3: 8 hours acknowledge, best effort
----

**Post-incident review template**

[source,yaml]
----
Incident Report
===============

Date: 2025-10-31
Duration: 45 minutes (15:00 - 15:45 UTC)
Severity: P1 - Critical
Impact: 40% of API traffic, 5000+ affected users
Revenue Impact: ~$50K in lost transactions

Timeline
========
15:00 - Alerts start firing (500 errors)
15:05 - On-call notified
15:10 - War room opened
15:15 - Root cause identified (db connections)
15:25 - Mitigation started (pod restart)
15:40 - Service partially recovered
15:45 - Full recovery achieved

Root Cause
==========
Database connection pool exhausted due to:
- Query backlog during peak traffic
- Connections not being released due to timeout bug
- Bug introduced in deploy 3 hours earlier

Contributing Factors
====================
1. Insufficient monitoring on connection pool
2. Load test didn't simulate peak traffic
3. Rollback procedure not followed

Immediate Actions
=================
1. Revert problematic deploy (DONE)
2. Deploy connection pool fix (DONE)
3. Increase alerts on connection pool (IN PROGRESS)

Preventive Actions
===================
1. Add connection pool monitoring to dashboard
2. Improve load testing to simulate peak
3. Implement canary deployment process
4. Add pre-deployment checklist

Owners & Due Dates
==================
- Monitoring: John Smith, 2025-11-07
- Load testing: Jane Doe, 2025-11-14
- Canary deployment: Bob Johnson, 2025-11-21
----

**Best practices para troubleshooting**

[source,text]
----
1. Metodología:
   - Gather facts (no asumir)
   - Ask questions (why 5 times)
   - Test assumptions
   - Document process
   - Share learnings

2. Tooling:
   - Mantener herramientas actualizadas
   - Automatizar troubleshooting común
   - Integrar herramientas (Prometheus, etc.)
   - Dashboards por use-case

3. Comunicación:
   - Update frecuentes
   - Clear language (sin jargon)
   - Status page para users
   - Post-mortem abierto

4. Prevention:
   - Learn from incidents
   - Implement preventive measures
   - Test disaster scenarios
   - Regular game days
----

   - Learn from incidents
   - Implement preventive measures
   - Test disaster scenarios
   - Regular game days
----

== Módulo 7: Operaciones y Mantenimiento

=== 7.1 Backup y recuperación

Un plan de backup y recuperación robusto es fundamental para garantizar la continuidad del negocio y la protección de datos en Rancher y Kubernetes.

==== Estrategias de backup

**RTO y RPO - Objetivos de backup**

Dos conceptos críticos para definir la estrategia:

[cols="1,1,2", options="header"]
|===
|Métrica |Significado |Ejemplo

|RTO
|Recovery Time Objective
|¿Cuánto tiempo es aceptable de downtime?

|RPO
|Recovery Point Objective
|¿Cuántos datos podemos perder (max)?

|RTR
|Recovery Test Result
|¿Qué tan rápido se restaura en realidad?
|===

.Ejemplo de RTO/RPO:
[source,text]
----
Aplicación crítica (ecommerce):
- RTO: 1 hora máximo
- RPO: 15 minutos (perder máx 15 min de datos)
- Requiere: Backups cada 15 min + replicación

Aplicación no crítica (analytics):
- RTO: 24 horas
- RPO: 1 día
- Requiere: Backup diario

Staging/Testing:
- RTO: N/A
- RPO: N/A
- Requiere: Backup semanal
----

**Niveles de backup**

[cols="1,2,2,1", options="header"]
|===
|Nivel |Qué se respalda |Caso de uso |Frecuencia

|etcd
|Estado del cluster
|Recoverable
|Cada 1 hora

|Aplicaciones
|Deployments, configs, datos
|Critical
|Cada 15 min

|Persistencia
|PVs, databases, files
|Critical
|Cada 1 hora

|Infraestructura
|IaC, Terraform, Helm
|Best practice
|Con cambios
|===

**Estrategias de ubicación de backup**

.3-2-1 rule (best practice):
[source,text]
----
3 copias de datos:
- 1 original en cluster
- 2 copias de backup

2 formatos de almacenamiento:
- 1 local/rápido (para restauración rápida)
- 1 diferente (protección contra corrupción)

1 copia fuera de sitio:
- Almacenar en cloud/site diferente
- Protección contra desastres geográficos

Ejemplo implementación:
- Original: etcd en cluster Rancher
- Backup local: Longhorn snapshots (1 hora)
- Backup remoto: S3 (diario), cifrado
- Geo-replica: S3 replica en otra región
----

==== Herramientas de backup

**Rancher Backup - Backup nativo**

Rancher Backup es la herramienta recomendada para backups del cluster.

.Instalar Rancher Backup:
[source,bash]
----

helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
helm install rancher-backup rancher-latest/rancher-backup \
  --namespace cattle-system \
  --create-namespace \
  --set persistence.enabled=true \
  --set persistence.size=100Gi
----

.YAML BackupStorageLocation (S3):
[source,yaml]
----
apiVersion: resources.cattle.io/v1
kind: BackupStorageLocation
metadata:
  name: s3-backup
  namespace: cattle-system
spec:
  s3:
    credentialSecret:
      name: aws-s3-credentials
      namespace: cattle-system
    bucket: rancher-backups
    region: us-west-2
    endpointURL: https://s3.amazonaws.com
    insecureTLS: false
  provider: s3

--- 
apiVersion: v1
kind: Secret
metadata:
  name: aws-s3-credentials
  namespace: cattle-system
type: Opaque
stringData:
  accessKey: AKIAIOSFODNN7EXAMPLE
  secretKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
----

.YAML Backup Schedule (automático):
[source,yaml]
----
apiVersion: resources.cattle.io/v1
kind: Backup
metadata:
  name: rancher-backup-daily
  namespace: cattle-system
spec:
  backupStorageLocation:
    name: s3-backup
  
  # Schedule en cron
  schedule: "0 2 * * *"  # Cada día a las 2 AM
  
  # Retención
  retention:
    days: 30        # Borrar backups > 30 días
    maxBackups: 100  # Máximo 100 backups
  
  # Componentes a respaldar
  resourceSet:
    desiredState: "all"
    
  # Encriptación
  encryption:
    enabled: true
    secrets: true

---
  # Restore desde backup
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: rancher-restore-from-backup
  namespace: cattle-system
spec:
  backupStorageLocation:
    name: s3-backup
  
  backupName: rancher-backup-daily-20251031
  
    # Restore selectivos
  resourceSet:
    desiredState: "all"
  
    # Wait for completion
  prune: false
----

.Gestionar backups via CLI:
[source,bash]
----

  # Ver backups disponibles
kubectl get backups -n cattle-system

  # Ver detalles de backup
kubectl describe backup rancher-backup-daily-20251031 -n cattle-system

  # Crear backup manual
kubectl apply -f - <<EOF
apiVersion: resources.cattle.io/v1
kind: Backup
metadata:
  name: manual-backup-now
  namespace: cattle-system
spec:
  backupStorageLocation:
    name: s3-backup
  resourceSet:
    desiredState: "all"
EOF

  # Esperar a que se complete
kubectl wait --for=condition=Complete backup/manual-backup-now -n cattle-system --timeout=600s

  # Ver backups en S3
aws s3 ls s3://rancher-backups/

  # Descargar backup localmente
aws s3 cp s3://rancher-backups/rancher-backup-daily-20251031.tar.gz ./

  # Ver contenido del backup
tar tzf rancher-backup-daily-20251031.tar.gz | head -50
----

**Velero - Backup para datos persistentes**

Velero es ideal para respaldar volúmenes persistentes y datos de aplicaciones.

.Instalar Velero:
[source,bash]
----
  # Descargar Velero
wget https://github.com/vmware-tanzu/velero/releases/download/v1.13.0/velero-v1.13.0-linux-amd64.tar.gz
tar xzf velero-v1.13.0-linux-amd64.tar.gz
mv velero-v1.13.0-linux-amd64/velero /usr/local/bin/

  # Crear credenciales AWS
cat > credentials-velero <<EOF
[default]
aws_access_key_id = AKIAIOSFODNN7EXAMPLE
aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
EOF

  # Instalar Velero en cluster
velero install \
  --provider aws \
  --bucket velero-backups \
  --region us-west-2 \
  --secret-file ./credentials-velero \
  --use-volume-snapshots=true \
  --snapshot-location-provider aws \
  --volume-snapshot-location aws

  # Verificar instalación
velero version
kubectl get pods -n velero
----

.YAML Schedule de Velero:
[source,yaml]
----
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-full-backup
  namespace: velero
spec:
  # Cronograma
  schedule: "0 2 * * *"  # Cada día a las 2 AM
  
  # Template para backups
  template:
    includedNamespaces:
    - '*'  # Todos los namespaces
    
    includedClusterResources:
    - '*'  # Todos los recursos del cluster
    
    storageLocation: aws-s3
    
    volumeSnapshotLocation: aws-snapshots
    
    # Hooks (pre-backup cleanup, post-restore validation)
    hooks:
      resources:
      - name: pre-backup-hook
        includedNamespaces:
        - production
        execOnPod:
          container: postgres
          command:
          - /bin/bash
          - -c
          - pg_dump mydb > /tmp/backup.sql
    
    # TTL para expiración automática
    ttl: 720h  # 30 días

---
  # Backup on-demand
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: manual-backup-now
  namespace: velero
spec:
  includedNamespaces:
  - production
  
  includedResources:
  - deployments
  - statefulsets
  - persistentvolumeclaims
  
  storageLocation: aws-s3
  volumeSnapshotLocation: aws-snapshots
  
  # Exclude specific resources
  excludedNamespaces:
  - kube-system
  - velero

---
  # Restore desde backup
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore-from-backup
  namespace: velero
spec:
  backupName: daily-full-backup-20251031
  
  # Restore selectivo
  includedNamespaces:
  - production
  
  # Wait for completion
  restorePVs: true
  restoreStatus:
    includedResources:
    - pods
  
  # Restaurar a namespace diferente
  namespaceMapping:
    production: production-restored
----

.Gestionar Velero via CLI:
[source,bash]
----
  # Ver backups disponibles
velero backup get

  # Crear backup manual
velero backup create manual-backup-now \
  --include-namespaces production

  # Ver detalles de backup
velero backup describe manual-backup-now
velero backup logs manual-backup-now

  # Listar snapshots
velero snapshot-location get

  # Restaurar desde backup
velero restore create --from-backup daily-full-backup-20251031

  # Ver restores
velero restore get

  # Ver logs de restore
velero restore logs restore-daily-full-backup-20251031

  # Eliminar backup antiguo
velero backup delete daily-full-backup-20250101

  # Schedule diario
velero schedule create daily-backup \
  --schedule="0 2 * * *" \
  --include-namespaces='*' \
  --default-volumes-to-restic
----

**Longhorn - Snapshots y Backups distribuidos**

Si usa Longhorn StorageClass, puede aprovechar snapshots nativos.

.YAML Snapshot de Longhorn:
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: postgres-snapshot-001
  namespace: databases
spec:
  volumeSnapshotClassName: longhorn
  source:
    persistentVolumeClaimName: postgres-data

---
  # Ver snapshots
  # kubectl get volumesnapshot -n databases

  # Restaurar desde snapshot
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data-restored
  namespace: databases
spec:
  dataSource:
    name: postgres-snapshot-001
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn
  resources:
    requests:
      storage: 100Gi
----

==== Planes de recuperación de desastres

**Disaster Recovery Plan (DRP)**

.Componentes del DRP:
[source,text]
----
1. ASSESSMENT
   - Identificar sistemas críticos
   - Definir RTO/RPO para cada sistema
   - Evaluación de riesgos

2. MITIGATION
   - Geografía redundante
   - Multi-region deployment
   - Data replication
   - Network redundancy

3. PREPARATION
   - Backups regulares
   - Procedimientos documentados
   - Team training
   - Runbooks preparados

4. DETECTION
   - Alertas de anomalías
   - Monitoreo continuo
   - Pruebas periódicas

5. RESPONSE
   - Escalation procedures
   - Communication plan
   - Recovery procedures
   - Status updates

6. RECOVERY
   - Restore data
   - Validate integrity
   - Restore connectivity
   - Verify functionality

7. LEARNING
   - Post-disaster review
   - Document lessons learned
   - Update procedures
   - Improve controls
----

**Multi-region backup strategy**

.Arquitectura recomendada:
[source,yaml]
----
  # Región Primary (us-west-2)
apiVersion: resources.cattle.io/v1
kind: BackupStorageLocation
metadata:
  name: primary-s3
  namespace: cattle-system
spec:
  s3:
    bucket: rancher-backups-primary
    region: us-west-2
  provider: s3

---
  # Región Secondary (us-east-1) - Geo-replica
apiVersion: resources.cattle.io/v1
kind: BackupStorageLocation
metadata:
  name: dr-s3
  namespace: cattle-system
spec:
  s3:
    bucket: rancher-backups-dr
    region: us-east-1
  provider: s3

---
  # Backup job que replica a ambas regiones
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-to-both-regions
  namespace: cattle-system
spec:
  schedule: "0 * * * *"  # Cada hora
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-replicator
          containers:
          - name: backup-sync
            image: amazon/aws-cli:latest
            command:
            - /bin/sh
            - -c
            - |
              # Sync from primary to secondary
              aws s3 sync s3://rancher-backups-primary/ \
                s3://rancher-backups-dr/ \
                --region us-west-2 \
                --copy-props metadata \
                --storage-class STANDARD_IA
          restartPolicy: OnFailure
----

**Runbook: Disaster Recovery**

.Procedimiento de recuperación de desastres:
[source,text]
----
SEVERITY: P1 - Critical System Down

>> DETECTION PHASE
1. Multiple alerts firing (multiple systems down)
2. Primary region unavailable
3. RTO timer starts

>> 0-5 MIN: INITIAL ASSESSMENT
- Confirm disaster scope
- Verify primary region is down
- Declare disaster
- Page all staff
- Open war room

>> 5-15 MIN: DECLARE FAILOVER
- Announce: "Switching to DR site"
- Notify management
- Notify customers (status page)
- Start disaster recovery runbook

>> 15-30 MIN: PROVISION DR ENVIRONMENT
- Provision new cluster in DR region
  * kubectl create cluster
  * Or use pre-provisioned standby

- Deploy networking
  * Load balancer in DR region
  * DNS failover (Route 53)
  * VPN/connectivity

>> 30-60 MIN: RESTORE FROM BACKUP
- Restore Rancher backup (from S3):
  velero restore create --from-backup latest

- Restore application data:
  - Velero restores PVs automatically
  - Or restore from DB replicas

- Verify integrity:
  kubectl get deployments -A
  kubectl get statefulsets -A
  kubectl get pvc -A

>> 60-90 MIN: VERIFY FUNCTIONALITY
- Health checks on all services
- Run smoke tests
- Check data consistency
- Verify connectivity

>> 90+ MIN: COMMUNICATE RECOVERY
- Update status page: "Services recovering"
- Update stakeholders
- Continue monitoring

>> POST-RECOVERY: ROOT CAUSE ANALYSIS
- What caused the disaster?
- How do we prevent it?
- Update DRP based on lessons learned
----

==== Testing de restauración

**Disaster Recovery Drill (DRD)**

Los drills regulares garantizan que el plan funciona.

.Tabla de frecuencia de testing:
[cols="1,1,2,2", options="header"]
|===
|Tipo |Frecuencia |Scope |Duración

|Backup verification
|Semanal
|Verify backup integrity
|15 min

|Restore test
|Mensual
|Restore to staging
|1-2 horas

|Full DR drill
|Trimestral
|Full failover simulation
|4-8 horas

|Live DR drill
|Anual
|Actual failover (planeado)
|Varios días
|===

.Script de testing automático:
[source,bash]
----
#!/bin/bash
# backup-verification.sh

set -e

BACKUP_STORAGE="s3://rancher-backups"
STAGING_CLUSTER="staging"

echo "=== Backup Verification ===" 
echo "[$(date)] Starting backup verification"

  # 1. Verificar que existen backups recientes
echo "[$(date)] Checking for recent backups..."
LATEST_BACKUP=$(aws s3 ls $BACKUP_STORAGE/ | tail -1 | awk '{print $4}')
BACKUP_AGE=$(($(date +%s) - $(date -d "$(aws s3 ls $BACKUP_STORAGE/$LATEST_BACKUP | head -1 | awk '{print $1, $2}')" +%s)))

  if [ $BACKUP_AGE -gt 86400 ]; then  # 24 horas
  echo "[ERROR] Latest backup is older than 24 hours"
  exit 1
fi

echo "[OK] Latest backup: $LATEST_BACKUP (age: ${BACKUP_AGE}s)"

  # 2. Restaurar a cluster de staging
echo "[$(date)] Restoring to staging cluster..."
velero restore create test-restore-$(date +%s) \
  --from-backup $(basename $LATEST_BACKUP .tar.gz) \
  --wait

  # 3. Verificar restore completado
echo "[$(date)] Waiting for restore to complete..."
RESTORE_STATUS=$(velero restore get | grep test-restore | awk '{print $4}')

if [ "$RESTORE_STATUS" != "Completed" ]; then
  echo "[ERROR] Restore failed with status: $RESTORE_STATUS"
  exit 1
fi

echo "[OK] Restore completed successfully"

  # 4. Run smoke tests en staging
echo "[$(date)] Running smoke tests..."
kubectl exec -n production-staging deployment/app -- curl http://localhost:8080/health

  # 5. Verificar datos
echo "[$(date)] Verifying data..."
RECORD_COUNT=$(kubectl exec -n production-staging pod/postgres-0 -- \
  psql -U postgres -d myapp -c "SELECT COUNT(*) FROM users;" | tail -2 | head -1)

echo "[OK] Data verification: $RECORD_COUNT records"

  # 6. Cleanup
echo "[$(date)] Cleaning up staging..."
velero restore delete test-restore-*

echo "[$(date)] Backup verification completed successfully"
----

.Ejecutar testing automatizado:
[source,bash]
----
  # Via cron (semanal)
cat >> /etc/cron.d/backup-verification <<EOF
0 3 * * 0 /usr/local/bin/backup-verification.sh >> /var/log/backup-verification.log 2>&1
EOF

  # Monitorear resultados
tail -f /var/log/backup-verification.log

  # Ver histórico de backups
ls -lah /backup-verification-*.log
----

**Restore test procedure**

.Procedimiento paso a paso:
[source,text]
----
1. PREPARATION (30 min antes)
   - Notify team
   - Create staging cluster (if needed)
   - Backup current state
   - Review runbook

2. SETUP (0-15 min)
   - Start timer
   - Open war room
   - Record process
   - Baseline metrics

3. RESTORE (15-45 min)
   - Download backup
   - Verify integrity (checksums)
   - Restore to staging
   - Monitor restore progress
   - Document issues

4. VALIDATION (45-90 min)
   - Verify all pods running
   - Check data integrity
   - Run smoke tests
   - Performance testing
   - Document any gaps

5. CLEANUP (90-120 min)
   - Delete test data
   - Document results
   - Create tickets for improvements

6. REVIEW (post-test)
   - Team retrospective
   - What went well?
   - What failed?
   - Action items
   - Update procedures
----

.Testing checklist:
[source,text]
----
Pre-Test Checks:
[ ] Backup exists and is < 24h old
[ ] Staging cluster has resources
[ ] Team members available
[ ] Network connectivity verified
[ ] Runbook reviewed

During Test:
[ ] Backup downloaded successfully
[ ] Restore started without errors
[ ] All resources recreated
[ ] Pods reached Running state
[ ] Services have endpoints
[ ] Data integrity verified
[ ] Application health checks pass
[ ] Network connectivity works
[ ] Performance acceptable

Post-Test:
[ ] All resources cleaned up
[ ] No failed pods remaining
[ ] Test logged and documented
[ ] Issues created for gaps
[ ] Team debriefing completed
[ ] Lessons learned documented
----

**Best practices para backup y DR**

[source,text]
----
Operacional:
1. RTO/RPO definidos y comunicados
2. Backups validados regularmente (no falsos positivos)
3. Testing mensual de restauración
4. DR drill anual con failover real
5. Documentación actualizada

Technical:
1. Cifrado en tránsito y en reposo
2. Verificación de integridad (checksums)
3. Retención basada en regulaciones
4. Geo-distributed storage
5. Automated failover capabilities

Governance:
1. Change log de backups
2. Auditoría de acceso
3. Segregación de acceso (least privilege)
4. Políticas de retención documentadas
5. Regular compliance reviews

Comunicación:
1. RTO/RPO comunicado a stakeholders
2. Status page durante incidentes
3. Post-mortem after DR events
4. Training material actualizado
5. Runbooks accessible off-site
----

=== 7.2 Actualizaciones y upgrades

Las actualizaciones en Rancher deben planificarse cuidadosamente para minimizar disrupciones. Esta sección cubre estrategias de actualización, garantías de cero downtime, procedimientos de rollback y gestión de versiones.

==== Estrategias de actualización

Las estrategias de actualización definen cómo y cuándo se actualiza Rancher manteniendo la disponibilidad.

**Tipos de estrategias:**

.Comparación de estrategias de actualización:
[cols="1,2,2,2"]
|===
|Estrategia |Ventajas |Desventajas |Caso de uso

|Rolling Update
|Sin downtime, capacidad gradual
|Versiones múltiples en transición, complejidad
|Producción con alta disponibilidad

|Blue-Green
|Rollback instantáneo, fácil validación
|Requiere el doble de recursos
|Actualizaciones críticas

|Canary
|Riesgo mínimo, validación gradual
|Complejidad operacional, monitoreo intenso
|Cambios mayores, nuevas versiones

|Maintenance Window
|Simple, recursos mínimos
|Downtime planificado, afecta usuarios
|Desarrollo, entornos no-producción
|===

**Rolling Update (Recomendado):**

Actualiza nodos secuencialmente, manteniendo servicio disponible.

[source,yaml]
----
## RKE2 Cluster con Rolling Update
apiVersion: rke.rancher.io/v1
kind: RKE2Config
metadata:
  name: prod-cluster-rolling-update
spec:
  kubernetesVersion: v1.29.0
  
  ## Actualización Rolling
  nodeDrainOptions:
    enabled: true
    timeout: 360s
    graceperiod: 30s
    skipWaitForDeleteTimeoutSeconds: 0
    deleteEmptyDirData: true
    ignoreDaemonsets: true
    ignoreErrors: false
    force: false
  
  ## Max nodes no disponibles
  maxUnavailable: 1
  
  ## Máximo de nodos actualizándose
  maxSurge: 0
  
  ## Worker node update strategy
  workerNodeUpdateStrategy:
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
      maxUnavailablePercentage: null
      maxSurgePercentage: null
  
  ## Control plane update strategy
  controlPlaneNodeUpdateStrategy:
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  
  ## Drain policy
  drainBeforeDelete: true
  skipWaitForDeleteTimeoutSeconds: 0
----

**Blue-Green Deployment:**

Mantiene dos ambientes paralelos: uno activo (blue) y otro en standby (green).

[source,yaml]
----
## Estrategia Blue-Green con Rancher
apiVersion: v1
kind: Namespace
metadata:
  name: rancher-blue-green

---
## Blue Environment (actual)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rancher-blue
  namespace: rancher-blue-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rancher
      environment: blue
  template:
    metadata:
      labels:
        app: rancher
        environment: blue
    spec:
      containers:
      - name: rancher
        image: rancher/rancher:v2.8.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 443
        livenessProbe:
          httpGet:
            path: /health
            port: 443
            scheme: HTTPS
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 5

---
## Green Environment (new version - standby)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rancher-green
  namespace: rancher-blue-green
spec:
  replicas: 0  ## Inicialmente escalado a 0
  selector:
    matchLabels:
      app: rancher
      environment: green
  template:
    metadata:
      labels:
        app: rancher
        environment: green
    spec:
      containers:
      - name: rancher
        image: rancher/rancher:v2.9.0  ## Nueva versión
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 443
        livenessProbe:
          httpGet:
            path: /health
            port: 443
            scheme: HTTPS
          initialDelaySeconds: 60
          periodSeconds: 10

---
## Service que apunta a Blue inicialmente
apiVersion: v1
kind: Service
metadata:
  name: rancher-traffic
  namespace: rancher-blue-green
spec:
  type: LoadBalancer
  selector:
    app: rancher
    environment: blue  ## Cambiar a 'green' durante cutover
  ports:
  - name: https
    port: 443
    targetPort: 443
    protocol: TCP
----

**Canary Deployment:**

Deploya la nueva versión a un pequeño porcentaje de usuarios primero.

[source,yaml]
----
## Canary Deployment con Istio (recomendado)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: rancher-canary
spec:
  hosts:
  - rancher.example.com
  http:
  - match:
    - uri:
        prefix: "/canary"
    route:
    - destination:
        host: rancher
        subset: v2-canary
      weight: 100
  - route:
    - destination:
        host: rancher
        subset: v1-stable
      weight: 95
    - destination:
        host: rancher
        subset: v2-canary
      weight: 5

---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: rancher-canary
spec:
  host: rancher
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 100
  subsets:
  - name: v1-stable
    labels:
      version: v2.8.0
  - name: v2-canary
    labels:
      version: v2.9.0
----

==== Zero-Downtime Upgrades

Garantizar disponibilidad continua durante actualizaciones.

**Prerequisitos:**

* Múltiples réplicas de Rancher (mínimo 3)
* Almacenamiento persistente sincronizado
* Health checks configurados
* Monitores y alertas activos
* Plan de rollback documentado

**Procedimiento de Zero-Downtime:**

[source,bash]
----
## Pre-upgrade validation
echo "=== Pre-Upgrade Validation ==="
kubectl get nodes -o wide
kubectl get pods -n cattle-system -o wide
helm list -n cattle-system

## Snapshot actual para rollback
echo "=== Creating pre-upgrade snapshot ==="
kubectl create namespace rancher-backup
kubectl apply -f - <<EOF
apiVersion: resources.cattle.io/v1
kind: Backup
metadata:
  name: pre-upgrade-backup-$(date +%Y%m%d-%H%M%S)
  namespace: rancher-backup
spec:
  backupStorageLocation:
    name: s3-backup
  resourceSet:
    desiredState: "all"
  encryption:
    enabled: true
EOF

## Esperar a que se complete el backup
kubectl wait --for=condition=Complete backup -n rancher-backup --timeout=600s

## Drain nodos worker (NO control plane)
echo "=== Draining worker nodes ==="
for node in $(kubectl get nodes -l node-role.kubernetes.io/worker -o jsonpath='{.items[*].metadata.name}'); do
  echo "Draining $node..."
  kubectl drain $node --ignore-daemonsets --delete-emptydir-data --pod-selector='app!=rancher' --timeout=600s
done

## Actualizar Rancher via Helm
echo "=== Updating Rancher ==="
CURRENT_VERSION=$(helm list -n cattle-system -o json | jq -r '.[0].app_version')
NEW_VERSION="v2.9.0"

echo "Current version: $CURRENT_VERSION"
echo "New version: $NEW_VERSION"

helm repo update rancher-latest
helm upgrade rancher rancher-latest/rancher \
  --namespace cattle-system \
  --version ${NEW_VERSION} \
  --set hostname=rancher.example.com \
  --set replicas=3 \
  --set bootstrap.password=admin \
  --wait \
  --timeout=600s

## Verificar rollout
echo "=== Monitoring rollout ==="
kubectl rollout status deployment/rancher -n cattle-system --timeout=600s
kubectl get pods -n cattle-system -o wide

## Verificar health de Rancher
echo "=== Health check ==="
RANCHER_IP=$(kubectl get service -n cattle-system rancher -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
for i in {1..30}; do
  HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -k https://${RANCHER_IP}/health)
  if [ "$HTTP_CODE" = "200" ]; then
    echo "✓ Rancher health check passed"
    break
  fi
  echo "Attempt $i/30: HTTP $HTTP_CODE"
  sleep 10
done

## Uncordon worker nodes
echo "=== Uncordoning worker nodes ==="
for node in $(kubectl get nodes -l node-role.kubernetes.io/worker -o jsonpath='{.items[*].metadata.name}'); do
  echo "Uncordoning $node..."
  kubectl uncordon $node
done

## Validación post-upgrade
echo "=== Post-upgrade validation ==="
kubectl get nodes -o wide
kubectl get pods -n cattle-system -o wide
kubectl top nodes
kubectl top pods -n cattle-system

echo "=== Upgrade completed successfully ==="
----

**Monitoreo durante actualización:**

[source,bash]
----
#!/bin/bash
# Monitor zero-downtime upgrade progress

NAMESPACE="cattle-system"
DEPLOYMENT="rancher"

echo "Monitoring upgrade progress..."
while true; do
  READY=$(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.status.readyReplicas}')
  DESIRED=$(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.spec.replicas}')
  UPDATED=$(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.status.updatedReplicas}')
  
  TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')
  echo "[$TIMESTAMP] Ready: $READY/$DESIRED | Updated: $UPDATED | Status: $(kubectl get deployment $DEPLOYMENT -n $NAMESPACE -o jsonpath='{.status.conditions[?(@.type=="Progressing")].message}')"
  
  if [ "$READY" = "$DESIRED" ] && [ "$UPDATED" = "$DESIRED" ]; then
    echo "✓ Upgrade complete!"
    break
  fi
  
  sleep 5
done
----

==== Rollback Procedures

Procedimiento rápido para revertir a versión anterior si hay problemas.

**Niveles de rollback:**

1. **Inmediato (< 1 minuto)**: Blue-Green deployment
2. **Rápido (< 5 minutos)**: Helm rollback
3. **Completo (< 30 minutos)**: Restore from backup

**Rollback via Helm:**

[source,bash]
----
## Ver histórico de releases
echo "=== Helm release history ==="
helm history rancher -n cattle-system

## Rollback a la release anterior
echo "=== Rolling back to previous version ==="
PREVIOUS_RELEASE=$(helm history rancher -n cattle-system -o json | \
  jq -r 'sort_by(.revision) | .[-2].revision')

helm rollback rancher $PREVIOUS_RELEASE -n cattle-system --wait --timeout=600s

## Verificar rollback
kubectl rollout status deployment/rancher -n cattle-system --timeout=600s

## Validación post-rollback
echo "=== Post-rollback validation ==="
kubectl get pods -n cattle-system
kubectl get deployment rancher -n cattle-system -o jsonpath='{.spec.template.spec.containers[0].image}'

## Test connectivity
RANCHER_IP=$(kubectl get service -n cattle-system rancher -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl -k -s https://${RANCHER_IP}/health | jq .
----

**Rollback desde backup:**

[source,bash]
----
## Listar backups disponibles
echo "=== Available backups ==="
kubectl get backups -n rancher-backup -o wide

## Seleccionar backup pre-upgrade
BACKUP_NAME="pre-upgrade-backup-20251031-143022"

## Crear restore job
echo "=== Creating restore ==="
kubectl apply -f - <<EOF
apiVersion: resources.cattle.io/v1
kind: Restore
metadata:
  name: restore-from-${BACKUP_NAME}
  namespace: rancher-backup
spec:
  backupStorageLocation:
    name: s3-backup
  backupName: ${BACKUP_NAME}
  resourceSet:
    desiredState: "all"
EOF

## Monitorear restore
echo "=== Monitoring restore ==="
kubectl wait --for=condition=Complete restore/restore-from-${BACKUP_NAME} \
  -n rancher-backup --timeout=1200s

## Verificar estado
kubectl get restore -n rancher-backup -o wide
kubectl describe restore restore-from-${BACKUP_NAME} -n rancher-backup
----

**Checklist de rollback:**

[cols="1,3,1"]
|===
|Item |Descripción |Estado

|Backup Pre-Upgrade
|Backup completado antes de actualización
|☐

|Helm History
|Verificar releases anteriores disponibles
|☐

|Test Rollback
|Ejecutar rollback en ambiente test
|☐

|Communication
|Notificar a stakeholders
|☐

|Ejecutar Rollback
|Rollback Helm o Restore
|☐

|Validación
|Verificar health y funcionalidad
|☐

|Post-Mortem
|Documentar causa y lecciones
|☐
|===

==== Gestión de versiones

Estrategia integral de versionado para Rancher, Kubernetes y componentes.

**Matriz de compatibilidad:**

[cols="1,2,2,2"]
|===
|Rancher Version |Min K8s |Max K8s |RKE2 Version

|v2.8.x
|v1.27
|v1.28
|v1.28.x

|v2.9.0
|v1.28
|v1.29
|v1.29.x

|v3.0.0 (beta)
|v1.29
|v1.31
|v1.31.x
|===

**Planificación de versiones:**

[source,yaml]
----
## Version management planning document
apiVersion: v1
kind: ConfigMap
metadata:
  name: rancher-version-plan
  namespace: cattle-system
data:
  current-version: "v2.8.5"
  target-version: "v2.9.0"
  
  kubernetes-upgrade-plan: |
    Phase 1: K8s v1.28.0 -> v1.28.3 (patch)
    Phase 2: K8s v1.28.3 -> v1.29.0 (minor)
    Phase 3: Rancher v2.8.5 -> v2.9.0 (minor)
    Phase 4: Component updates
  
  support-timeline: |
    v2.8.x: Maintenance until 2026-01-01
    v2.9.x: Active support until 2027-01-01
    v3.0.x: Early adopter until GA
  
  testing-checklist: |
    - Unit tests
    - Integration tests
    - End-to-end tests
    - Performance tests
    - Security scanning
    - Backup/restore validation
    - Upgrade/downgrade validation
----

**Política de versiones recomendada:**

* **Major Version (X.0.0)**: Ruptura de compatibilidad, planificación detallada requerida
* **Minor Version (X.Y.0)**: Nuevas características, compatibilidad mantenida, ≥3 meses entre versiones
* **Patch Version (X.Y.Z)**: Bugfixes y security patches, aplicar dentro de 2 semanas

**Ciclo de vida de versiones:**

.Version Lifecycle:
[cols="1,2,3"]
|===
|Fase |Duración |Acción

|Early Access
|2-4 semanas
|Testing beta, feedback recolectado

|General Availability (GA)
|12 meses
|Soporte full, updates regulares

|Maintenance
|6 meses
|Solo security patches

|End of Life (EOL)
|Post-EOL
|Sin soporte
|===

**Herramientas de versionado:**

[source,bash]
----
#!/bin/bash
## Version management utility

show_versions() {
  echo "=== Current Versions ==="
  echo "Rancher: $(helm list -n cattle-system -o json | jq -r '.[0].app_version')"
  echo "Kubernetes: $(kubectl version --short | grep Server | awk '{print $3}')"
  echo "RKE2: $(cat /etc/rancher/rke2/version.txt 2>/dev/null || echo 'N/A')"
  echo "Docker: $(docker version --format '{{.Server.Version}}' 2>/dev/null || echo 'N/A')"
  echo "Helm: $(helm version --short)"
}

check_compatibility() {
  RANCHER_VERSION=$1
  K8S_VERSION=$2
  
  ## Verificar matriz de compatibilidad
  case "$RANCHER_VERSION" in
    v2.8.*)
      if [[ "$K8S_VERSION" =~ ^v1\.(27|28) ]]; then
        echo "✓ Compatible"
        return 0
      fi
      ;;
    v2.9.*)
      if [[ "$K8S_VERSION" =~ ^v1\.(28|29) ]]; then
        echo "✓ Compatible"
        return 0
      fi
      ;;
  esac
  
  echo "✗ Incompatible"
  return 1
}

get_upgrade_path() {
  CURRENT=$1
  TARGET=$2
  
  echo "Upgrade path from $CURRENT to $TARGET:"
  echo "1. Test in staging environment"
  echo "2. Create pre-upgrade backup"
  echo "3. Plan maintenance window"
  echo "4. Execute upgrade with monitoring"
  echo "5. Validate all components"
  echo "6. Monitor for 24 hours"
}

show_versions
check_compatibility v2.9.0 v1.29.0
get_upgrade_path v2.8.5 v2.9.0
----

=== 7.3 Performance y optimización

Optimizar el rendimiento de Rancher y sus cargas de trabajo requiere monitoreo continuo, rightsizing de recursos y análisis de capacidad. Esta sección cubre optimización de recursos, troubleshooting de performance, capacity planning y cost monitoring.

==== Optimización de recursos

Garantizar que Rancher y las aplicaciones reciban suficientes recursos sin exceso de provisión.

**Configuración de requests y limits:**

[source,yaml]
----
## Rancher Deployment con resource requests/limits optimizados
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rancher
  namespace: cattle-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rancher
  template:
    metadata:
      labels:
        app: rancher
    spec:
      containers:
      - name: rancher
        image: rancher/rancher:v2.8.5
        imagePullPolicy: IfNotPresent
        
        ## Requests: mínimo garantizado
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          ## Limits: máximo permitido
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        ## Probes para evitar restart innecesarios
        livenessProbe:
          httpGet:
            path: /health
            port: 443
            scheme: HTTPS
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health
            port: 443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
      
      ## Affinity rules para distribuir pods
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - rancher
              topologyKey: kubernetes.io/hostname
----

**Resource Quotas por Namespace:**

[source,yaml]
----
## ResourceQuota para limitar uso en namespaces
apiVersion: v1
kind: ResourceQuota
metadata:
  name: rancher-quotas
  namespace: cattle-system
spec:
  hard:
    requests.memory: "16Gi"
    requests.cpu: "8"
    limits.memory: "32Gi"
    limits.cpu: "16"
    pods: "50"
    services: "10"
    persistentvolumeclaims: "5"

---
## LimitRange para defaults en pods
apiVersion: v1
kind: LimitRange
metadata:
  name: rancher-limits
  namespace: cattle-system
spec:
  limits:
  - type: Pod
    max:
      memory: "8Gi"
      cpu: "4"
    min:
      memory: "256Mi"
      cpu: "100m"
    default:
      memory: "1Gi"
      cpu: "500m"
    defaultRequest:
      memory: "512Mi"
      cpu: "250m"
  - type: Container
    max:
      memory: "4Gi"
      cpu: "2"
    min:
      memory: "128Mi"
      cpu: "50m"
    default:
      memory: "512Mi"
      cpu: "250m"
    defaultRequest:
      memory: "256Mi"
      cpu: "100m"
----

**Horizontal Pod Autoscaling (HPA):**

[source,yaml]
----
## HPA basado en CPU y memoria
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: rancher-hpa
  namespace: cattle-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: rancher
  
  minReplicas: 3
  maxReplicas: 10
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
----

=== 7.3.1 Troubleshooting de performance

Identificar y resolver cuellos de botella de rendimiento.

**Herramientas de profiling:**

[source,bash]
----
#!/bin/bash
## Performance troubleshooting utilities

check_resources() {
  echo "=== Rancher Pod Resources ==="
  kubectl get pods -n cattle-system rancher-* \
    -o custom-columns=NAME:.metadata.name,\
CPU_REQ:.spec.containers[0].resources.requests.cpu,\
CPU_LIM:.spec.containers[0].resources.limits.cpu,\
MEM_REQ:.spec.containers[0].resources.requests.memory,\
MEM_LIM:.spec.containers[0].resources.limits.memory
  
  echo ""
  echo "=== Current Usage ==="
  kubectl top pods -n cattle-system --containers=true
}

check_latency() {
  echo "=== API Server Latency ==="
  kubectl get --raw /metrics | grep apiserver_request_duration_seconds | head -10
}

check_memory() {
  echo "=== Node Memory Status ==="
  kubectl describe nodes | grep -A 5 "Allocated resources"
}

check_disk() {
  echo "=== Node Disk Pressure ==="
  kubectl describe nodes | grep -A 3 "DiskPressure\|InodesFree\|PIDPressure"
}

check_etcd() {
  echo "=== Etcd Commit Duration ==="
  kubectl get --raw /metrics | grep etcd_disk_backend_commit_duration
}

check_resources
check_latency
check_memory
check_disk
check_etcd
----

**Análisis de bottlenecks:**

[source,bash]
----
#!/bin/bash
## Bottleneck detection script

analyze_rancher_performance() {
  local NAMESPACE="cattle-system"
  
  echo "=== Rancher Performance Analysis ==="
  
  ## CPU throttling
  echo ""
  echo "CPU Throttling:"
  kubectl get --raw /metrics | grep container_cpu_cfs_throttled_seconds | \
    grep cattle-system | head -5
  
  ## Memory OOM
  echo ""
  echo "Memory OOM Events:"
  kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | grep OOMKilled
  
  ## Pod evictions
  echo ""
  echo "Pod Evictions:"
  kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | grep Evicted
  
  ## Network latency
  echo ""
  echo "Network Performance:"
  for pod in $(kubectl get pods -n $NAMESPACE -l app=rancher -o jsonpath='{.items[0].metadata.name}'); do
    kubectl exec -it $pod -n $NAMESPACE -- ping -c 3 localhost
  done
  
  ## Disk I/O
  echo ""
  echo "Disk I/O Performance:"
  kubectl exec -it $(kubectl get pods -n $NAMESPACE -l app=rancher -o jsonpath='{.items[0].metadata.name}') \
    -n $NAMESPACE -- df -h
}

analyze_rancher_performance
----

**Profiling de aplicaciones:**

[source,yaml]
----
## ServiceMonitor para capturar métricas de Rancher
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: rancher-metrics
  namespace: cattle-system
spec:
  selector:
    matchLabels:
      app: rancher
  endpoints:
  - port: metrics
    interval: 30s
    scrapeTimeout: 10s
    path: /metrics
    scheme: https
    tlsConfig:
      insecureSkipVerify: true

---
## PrometheusRule para alertas de performance
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: rancher-performance-alerts
  namespace: cattle-system
spec:
  groups:
  - name: rancher.performance
    interval: 30s
    rules:
    - alert: RancherHighCPUUsage
      expr: 'rate(container_cpu_usage_seconds_total{pod=~"rancher-.*",namespace="cattle-system"}[5m]) > 1.5'
      for: 5m
      annotations:
        summary: "Rancher high CPU usage"
        description: "Pod {{ $labels.pod }} is consuming {{ $value }} CPUs"
    
    - alert: RancherHighMemoryUsage
      expr: 'container_memory_working_set_bytes{pod=~"rancher-.*",namespace="cattle-system"} / container_spec_memory_limit_bytes > 0.8'
      for: 5m
      annotations:
        summary: "Rancher high memory usage"
        description: "Pod {{ $labels.pod }} is at {{ $value }}% memory"
    
    - alert: RancherAPILatency
      expr: 'histogram_quantile(0.95, apiserver_request_duration_seconds_bucket) > 1'
      for: 5m
      annotations:
        summary: "High API latency"
        description: "95th percentile latency is {{ $value }}s"
----

=== 7.3.2 Capacity planning

Planificar el crecimiento futuro y escalabilidad.

**Forecasting de recursos:**

[source,bash]
----
#!/bin/bash
## Capacity planning analysis

forecast_resources() {
  echo "=== Resource Capacity Planning ==="
  
  ## Recopilar datos históricos (últimos 30 días)
  THIRTY_DAYS_AGO=$(date -d "30 days ago" +%s)
  
  ## Promedio de uso CPU
  echo ""
  echo ">> CPU Average Usage (30 days):"
  kubectl get --raw "/api/v1/query?query=avg(rate(container_cpu_usage_seconds_total{namespace=\"cattle-system\",pod=~\"rancher-.*\"}[5m])) by (pod)" | jq .
  
  ## Promedio de uso memoria
  echo ""
  echo ">> Memory Average Usage (30 days):"
  kubectl get --raw "/api/v1/query?query=avg(container_memory_working_set_bytes{namespace=\"cattle-system\",pod=~\"rancher-.*\"}) by (pod)" | jq .
  
  ## Proyección a 90 días
  echo ""
  echo ">> Projected Usage at 90 days:"
  echo "Assuming 10% monthly growth:"
  echo "- CPU: 10% increase per month"
  echo "- Memory: 10% increase per month"
  echo "- Storage: 15% increase per month"
}

forecast_resources
----

**Matriz de capacidad:**

[cols="1,2,2,2,2"]
|===
|Escala |Min Nodes |Min CPU |Min Memory |Min Storage

|Desarrollo
|1-3
|4 cores
|8 GB
|50 GB

|Producción Pequeña
|3-5
|16 cores
|32 GB
|200 GB

|Producción Media
|5-10
|32 cores
|64 GB
|500 GB

|Producción Grande
|10+
|64+ cores
|128+ GB
|1+ TB

|Producción Enterprise
|20+
|128+ cores
|256+ GB
|5+ TB
|===

**Planificación de escalado:**

[source,yaml]
----
## ClusterAutoscaler para scaling automático de nodos
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  nodes.max: "100"
  nodes.min: "3"
  scale-down-enabled: "true"
  scale-down-delay-after-add: "10m"
  scale-down-unneeded-time: "10m"
  scale-down-unready-time: "20m"
  max-node-provision-time: "15m"
  max-total-unready-percentage: "45"
  ok-total-unready-count: "3"
  max-graceful-termination-sec: "600"

---
## Vertical Pod Autoscaler para optimizar requests/limits
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: rancher-vpa
  namespace: cattle-system
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: rancher
  updatePolicy:
    updateMode: "Auto"  ## auto, recreate, off
  resourcePolicy:
    containerPolicies:
    - containerName: "*"
      minAllowed:
        cpu: 500m
        memory: "512Mi"
      maxAllowed:
        cpu: "4"
        memory: "4Gi"
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
----

=== 7.3.3 Cost monitoring

Monitorear y optimizar costos de infraestructura.

**Herramientas de cost analysis:**

[source,bash]
----
#!/bin/bash
## Cost monitoring and analysis

## 1. Calcular costo por recurso
calculate_resource_cost() {
  echo "=== Resource Cost Calculation ==="
  
  ## Precios de ejemplo (AWS)
  CPU_COST_PER_HOUR=0.05  ## $0.05 per CPU per hour
  MEMORY_COST_PER_GB=0.01  ## $0.01 per GB per hour
  
  ## Obtener recursos actuales
  TOTAL_CPU=$(kubectl get nodes -o jsonpath='{.items[*].status.allocatable.cpu}' | \
    sed 's/m//g' | awk '{s+=$1} END {print s/1000}')
  TOTAL_MEMORY=$(kubectl get nodes -o jsonpath='{.items[*].status.allocatable.memory}' | \
    sed 's/Ki//g' | awk '{s+=$1} END {print s/1024/1024}')
  
  echo "Total CPU: $TOTAL_CPU cores"
  echo "Total Memory: $TOTAL_MEMORY GB"
  
  ## Calcular costos
  CPU_COST=$(echo "$TOTAL_CPU * $CPU_COST_PER_HOUR" | bc)
  MEMORY_COST=$(echo "$TOTAL_MEMORY * $MEMORY_COST_PER_GB" | bc)
  HOURLY_COST=$(echo "$CPU_COST + $MEMORY_COST" | bc)
  MONTHLY_COST=$(echo "$HOURLY_COST * 730" | bc)
  YEARLY_COST=$(echo "$MONTHLY_COST * 12" | bc)
  
  echo ""
  echo "Hourly cost: \$$HOURLY_COST"
  echo "Monthly cost: \$$MONTHLY_COST"
  echo "Yearly cost: \$$YEARLY_COST"
}

## 2. Identificar recursos desperdiciados
find_wasted_resources() {
  echo ""
  echo "=== Unused Resources Detection ==="
  
  ## Pods con muy bajo uso
  echo ">> Pods using < 10% of requested resources:"
  kubectl get pods --all-namespaces -o json | jq -r '.items[] | 
    select(.spec.containers[].resources.requests.cpu != null) |
    "\(.metadata.namespace)/\(.metadata.name)"'
  
  ## PVCs no usados
  echo ""
  echo ">> Unused PersistentVolumeClaims:"
  kubectl get pvc --all-namespaces -o json | jq -r '.items[] |
    select(.metadata.annotations."pv-used-by" == null) |
    "\(.metadata.namespace)/\(.metadata.name) - \(.spec.resources.requests.storage)"'
}

## 3. RI/Savings Plans análisis
show_commitment_savings() {
  echo ""
  echo "=== Reserved Instance Savings Analysis ==="
  echo "On-Demand vs RI (1-year): 30-40% savings"
  echo "On-Demand vs RI (3-year): 50-70% savings"
  echo ""
  echo "Recomendación: Usar RIs para cargas estables (control plane, base nodes)"
  echo "Usar On-Demand o Spot para cargas variables"
}

calculate_resource_cost
find_wasted_resources
show_commitment_savings
----

**Cost allocation tags:**

[source,yaml]
----
## ResourceQuota with cost labels
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    cost-center: "engineering"
    environment: "production"
    team: "platform"
    project: "rancher"

---
## Pod con cost tracking labels
apiVersion: v1
kind: Pod
metadata:
  name: rancher-app
  namespace: production
  labels:
    app: rancher
    cost-center: "engineering"
    environment: "production"
    team: "platform"
    version: "v2.8.5"
  annotations:
    cost.allocation/team: "platform"
    cost.allocation/project: "rancher"
    cost.allocation/environment: "production"
spec:
  containers:
  - name: rancher
    image: rancher/rancher:v2.8.5
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
      limits:
        memory: "4Gi"
        cpu: "2000m"
----

**Dashboard de costos:**

[source,yaml]
----
## Prometheus query para costos
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-metrics-queries
  namespace: monitoring
data:
  hourly-cost: |
    sum(
      sum(rate(container_cpu_usage_seconds_total[5m])) * 0.05 +
      sum(container_memory_working_set_bytes / 1024 / 1024 / 1024) * 0.01
    )
  
  namespace-cost: |
    sum by (namespace) (
      sum(rate(container_cpu_usage_seconds_total{pod!=""}[5m])) by (pod, namespace) * 0.05 +
      sum(container_memory_working_set_bytes{pod!=""} / 1024 / 1024 / 1024) by (pod, namespace) * 0.01
    )
  
  pod-cost: |
    sum by (pod, namespace) (
      sum(rate(container_cpu_usage_seconds_total[5m])) * 0.05 +
      sum(container_memory_working_set_bytes / 1024 / 1024 / 1024) * 0.01
    )
----

**Recomendaciones de optimización de costos:**

[cols="1,2,3,1"]
|===
|Estrategia |Descripción |Impacto |Esfuerzo

|Rightsizing
|Ajustar requests/limits a uso real
|15-25% reducción
|Bajo

|Reserved Instances
|Comprar RIs para cargas estables
|30-50% reducción
|Medio

|Spot Instances
|Usar Spot para cargas no-críticas
|60-90% reducción
|Medio

|Auto-scaling
|Escalar down recursos no-utilizados
|20-30% reducción
|Medio

|Cost Allocation
|Rastrear y optimizar por equipo
|10-20% reducción
|Bajo

|Scheduler Optimization
|Mejorar bin packing de pods
|15-30% reducción
|Alto
|===

== Recursos Adicionales

=== Documentación oficial
- https://ranchermanager.docs.rancher.com/[Documentación de Rancher Manager]
- https://rancherfederal.docs.rancher.com/[Rancher Government Documentation]
- https://github.com/rancherfederal/rancher[Rancher Federal GitHub]

=== Comunidades y soporte
- https://forums.rancher.com/[Foros de Rancher]
- https://slack.rancher.io/[Slack Community]
- https://www.suse.com/support/[Soporte SUSE]

=== Certificaciones
- https://www.suse.com/training/certification/[Certificaciones SUSE]
- https://www.rancher.com/training/[Rancher Training]

=== Herramientas recomendadas
- kubectl, helm, k9s
- Docker Desktop, Lens
- Prometheus, Grafana
- Velero, Longhorn